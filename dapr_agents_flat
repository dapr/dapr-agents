This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
agent/
  actor/
    __init__.py
    agent.py
    base.py
    interface.py
    schemas.py
    service.py
  patterns/
    openapi/
      __init__.py
      react.py
      tools.py
    react/
      __init__.py
      base.py
    toolcall/
      __init__.py
      base.py
    __init__.py
  telemetry/
    __init__.py
    otel.py
  utils/
    auth.py
    factory.py
    message_converter.py
    text_printer.py
  __init__.py
  base.py
document/
  embedder/
    __init__.py
    base.py
    nvidia.py
    openai.py
    sentence.py
  fetcher/
    __init__.py
    arxiv.py
    base.py
  reader/
    pdf/
      __init__.py
      pymupdf.py
      pypdf.py
    __init__.py
    base.py
    text.py
  splitter/
    __init__.py
    base.py
    text.py
  __init__.py
executors/
  utils/
    package_manager.py
  __init__.py
  base.py
  docker.py
  local.py
  sandbox.py
llm/
  dapr/
    __init__.py
    chat.py
    client.py
  elevenlabs/
    __init__.py
    client.py
    speech.py
  huggingface/
    __init__.py
    chat.py
    client.py
  nvidia/
    __init__.py
    chat.py
    client.py
    embeddings.py
  openai/
    client/
      __init__.py
      azure.py
      base.py
      openai.py
    __init__.py
    audio.py
    chat.py
    embeddings.py
  utils/
    __init__.py
    http.py
    request.py
    response.py
    stream.py
    structure.py
  __init__.py
  base.py
  chat.py
memory/
  __init__.py
  base.py
  daprstatestore.py
  liststore.py
  vectorstore.py
prompt/
  utils/
    chat.py
    fstring.py
    jinja.py
    prompty.py
    string.py
  __init__.py
  base.py
  chat.py
  prompty.py
  string.py
service/
  fastapi/
    __init__.py
    base.py
    dapr.py
  __init__.py
  base.py
storage/
  daprstores/
    __init__.py
    base.py
    secretstore.py
    statestore.py
  graphstores/
    neo4j/
      __init__.py
      base.py
      client.py
      utils.py
    __init__.py
    base.py
  vectorstores/
    __init__.py
    base.py
    chroma.py
    postgres.py
  __init__.py
tool/
  http/
    __init__.py
    client.py
  mcp/
    __init__.py
    client.py
    prompt.py
    schema.py
    transport.py
  storage/
    __init__.py
    vectorstore.py
  utils/
    __init__.py
    function_calling.py
    openapi.py
    tool.py
  __init__.py
  base.py
  executor.py
types/
  __init__.py
  agent.py
  document.py
  exceptions.py
  executor.py
  graph.py
  llm.py
  message.py
  schemas.py
  tools.py
  workflow.py
workflow/
  agents/
    assistant/
      __init__.py
      agent.py
      schemas.py
      state.py
    __init__.py
    base.py
  messaging/
    __init__.py
    decorator.py
    parser.py
    pubsub.py
    routing.py
    utils.py
  orchestrators/
    llm/
      __init__.py
      orchestrator.py
      prompts.py
      schemas.py
      state.py
      utils.py
    __init__.py
    base.py
    random.py
    roundrobin.py
  __init__.py
  agentic.py
  base.py
  decorators.py
  task.py
  utils.py
__init__.py

================================================================
Files
================================================================

================
File: agent/actor/__init__.py
================
from .base import AgentActorBase
from .interface import AgentActorInterface
from .service import AgentActorService
from .agent import AgentActor

================
File: agent/actor/agent.py
================
import logging
from dapr_agents.agent.actor.schemas import (
    AgentTaskResponse,
    TriggerAction,
    BroadcastMessage,
)
from dapr_agents.agent.actor.service import AgentActorService
from dapr_agents.types.agent import AgentActorMessage
from dapr_agents.workflow.messaging.decorator import message_router

logger = logging.getLogger(__name__)


class AgentActor(AgentActorService):
    """
    A Pydantic-based class for managing services and exposing FastAPI routes with Dapr pub/sub and actor support.
    """

    @message_router
    async def process_trigger_action(self, message: TriggerAction):
        """
        Processes TriggerAction messages sent directly to the agent's topic.
        """
        try:
            metadata = message.pop("_message_metadata", {})
            source = metadata.get("source", "unknown_source")
            message_type = metadata.get("type", "unknown_type")

            logger.info(f"{self.agent.name} received {message_type} from {source}.")

            # Extract workflow_instance_id if available
            workflow_instance_id = message.get("workflow_instance_id") or None
            logger.debug(f"Workflow instance ID: {workflow_instance_id}")

            # Execute the task or fallback to memory
            task = message.get("task", None)
            if not task:
                logger.info(f"{self.agent.name} executing default task from memory.")

            response = await self.invoke_task(task)

            # Check if the response exists
            content = (
                response.body.decode()
                if response and response.body
                else "Task completed but no response generated."
            )

            # Broadcast result
            response_message = BroadcastMessage(
                name=self.agent.name, role="user", content=content
            )
            await self.broadcast_message(message=response_message)

            # Update response
            response_message = response_message.model_dump()
            response_message["workflow_instance_id"] = workflow_instance_id
            agent_response = AgentTaskResponse(**response_message)

            # Send the message to the target agent
            await self.send_message_to_agent(name=source, message=agent_response)
        except Exception as e:
            logger.error(f"Error processing trigger action: {e}", exc_info=True)

    @message_router(broadcast=True)
    async def process_broadcast_message(self, message: BroadcastMessage):
        """
        Processes a message from the broadcast topic.
        """
        try:
            metadata = message.pop("_message_metadata", {})

            if not isinstance(metadata, dict):
                logger.warning(
                    f"{getattr(self, 'name', 'agent')} received a broadcast with invalid metadata. Ignoring."
                )
                return

            source = metadata.get("source", "unknown_source")
            message_type = metadata.get("type", "unknown_type")
            message_content = message.get("content", "No content")

            logger.info(
                f"{self.agent.name} received broadcast message of type '{message_type}' from '{source}'."
            )

            # Ignore messages sent by this agent
            if source == self.agent.name:
                logger.info(
                    f"{self.agent.name} ignored its own broadcast message of type '{message_type}'."
                )
                return

            # Log and process the valid broadcast message
            logger.debug(
                f"{self.agent.name} is processing broadcast message of type '{message_type}' from '{source}'."
            )
            logger.debug(f"Message content: {message_content}")

            # Add the message to the agent's memory
            self.agent.memory.add_message(message)

            # Add the message to the actor's state
            actor_message = AgentActorMessage(**message)
            await self.add_message(actor_message)

        except Exception as e:
            logger.error(f"Error processing broadcast message: {e}", exc_info=True)

================
File: agent/actor/base.py
================
import logging
from typing import List, Optional, Union
from dapr.actor import Actor
from dapr.actor.id import ActorId
from dapr.actor.runtime.context import ActorRuntimeContext
from dapr_agents.agent.actor.interface import AgentActorInterface
from dapr_agents.agent.base import AgentBase
from dapr_agents.types.agent import (
    AgentActorMessage,
    AgentActorState,
    AgentStatus,
    AgentTaskEntry,
    AgentTaskStatus,
)
from pydantic import ValidationError

logger = logging.getLogger(__name__)


class AgentActorBase(Actor, AgentActorInterface):
    """Base class for all agent actors, including task execution and agent state management."""

    def __init__(self, ctx: ActorRuntimeContext, actor_id: ActorId):
        super().__init__(ctx, actor_id)
        self.actor_id = actor_id
        self.agent: AgentBase
        self.agent_state_key = "agent_state"

    async def _on_activate(self) -> None:
        """
        Called when the actor is activated. Initializes the agent's state if not present.
        """
        logger.info(f"Activating actor with ID: {self.actor_id}")
        has_state, state_data = await self._state_manager.try_get_state(
            self.agent_state_key
        )

        if not has_state:
            # Initialize state with default values if it doesn't exist
            logger.info(f"Initializing state for {self.actor_id}")
            self.state = AgentActorState(overall_status=AgentStatus.IDLE)
            await self._state_manager.set_state(
                self.agent_state_key, self.state.model_dump()
            )
            await self._state_manager.save_state()
        else:
            # Load existing state
            logger.info(f"Loading existing state for {self.actor_id}")
            logger.debug(f"Existing state for {self.actor_id}: {state_data}")
            self.state = AgentActorState(**state_data)

    async def _on_deactivate(self) -> None:
        """
        Called when the actor is deactivated.
        """
        logger.info(
            f"Deactivate {self.__class__.__name__} actor with ID: {self.actor_id}."
        )

    async def set_status(self, status: AgentStatus) -> None:
        """
        Sets the current operational status of the agent and saves the state.
        """
        self.state.overall_status = status
        await self._state_manager.set_state(
            self.agent_state_key, self.state.model_dump()
        )
        await self._state_manager.save_state()

    async def invoke_task(self, task: Optional[str] = None) -> str:
        """
        Execute the agent's main task, log the input/output in the task history,
        and update state with observations, plans, and feedback.

        If no task is provided, use the most recent message content as the task entry input,
        but still execute `run()` directly if no task is passed.
        """
        logger.info(f"Actor {self.actor_id} invoking a task")

        # Determine the input for the task entry
        messages = await self.get_messages()  # Fetch messages from state
        default_task = None

        if messages:
            # Look for the last message in the conversation history
            last_message = messages[-1]
            default_task = last_message.get("content")
            logger.debug(
                f"Default task entry input derived from last message: {default_task}"
            )

        # Prepare the input for task entry
        task_entry_input = task or default_task or "Triggered without a specific task"
        logger.debug(f"Task entry input: {task_entry_input}")

        # Set the agent's status to active
        await self.set_status(AgentStatus.ACTIVE)

        # Create a new task entry with the determined input
        task_entry = AgentTaskEntry(
            input=task_entry_input,
            status=AgentTaskStatus.IN_PROGRESS,
        )
        self.state.task_history.append(task_entry)

        # Save initial task state with IN_PROGRESS status
        await self._state_manager.set_state(
            self.agent_state_key, self.state.model_dump()
        )
        await self._state_manager.save_state()

        try:
            # Run the task if provided, or fallback to agent.run() if no task
            task_input = task or None
            result = await self.agent.run(task_input)

            # Update the task entry with the result and mark as COMPLETE
            task_entry.output = result
            task_entry.status = AgentTaskStatus.COMPLETE

            # Add the result as a new message in conversation history
            assistant_message = AgentActorMessage(role="assistant", content=result)
            await self.add_message(assistant_message)

            return result

        except Exception as e:
            # Handle task failure
            logger.error(f"Error running task for actor {self.actor_id}: {str(e)}")
            task_entry.status = AgentTaskStatus.FAILED
            task_entry.output = str(e)
            raise e

        finally:
            # Ensure the final state of the task is saved
            await self._state_manager.set_state(
                self.agent_state_key, self.state.model_dump()
            )
            await self._state_manager.save_state()
            # Revert the agent's status to idle
            await self.set_status(AgentStatus.IDLE)

    async def add_message(self, message: Union[AgentActorMessage, dict]) -> None:
        """
        Adds a message to the conversation history in the actor's state.

        Args:
            message (Union[AgentActorMessage, dict]): The message to add, either as a dictionary or an AgentActorMessage instance.
        """
        # Convert dictionary to AgentActorMessage if necessary
        if isinstance(message, dict):
            message = AgentActorMessage(**message)

        # Add the new message to the state
        self.state.messages.append(message)
        self.state.message_count += 1

        # Save state back to Dapr
        await self._state_manager.set_state(
            self.agent_state_key, self.state.model_dump()
        )
        await self._state_manager.save_state()

    async def get_messages(self) -> List[dict]:
        """
        Retrieves the messages from the actor's state, validates it using Pydantic,
        and returns a list of dictionaries if valid.
        """
        has_state, state_data = await self._state_manager.try_get_state(
            self.agent_state_key
        )

        if has_state:
            try:
                # Validate the state data using Pydantic
                state: AgentActorState = AgentActorState.model_validate(state_data)

                # Return the list of messages as dictionaries (timestamp will be automatically serialized to ISO format)
                return [message.model_dump() for message in state.messages]
            except ValidationError as e:
                # Handle validation errors
                print(f"Validation error: {e}")
                return []
        return []

================
File: agent/actor/interface.py
================
from abc import abstractmethod
from typing import List, Optional, Union
from dapr.actor import ActorInterface, actormethod
from dapr_agents.types.agent import AgentActorMessage, AgentStatus


class AgentActorInterface(ActorInterface):
    @abstractmethod
    @actormethod(name="InvokeTask")
    async def invoke_task(self, task: Optional[str] = None) -> str:
        """
        Invoke a task and returns the result as a string.
        """
        pass

    @abstractmethod
    @actormethod(name="AddMessage")
    async def add_message(self, message: Union[AgentActorMessage, dict]) -> None:
        """
        Adds a message to the conversation history in the actor's state.
        """
        pass

    @abstractmethod
    @actormethod(name="GetMessages")
    async def get_messages(self) -> List[dict]:
        """
        Retrieves the conversation history from the actor's state.
        """
        pass

    @abstractmethod
    @actormethod(name="SetStatus")
    async def set_status(self, status: AgentStatus) -> None:
        """
        Sets the current operational status of the agent.
        """
        pass

================
File: agent/actor/schemas.py
================
from typing import Optional
from pydantic import BaseModel, Field
from dapr_agents.types.message import BaseMessage


class AgentTaskResponse(BaseMessage):
    """
    Represents a response message from an agent after completing a task.
    """

    workflow_instance_id: Optional[str] = Field(
        default=None, description="Dapr workflow instance id from source if available"
    )


class TriggerAction(BaseModel):
    """
    Represents a message used to trigger an agent's activity within the workflow.
    """

    task: Optional[str] = Field(
        None,
        description="The specific task to execute. If not provided, the agent will act based on its memory or predefined behavior.",
    )
    iteration: Optional[int] = Field(0, description="")
    workflow_instance_id: Optional[str] = Field(
        default=None, description="Dapr workflow instance id from source if available"
    )


class BroadcastMessage(BaseMessage):
    """
    Represents a broadcast message from an agent
    """

================
File: agent/actor/service.py
================
import asyncio
import json
import logging
from contextlib import asynccontextmanager
from datetime import timedelta
from typing import Any, Callable, Dict, Optional, Tuple, Type, Union
import time

from fastapi import FastAPI, HTTPException, Response, status
from fastapi.encoders import jsonable_encoder
from fastapi.responses import JSONResponse

from dapr.actor import ActorId, ActorProxy
from dapr.actor.runtime.config import (
    ActorReentrancyConfig,
    ActorRuntimeConfig,
    ActorTypeConfig,
)
from dapr.actor.runtime.runtime import ActorRuntime
from dapr.clients import DaprClient
from dapr.clients.grpc._request import (
    TransactionOperationType,
    TransactionalStateOperation,
)
from dapr.clients.grpc._response import StateResponse
from dapr.clients.grpc._state import Concurrency, Consistency, StateOptions
from dapr.ext.fastapi import DaprActor

from pydantic import BaseModel, ConfigDict, Field, PrivateAttr, model_validator

from dapr_agents.agent import AgentBase
from dapr_agents.agent.actor import AgentActorBase, AgentActorInterface
from dapr_agents.service.fastapi import FastAPIServerBase
from dapr_agents.types.agent import AgentActorMessage
from dapr_agents.workflow.messaging import DaprPubSub
from dapr_agents.workflow.messaging.routing import MessageRoutingMixin

logger = logging.getLogger(__name__)


class AgentActorService(DaprPubSub, MessageRoutingMixin):
    agent: AgentBase
    name: Optional[str] = Field(
        default=None,
        description="Name of the agent actor, derived from the agent if not provided.",
    )
    agent_topic_name: Optional[str] = Field(
        None,
        description="The topic name dedicated to this specific agent, derived from the agent's name if not provided.",
    )
    broadcast_topic_name: str = Field(
        "beacon_channel",
        description="The default topic used for broadcasting messages to all agents.",
    )
    agents_registry_store_name: str = Field(
        ...,
        description="The name of the Dapr state store component used to store and share agent metadata centrally.",
    )
    agents_registry_key: str = Field(
        default="agents_registry",
        description="Dapr state store key for agentic workflow state.",
    )
    service_port: Optional[int] = Field(
        default=None, description="The port number to run the API server on."
    )
    service_host: Optional[str] = Field(
        default="0.0.0.0", description="Host address for the API server."
    )

    # Fields initialized in model_post_init
    actor: Optional[DaprActor] = Field(
        default=None, init=False, description="DaprActor for actor lifecycle support."
    )
    actor_name: Optional[str] = Field(
        default=None, init=False, description="Actor name"
    )
    actor_proxy: Optional[ActorProxy] = Field(
        default=None,
        init=False,
        description="Proxy for invoking methods on the agent's actor.",
    )
    actor_class: Optional[type] = Field(
        default=None,
        init=False,
        description="Dynamically created actor class for the agent",
    )
    agent_metadata: Optional[dict] = Field(
        default=None, init=False, description="Agent's metadata"
    )

    # Private internal attributes (not schema/validated)
    _http_server: Optional[Any] = PrivateAttr(default=None)
    _shutdown_event: asyncio.Event = PrivateAttr(default_factory=asyncio.Event)
    _dapr_client: Optional[DaprClient] = PrivateAttr(default=None)
    _is_running: bool = PrivateAttr(default=False)
    _subscriptions: Dict[str, Callable] = PrivateAttr(default_factory=dict)
    _topic_handlers: Dict[
        Tuple[str, str], Dict[Type[BaseModel], Callable]
    ] = PrivateAttr(default_factory=dict)

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @model_validator(mode="before")
    def set_derived_fields(cls, values: dict):
        agent: AgentBase = values.get("agent")
        # Derive agent_topic_name if missing
        if not values.get("agent_topic_name") and agent:
            values["agent_topic_name"] = agent.name or agent.role
        # Derive name from agent if missing
        if not values.get("name") and agent:
            values["name"] = agent.name or agent.role
        return values

    def model_post_init(self, __context: Any) -> None:
        # Proceed with base model setup
        super().model_post_init(__context)

        # Dynamically create the actor class based on the agent's name
        actor_class_name = f"{self.agent.name}Actor"

        # Create the actor class dynamically using the 'type' function
        self.actor_class = type(
            actor_class_name,
            (AgentActorBase,),
            {
                "__init__": lambda self, ctx, actor_id: AgentActorBase.__init__(
                    self, ctx, actor_id
                ),
                "agent": self.agent,
            },
        )

        # Prepare agent metadata
        self.agent_metadata = {
            "name": self.agent.name,
            "role": self.agent.role,
            "goal": self.agent.goal,
            "topic_name": self.agent_topic_name,
            "pubsub_name": self.message_bus_name,
            "orchestrator": False,
        }

        # Proxy for actor methods
        self.actor_name = self.actor_class.__name__
        self.actor_proxy = ActorProxy.create(
            self.actor_name, ActorId(self.agent.name), AgentActorInterface
        )

        # Initialize Sync Dapr Client
        self._dapr_client = DaprClient()

        # FastAPI Server
        self._http_server: FastAPIServerBase = FastAPIServerBase(
            service_name=self.agent.name,
            service_port=self.service_port,
            service_host=self.service_host,
        )
        self._http_server.app.router.lifespan_context = self.lifespan

        # Create DaprActor using FastAPI app
        self.actor = DaprActor(self.app)

        self.app.add_api_route("/GetMessages", self.get_messages, methods=["GET"])

        logger.info(f"Dapr Actor class {self.actor_class.__name__} initialized.")

    @property
    def app(self) -> "FastAPI":
        """
        Returns the FastAPI application instance if the workflow was initialized as a service.

        Raises:
            RuntimeError: If the FastAPI server has not been initialized via `.as_service()` first.
        """
        if self._http_server:
            return self._http_server.app
        raise RuntimeError("FastAPI server not initialized.")

    @asynccontextmanager
    async def lifespan(self, app: FastAPI):
        # Register actor
        actor_runtime_config = ActorRuntimeConfig()
        actor_runtime_config.update_actor_type_configs(
            [
                ActorTypeConfig(
                    actor_type=self.actor_class.__name__,
                    actor_idle_timeout=timedelta(hours=1),
                    actor_scan_interval=timedelta(seconds=30),
                    drain_ongoing_call_timeout=timedelta(minutes=1),
                    drain_rebalanced_actors=True,
                    reentrancy=ActorReentrancyConfig(enabled=True),
                )
            ]
        )
        ActorRuntime.set_actor_config(actor_runtime_config)

        await self.actor.register_actor(self.actor_class)
        logger.info(f"{self.actor_name} Dapr actor registered.")

        # Register agent metadata and pubsub routes
        self.register_agent_metadata()
        self.register_message_routes()

        try:
            yield
        finally:
            await self.stop()

    async def start(self):
        if self._is_running:
            logger.warning(
                "Service is already running. Ignoring duplicate start request."
            )
            return

        logger.info("Starting Agent Actor Service...")
        self._shutdown_event.clear()

        await self._http_server.start()

        self._is_running = True

    async def stop(self):
        if not self._is_running:
            return

        await self._http_server.stop()

        for (pubsub_name, topic_name), close_fn in self._subscriptions.items():
            try:
                logger.info(
                    f"Unsubscribing from pubsub '{pubsub_name}' topic '{topic_name}'"
                )
                close_fn()
            except Exception as e:
                logger.error(f"Failed to unsubscribe from topic '{topic_name}': {e}")

        self._subscriptions.clear()
        self._is_running = False
        logger.info("Agent Actor Service stopped.")

    def get_data_from_store(self, store_name: str, key: str) -> Optional[dict]:
        """
        Retrieve data from a specified Dapr state store using a provided key.

        Args:
            store_name (str): The name of the Dapr state store component.
            key (str): The key under which the data is stored.

        Returns:
            Optional[dict]: The data stored under the specified key if found; otherwise, None.
        """
        try:
            response: StateResponse = self._dapr_client.get_state(
                store_name=store_name, key=key
            )
            data = response.data

            return json.loads(data) if data else None
        except Exception:
            logger.warning(
                f"Error retrieving data for key '{key}' from store '{store_name}'"
            )
            return None

    def get_agents_metadata(
        self, exclude_self: bool = True, exclude_orchestrator: bool = False
    ) -> dict:
        """
        Retrieves metadata for all registered agents while ensuring orchestrators do not interact with other orchestrators.

        Args:
            exclude_self (bool, optional): If True, excludes the current agent (`self.agent.name`). Defaults to True.
            exclude_orchestrator (bool, optional): If True, excludes all orchestrators from the results. Defaults to False.

        Returns:
            dict: A mapping of agent names to their metadata. Returns an empty dict if no agents are found.

        Raises:
            RuntimeError: If the state store is not properly configured or retrieval fails.
        """
        try:
            # Fetch agent metadata from the registry
            agents_metadata = (
                self.get_data_from_store(
                    self.agents_registry_store_name, self.agents_registry_key
                )
                or {}
            )

            if agents_metadata:
                logger.info(
                    f"Agents found in '{self.agents_registry_store_name}' for key '{self.agents_registry_key}'."
                )

                # Filter based on exclusion rules
                filtered_metadata = {
                    name: metadata
                    for name, metadata in agents_metadata.items()
                    if not (
                        exclude_self and name == self.agent.name
                    )  # Exclude self if requested
                    and not (
                        exclude_orchestrator and metadata.get("orchestrator", False)
                    )  # Exclude all orchestrators if exclude_orchestrator=True
                }

                if not filtered_metadata:
                    logger.info("No other agents found after filtering.")

                return filtered_metadata

            logger.info(
                f"No agents found in '{self.agents_registry_store_name}' for key '{self.agents_registry_key}'."
            )
            return {}
        except Exception as e:
            logger.error(f"Failed to retrieve agents metadata: {e}", exc_info=True)
            return {}

    def register_agent_metadata(self) -> None:
        """
        Registers the agent's metadata in the Dapr state store under 'agents_metadata'.
        """
        try:
            # Update the agents registry store with the new agent metadata
            self.register_agent(
                store_name=self.agents_registry_store_name,
                store_key=self.agents_registry_key,
                agent_name=self.name,
                agent_metadata=self.agent_metadata,
            )
            logger.info(
                f"{self.name} registered its metadata under key '{self.agents_registry_key}'"
            )
        except Exception as e:
            logger.error(
                f"Failed to register metadata for agent {self.agent.name}: {e}"
            )
            raise e

    def register_agent(
        self, store_name: str, store_key: str, agent_name: str, agent_metadata: dict
    ) -> None:
        """
        Merges the existing data with the new data and updates the store.

        Args:
            store_name (str): The name of the Dapr state store component.
            key (str): The key to update.
            data (dict): The data to update the store with.
        """
        # retry the entire operation up to ten times sleeping 1 second between each attempt
        for attempt in range(1, 11):
            try:
                response: StateResponse = self._dapr_client.get_state(
                    store_name=store_name, key=store_key
                )
                if not response.etag:
                    # if there is no etag the following transaction won't work as expected
                    # so we need to save an empty object with a strong consistency to force the etag to be created
                    self._dapr_client.save_state(
                        store_name=store_name,
                        key=store_key,
                        value=json.dumps({}),
                        state_metadata={"contentType": "application/json"},
                        options=StateOptions(
                            concurrency=Concurrency.first_write,
                            consistency=Consistency.strong,
                        ),
                    )
                    # raise an exception to retry the entire operation
                    raise Exception(f"No etag found for key: {store_key}")
                existing_data = json.loads(response.data) if response.data else {}
                if (agent_name, agent_metadata) in existing_data.items():
                    logger.debug(f"agent {agent_name} already registered.")
                    return None
                agent_data = {agent_name: agent_metadata}
                merged_data = {**existing_data, **agent_data}
                logger.debug(f"merged data: {merged_data} etag: {response.etag}")
                try:
                    # using the transactional API to be able to later support the Dapr outbox pattern
                    self._dapr_client.execute_state_transaction(
                        store_name=store_name,
                        operations=[
                            TransactionalStateOperation(
                                key=store_key,
                                data=json.dumps(merged_data),
                                etag=response.etag,
                                operation_type=TransactionOperationType.upsert,
                            )
                        ],
                        transactional_metadata={"contentType": "application/json"},
                    )
                except Exception as e:
                    raise e
                return None
            except Exception as e:
                logger.debug(f"Error on transaction attempt: {attempt}: {e}")
                logger.debug("Sleeping for 1 second before retrying transaction...")
                time.sleep(1)
        raise Exception(
            f"Failed to update state store key: {store_key} after 10 attempts."
        )

    async def invoke_task(self, task: Optional[str]) -> Response:
        """
        Use the actor to invoke a task by running the InvokeTask method through ActorProxy.

        Args:
            task (Optional[str]): The task string to invoke on the actor.

        Returns:
            Response: A FastAPI Response containing the result or an error message.
        """
        try:
            response = await self.actor_proxy.InvokeTask(task)
            return Response(content=response, status_code=status.HTTP_200_OK)
        except Exception as e:
            logger.error(f"Failed to run task for {self.actor_name}: {e}")
            raise HTTPException(
                status_code=500, detail=f"Error invoking task: {str(e)}"
            )

    async def add_message(self, message: AgentActorMessage) -> None:
        """
        Adds a message to the conversation history in the actor's state.
        """
        try:
            await self.actor_proxy.AddMessage(message.model_dump())
        except Exception as e:
            logger.error(f"Failed to add message to {self.actor_name}: {e}")

    async def get_messages(self) -> Response:
        """
        Retrieve the conversation history from the actor.
        """
        try:
            messages = await self.actor_proxy.GetMessages()
            return JSONResponse(
                content=jsonable_encoder(messages), status_code=status.HTTP_200_OK
            )
        except Exception as e:
            logger.error(f"Failed to retrieve messages for {self.actor_name}: {e}")
            raise HTTPException(
                status_code=500, detail=f"Error retrieving messages: {str(e)}"
            )

    async def broadcast_message(
        self,
        message: Union[BaseModel, dict],
        exclude_orchestrator: bool = False,
        **kwargs,
    ) -> None:
        """
        Sends a message to all agents (or only to non-orchestrator agents if exclude_orchestrator=True).

        Args:
            message (Union[BaseModel, dict]): The message content as a Pydantic model or dictionary.
            exclude_orchestrator (bool, optional): If True, excludes orchestrators from receiving the message. Defaults to False.
            **kwargs: Additional metadata fields to include in the message.
        """
        try:
            # Retrieve agents metadata while respecting the exclude_orchestrator flag
            agents_metadata = self.get_agents_metadata(
                exclude_orchestrator=exclude_orchestrator
            )

            if not agents_metadata:
                logger.warning("No agents available for broadcast.")
                return

            logger.info(f"{self.agent.name} broadcasting message to selected agents.")

            await self.publish_event_message(
                topic_name=self.broadcast_topic_name,
                pubsub_name=self.message_bus_name,
                source=self.agent.name,
                message=message,
                **kwargs,
            )

            logger.debug(f"{self.agent.name} broadcasted message.")
        except Exception as e:
            logger.error(f"Failed to broadcast message: {e}", exc_info=True)

    async def send_message_to_agent(
        self, name: str, message: Union[BaseModel, dict], **kwargs
    ) -> None:
        """
        Sends a message to a specific agent.

        Args:
            name (str): The name of the target agent.
            message (Union[BaseModel, dict]): The message content as a Pydantic model or dictionary.
            **kwargs: Additional metadata fields to include in the message.
        """
        try:
            agents_metadata = self.get_agents_metadata()

            if name not in agents_metadata:
                logger.warning(
                    f"Target '{name}' is not registered as an agent. Skipping message send."
                )
                return  # Do not raise an error—just warn and move on.

            agent_metadata = agents_metadata[name]
            logger.info(f"{self.agent.name} sending message to agent '{name}'.")

            await self.publish_event_message(
                topic_name=agent_metadata["topic_name"],
                pubsub_name=agent_metadata["pubsub_name"],
                source=self.name,
                message=message,
                **kwargs,
            )

            logger.debug(f"{self.name} sent message to agent '{name}'.")
        except Exception as e:
            logger.error(
                f"Failed to send message to agent '{name}': {e}", exc_info=True
            )

================
File: agent/patterns/openapi/__init__.py
================
from .react import OpenAPIReActAgent

================
File: agent/patterns/openapi/react.py
================
from dapr_agents.tool.utils.openapi import OpenAPISpecParser, openapi_spec_to_openai_fn
from dapr_agents.agent.patterns.react import ReActAgent
from dapr_agents.storage import VectorStoreBase
from dapr_agents.tool.storage import VectorToolStore
from typing import Dict, Optional, List, Any
from pydantic import Field, ConfigDict
import logging

logger = logging.getLogger(__name__)


class OpenAPIReActAgent(ReActAgent):
    """
    Extends ReActAgent with OpenAPI handling capabilities, including tools for managing API calls.
    """

    role: str = Field(
        default="OpenAPI Expert", description="The agent's role in the interaction."
    )
    goal: str = Field(
        default="Help users work with OpenAPI specifications and API integrations.",
        description="The main objective of the agent.",
    )
    instructions: List[str] = Field(
        default=[
            "You are an expert assistant specialized in working with OpenAPI specifications and API integrations.",
            "Your goal is to help users identify the correct API endpoints and execute API calls efficiently and accurately.",
            "You must first help users explore potential APIs by analyzing OpenAPI definitions, then assist in making authenticated API requests.",
            "Ensure that all API calls are executed with the correct parameters, authentication, and methods.",
            "Your responses should be concise, clear, and focus on guiding the user through the steps of working with APIs, including retrieving API definitions, understanding endpoint parameters, and handling errors.",
            "You only respond to questions directly related to your role.",
        ],
        description="Instructions to guide the agent's behavior.",
    )
    spec_parser: OpenAPISpecParser = Field(
        ..., description="Parser for handling OpenAPI specifications."
    )
    api_vector_store: VectorStoreBase = Field(
        ..., description="Vector store for storing API definitions."
    )
    auth_header: Optional[Dict] = Field(
        None, description="Authentication headers for executing API calls."
    )

    tool_vector_store: Optional[VectorToolStore] = Field(
        default=None, init=False, description="Internal vector store for OpenAPI tools."
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def model_post_init(self, __context: Any) -> None:
        """
        Post-initialization setup for OpenAPIReActAgent, including vector stores and OpenAPI tools.
        """
        logger.info("Setting up VectorToolStore for OpenAPIReActAgent...")

        # Initialize tool vector store using the api_vector_store
        self.tool_vector_store = VectorToolStore(vector_store=self.api_vector_store)

        # Load OpenAPI specifications into the tool vector store
        function_list = openapi_spec_to_openai_fn(self.spec_parser)
        self.tool_vector_store.add_tools(function_list)

        # Generate OpenAPI-specific tools
        from .tools import generate_api_call_executor, generate_get_openapi_definition

        openapi_tools = [
            generate_get_openapi_definition(self.tool_vector_store),
            generate_api_call_executor(self.spec_parser, self.auth_header),
        ]

        # Extend tools with OpenAPI tools
        self.tools.extend(openapi_tools)

        # Call parent model_post_init for additional setup
        super().model_post_init(__context)

================
File: agent/patterns/openapi/tools.py
================
import json
import logging
import requests
from urllib.parse import urlparse
from typing import Any, Dict, Optional, List

from pydantic import BaseModel, Field, ConfigDict

from dapr_agents.tool.base import tool
from dapr_agents.tool.storage import VectorToolStore
from dapr_agents.tool.utils.openapi import OpenAPISpecParser

logger = logging.getLogger(__name__)


def _extract_version(path: str) -> str:
    """Extracts the version prefix from a path if it exists, assuming it starts with 'v' followed by digits."""
    seg = path.lstrip("/").split("/", 1)[0]
    return seg if seg.startswith("v") and seg[1:].isdigit() else ""


def _join_url(base: str, path: str) -> str:
    """
    Join *base* and *path* while avoiding duplicated version segments
    and double slashes. Assumes base already ends at the **/servers[0].url**.
    """
    parsed = urlparse(base)
    origin = f"{parsed.scheme}://{parsed.netloc}"
    base_path = parsed.path.strip("/")

    b_ver, p_ver = _extract_version(base_path), _extract_version(path)
    if b_ver and b_ver == p_ver:
        path = path[len(f"/{p_ver}") :]

    pieces = [p for p in (base_path, path.lstrip("/")) if p]
    return f"{origin}/" + "/".join(pieces).replace("//", "/")


def _fmt_candidate(doc: str, meta: Dict[str, Any]) -> str:
    """Return a single nice, log-friendly candidate string."""
    meta_line = f"url={meta.get('url')} | method={meta.get('method', '').upper()} | name={meta.get('name')}"
    return f"{doc.strip()}\n{meta_line}"


class GetDefinitionInput(BaseModel):
    """Free-form query describing *one* desired operation (e.g. "multiply two numbers")."""

    user_input: str = Field(
        ..., description="Natural-language description of ONE desired API operation."
    )


def generate_get_openapi_definition(store: VectorToolStore):
    @tool(args_model=GetDefinitionInput)
    def get_openapi_definition(user_input: str) -> List[str]:
        """
        Search the vector store for OpenAPI *operation IDs / paths* most relevant
        to **one** user task.

        Always call this **once per new task** *before* attempting an
        `open_api_call_executor`. Returns up to 5 candidate operations.
        """
        result = store.get_similar_tools(query_texts=[user_input], k=5)
        docs: List[str] = result["documents"][0]
        metas: List[Dict[str, Any]] = result["metadatas"][0]

        return [_fmt_candidate(d, m) for d, m in zip(docs, metas)]

    return get_openapi_definition


class OpenAPIExecutorInput(BaseModel):
    path_template: str = Field(
        ..., description="Path template, may contain `{placeholder}` segments."
    )
    method: str = Field(..., description="HTTP verb, upper‑case.")
    path_params: Dict[str, Any] = Field(
        default_factory=dict, description="Replacements for path placeholders."
    )
    data: Dict[str, Any] = Field(
        default_factory=dict, description="JSON body for POST/PUT/PATCH."
    )
    headers: Optional[Dict[str, Any]] = Field(
        default=None, description="Extra request headers."
    )
    params: Optional[Dict[str, Any]] = Field(
        default=None, description="Query params (?key=value)."
    )

    model_config = ConfigDict(extra="allow")


def generate_api_call_executor(
    spec: OpenAPISpecParser, auth_header: Optional[Dict[str, str]] = None
):
    base_url = spec.spec.servers[0].url  # assumes at least one server entry

    @tool(args_model=OpenAPIExecutorInput)
    def open_api_call_executor(
        *,
        path_template: str,
        method: str,
        path_params: Dict[str, Any],
        data: Dict[str, Any],
        headers: Optional[Dict[str, Any]] = None,
        params: Optional[Dict[str, Any]] = None,
        **req_kwargs: Any,
    ) -> Any:
        """
        Execute **one** REST call described by an OpenAPI operation.

        Use this only *after* `get_openapi_definition` has returned a matching
        `path_template`/`method`.

        Authentication: merge `auth_header` given at agent-init time with
        any per-call `headers` argument (per-call overrides duplicates).
        """

        url = _join_url(base_url, path_template.format(**path_params))

        final_headers = (auth_header or {}).copy()
        if headers:
            final_headers.update(headers)

        # redact auth key in debug logs
        safe_hdrs = {
            k: ("***" if "auth" in k.lower() or "key" in k.lower() else v)
            for k, v in final_headers.items()
        }

        # Only convert data to JSON if we're doing a request that requires a body
        # and there's actually data to send
        body = None
        if method.upper() in ["POST", "PUT", "PATCH"] and data:
            body = json.dumps(data)

        # Add more detailed logging similar to old implementation
        logger.debug(
            "→ %s %s | headers=%s params=%s data=%s",
            method,
            url,
            safe_hdrs,
            params,
            "***" if body else None,
        )

        # For debugging purposes, similar to the old implementation
        print(f"Base Url: {base_url}")
        print(f"Requested Url: {url}")
        print(f"Requested Method: {method}")
        print(f"Requested Parameters: {params}")

        resp = requests.request(
            method, url, headers=final_headers, params=params, data=body, **req_kwargs
        )
        resp.raise_for_status()
        return resp.json()

    return open_api_call_executor

================
File: agent/patterns/react/__init__.py
================
from .base import ReActAgent

================
File: agent/patterns/react/base.py
================
import json
import logging
import textwrap
from datetime import datetime

import regex
from pydantic import ConfigDict, Field

from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union

from dapr_agents.agent import AgentBase
from dapr_agents.tool import AgentTool
from dapr_agents.types import AgentError, AssistantMessage, ChatCompletion

logger = logging.getLogger(__name__)


class ReActAgent(AgentBase):
    """
    Agent implementing the ReAct (Reasoning-Action) framework for dynamic, few-shot problem-solving by leveraging
    contextual reasoning, actions, and observations in a conversation flow.
    """

    stop_at_token: List[str] = Field(
        default=["\nObservation:"],
        description="Token(s) signaling the LLM to stop generation.",
    )
    tools: List[Union[AgentTool, Callable]] = Field(
        default_factory=list,
        description="Tools available for the agent, including final_answer.",
    )
    template_format: Literal["f-string", "jinja2"] = Field(
        default="jinja2",
        description="The format used for rendering the prompt template.",
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def construct_system_prompt(self) -> str:
        """
        Constructs a system prompt in the ReAct reasoning-action format based on the agent's attributes and tools.

        Returns:
            str: The structured system message content.
        """
        # Initialize prompt parts with the current date as the first entry
        prompt_parts = [f"# Today's date is: {datetime.now().strftime('%B %d, %Y')}"]

        # Append name if provided
        if self.name:
            prompt_parts.append("## Name\nYour name is {{name}}.")

        # Append role and goal with default values if not set
        prompt_parts.append("## Role\nYour role is {{role}}.")
        prompt_parts.append("## Goal\n{{goal}}.")

        # Append instructions if provided
        if self.instructions:
            prompt_parts.append("## Instructions\n{{instructions}}")

        # Tools section with schema details
        tools_section = "## Tools\nYou have access ONLY to the following tools:\n"
        for tool in self.tools:
            tools_section += (
                f"{tool.name}: {tool.description}. Args schema: {tool.args_schema}\n"
            )
        prompt_parts.append(
            tools_section.rstrip()
        )  # Trim any trailing newlines from tools_section

        # Additional Guidelines
        additional_guidelines = textwrap.dedent(
            """
        If you think about using tool, it must use the correct tool JSON blob format as shown below:
        ```
        {
            "name": $TOOL_NAME,
            "arguments": $INPUT
        }
        ```
        """
        ).strip()
        prompt_parts.append(additional_guidelines)

        # ReAct specific guidelines
        react_guidelines = textwrap.dedent(
            """
        ## ReAct Format
        Thought: Reflect on the current state of the conversation or task. If additional information is needed, determine if using a tool is necessary. When a tool is required, briefly explain why it is needed for the specific step at hand, and immediately follow this with an `Action:` statement to address that specific requirement. Avoid combining multiple tool requests in a single `Thought`. If no tools are needed, proceed directly to an `Answer:` statement.
        Action:
        ```
        {
            "name": $TOOL_NAME,
            "arguments": $INPUT
        }
        ```
        Observation: Describe the result of the action taken.
        ... (repeat Thought/Action/Observation as needed, but **ALWAYS proceed to a final `Answer:` statement when you have enough information**)
        Thought: I now have sufficient information to answer the initial question.
        Answer: ALWAYS proceed to a final `Answer:` statement once enough information is gathered or if the tools do not provide the necessary data.

        ### Providing a Final Answer
        Once you have enough information to answer the question OR if tools cannot provide the necessary data, respond using one of the following formats:

        1. **Direct Answer without Tools**:
        Thought: I can answer directly without using any tools. Answer: Direct answer based on previous interactions or current knowledge.

        2. **When All Needed Information is Gathered**:
        Thought: I now have sufficient information to answer the question. Answer: Complete final answer here.

        3. **If Tools Cannot Provide the Needed Information**:
        Thought: The available tools do not provide the necessary information. Answer: Explanation of limitation and relevant information if possible.

        ### Key Guidelines
        - Always Conclude with an `Answer:` statement.
        - Ensure every response ends with an `Answer:` statement that summarizes the most recent findings or relevant information, avoiding incomplete thoughts.
        - Direct Final Answer for Past or Known Information: If the user inquires about past interactions, respond directly with an Answer: based on the information in chat history.
        - Avoid Repetitive Thought Statements: If the answer is ready, skip repetitive Thought steps and proceed directly to Answer.
        - Minimize Redundant Steps: Use minimal Thought/Action/Observation cycles to arrive at a final Answer efficiently.
        - Reference Past Information When Relevant: Use chat history accurately when answering questions about previous responses to avoid redundancy.
        - Progressively Move Towards Finality: Reflect on the current step and avoid re-evaluating the entire user request each time. Aim to advance towards the final Answer in each cycle.

        ## Chat History
        The chat history is provided to avoid repeating information and to ensure accurate references when summarizing past interactions.
        """
        ).strip()
        prompt_parts.append(react_guidelines)

        return "\n\n".join(prompt_parts)

    async def run(self, input_data: Optional[Union[str, Dict[str, Any]]] = None) -> Any:
        """
        Runs the agent in a ReAct-style loop until it generates a final answer or reaches max iterations.

        Args:
            input_data (Optional[Union[str, Dict[str, Any]]]): Initial task or message input.

        Returns:
            Any: The agent's final answer.

        Raises:
            AgentError: If LLM fails or tool execution encounters issues.
        """
        logger.debug(
            f"Agent run started with input: {input_data or 'Using memory context'}"
        )

        # Format messages; construct_messages already includes chat history.
        messages = self.construct_messages(input_data or {})
        user_message = self.get_last_user_message(messages)

        # Add the new user message to memory only if input_data is provided and user message exists.
        if input_data and user_message:
            self.memory.add_message(user_message)

        # Always print the last user message for context, even if no input_data is provided
        if user_message:
            self.text_formatter.print_message(user_message)

        # Get Tool Names to validate tool selection
        available_tools = self.tool_executor.get_tool_names()

        # Initialize react_loop for iterative reasoning
        react_loop = ""

        for iteration in range(self.max_iterations):
            logger.info(f"Iteration {iteration + 1}/{self.max_iterations} started.")

            # Check if "react_loop" is already a variable in the template
            if "react_loop" in self.prompt_template.input_variables:
                # If "react_loop" exists as a variable, construct messages dynamically
                iteration_messages = self.construct_messages({"react_loop": react_loop})
            else:
                # Create a fresh copy of original_messages for this iteration
                iteration_messages = [msg.copy() for msg in messages]

                # Append react_loop to the last message (user or otherwise)
                for msg in reversed(iteration_messages):
                    if msg["role"] == "user":
                        msg["content"] += f"\n{react_loop}"
                        break
                else:
                    # Append react_loop to the last message if no user message is found
                    logger.warning(
                        "No user message found in the current messages; appending react_loop to the last message."
                    )
                    iteration_messages[-1][
                        "content"
                    ] += f"\n{react_loop}"  # Append react_loop to the last message

            try:
                response: ChatCompletion = self.llm.generate(
                    messages=iteration_messages, stop=self.stop_at_token
                )

                # Parse response into thought, action, and potential final answer
                thought_action, action, final_answer = self.parse_response(response)

                # Print Thought immediately
                self.text_formatter.print_react_part("Thought", thought_action)

                if final_answer:
                    assistant_final = AssistantMessage(final_answer)
                    self.memory.add_message(assistant_final)
                    self.text_formatter.print_separator()
                    self.text_formatter.print_message(
                        assistant_final, include_separator=False
                    )
                    logger.info("Agent provided a direct final answer.")
                    return final_answer

                # If there's no action, update the loop and continue reasoning
                if not action:
                    logger.info(
                        "No action specified; continuing with further reasoning."
                    )
                    react_loop += f"Thought:{thought_action}\n"
                    continue  # Proceed to the next iteration

                action_name = action["name"]
                action_args = action["arguments"]

                # Print Action
                self.text_formatter.print_react_part("Action", json.dumps(action))

                if action_name not in available_tools:
                    raise AgentError(f"Unknown tool specified: {action_name}")

                logger.info(f"Executing {action_name} with arguments {action_args}")
                result = await self.tool_executor.run_tool(action_name, **action_args)

                # Print Observation
                self.text_formatter.print_react_part("Observation", result)
                react_loop += f"Thought:{thought_action}\nAction:{json.dumps(action)}\nObservation:{result}\n"

            except Exception as e:
                logger.error(f"Error during ReAct agent loop: {e}")
                raise AgentError(f"ReActAgent failed: {e}") from e

        logger.info("Max iterations reached. Agent has stopped.")

    def parse_response(
        self, response: ChatCompletion
    ) -> Tuple[str, Optional[dict], Optional[str]]:
        """
        Parses a ReAct-style LLM response into a Thought, optional Action (JSON blob), and optional Final Answer.

        Args:
            response (ChatCompletion): The LLM response object containing the message content.

        Returns:
            Tuple[str, Optional[dict], Optional[str]]:
                - Thought string.
                - Parsed Action dictionary, if present.
                - Final Answer string, if present.
        """
        pattern = r"\{(?:[^{}]|(?R))*\}"  # Recursive pattern to match nested JSON blobs
        content = response.get_content()

        # Compile reusable regex patterns
        action_split_regex = regex.compile(r"action:\s*", flags=regex.IGNORECASE)
        final_answer_regex = regex.compile(
            r"answer:\s*(.*)", flags=regex.IGNORECASE | regex.DOTALL
        )
        thought_label_regex = regex.compile(r"thought:\s*", flags=regex.IGNORECASE)

        # Strip leading "Thought:" labels (they get repeated a lot)
        content = thought_label_regex.sub("", content).strip()

        # Check if there's a final answer present
        if final_match := final_answer_regex.search(content):
            final_answer = final_match.group(1).strip()
            logger.debug(f"[parse_response] Final answer detected: {final_answer}")
            return content, None, final_answer

        # Split on first "Action:" to separate Thought and Action
        if action_split_regex.search(content):
            thought_part, action_block = action_split_regex.split(content, 1)
            thought_part = thought_part.strip()
            logger.debug(f"[parse_response] Thought extracted: {thought_part}")
            logger.debug(
                f"[parse_response] Action block to parse: {action_block.strip()}"
            )
        else:
            logger.debug(
                f"[parse_response] No action or answer found. Returning content as Thought: {content}"
            )
            return content, None, None

        # Attempt to extract the first valid JSON blob from the action block
        for match in regex.finditer(pattern, action_block, flags=regex.DOTALL):
            try:
                action_dict = json.loads(match.group())
                logger.debug(
                    f"[parse_response] Successfully parsed action: {action_dict}"
                )
                return thought_part, action_dict, None
            except json.JSONDecodeError as e:
                logger.debug(
                    f"[parse_response] Failed to parse action JSON blob: {match.group()} — {e}"
                )
                continue

        logger.debug(
            "[parse_response] No valid action JSON found. Returning Thought only."
        )
        return thought_part, None, None

    async def run_tool(self, tool_name: str, *args, **kwargs) -> Any:
        """
        Executes a tool by name, resolving async or sync tools automatically.

        Args:
            tool_name (str): The name of the registered tool.
            *args: Positional arguments.
            **kwargs: Keyword arguments.

        Returns:
            Any: The tool result.

        Raises:
            AgentError: If execution fails.
        """
        try:
            return await self.tool_executor.run_tool(tool_name, *args, **kwargs)
        except Exception as e:
            logger.error(f"Failed to run tool '{tool_name}' via ReActAgent: {e}")
            raise AgentError(f"Error running tool '{tool_name}': {e}") from e

================
File: agent/patterns/toolcall/__init__.py
================
from .base import ToolCallAgent

================
File: agent/patterns/toolcall/base.py
================
from dapr_agents.types import AgentError, AssistantMessage, ChatCompletion, ToolMessage
from dapr_agents.agent import AgentBase
from typing import List, Optional, Dict, Any, Union
from pydantic import Field, ConfigDict
import logging

logger = logging.getLogger(__name__)


class ToolCallAgent(AgentBase):
    """
    Agent that manages tool calls and conversations using a language model.
    It integrates tools and processes them based on user inputs and task orchestration.
    """

    tool_history: List[ToolMessage] = Field(
        default_factory=list, description="Executed tool calls during the conversation."
    )
    tool_choice: Optional[str] = Field(
        default=None,
        description="Strategy for selecting tools ('auto', 'required', 'none'). Defaults to 'auto' if tools are provided.",
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def model_post_init(self, __context: Any) -> None:
        """
        Initialize the agent's settings, such as tool choice and parent setup.
        Sets the tool choice strategy based on provided tools.
        """
        self.tool_choice = self.tool_choice or ("auto" if self.tools else None)

        # Proceed with base model setup
        super().model_post_init(__context)

    async def run(self, input_data: Optional[Union[str, Dict[str, Any]]] = None) -> Any:
        """
        Asynchronously executes the agent's main task using the provided input or memory context.

        Args:
            input_data (Optional[Union[str, Dict[str, Any]]]): User input as string or dict.

        Returns:
            Any: The agent's final output.

        Raises:
            AgentError: If user input is invalid or tool execution fails.
        """
        logger.debug(
            f"Agent run started with input: {input_data if input_data else 'Using memory context'}"
        )

        # Format messages; construct_messages already includes chat history.
        messages = self.construct_messages(input_data or {})
        user_message = self.get_last_user_message(messages)

        if input_data and user_message:
            # Add the new user message to memory only if input_data is provided and user message exists
            self.memory.add_message(user_message)

        # Always print the last user message for context, even if no input_data is provided
        if user_message:
            self.text_formatter.print_message(user_message)

        # Process conversation iterations
        return await self.process_iterations(messages)

    async def process_response(self, tool_calls: List[dict]) -> None:
        """
        Asynchronously executes tool calls and appends tool results to memory.

        Args:
            tool_calls (List[dict]): Tool calls returned by the LLM.

        Raises:
            AgentError: If a tool execution fails.
        """
        for tool in tool_calls:
            function_name = tool.function.name
            try:
                logger.info(
                    f"Executing {function_name} with arguments {tool.function.arguments}"
                )
                result = await self.tool_executor.run_tool(
                    function_name, **tool.function.arguments_dict
                )
                tool_message = ToolMessage(
                    tool_call_id=tool.id, name=function_name, content=str(result)
                )
                self.text_formatter.print_message(tool_message)
                self.tool_history.append(tool_message)
            except Exception as e:
                logger.error(f"Error executing tool {function_name}: {e}")
                raise AgentError(f"Error executing tool '{function_name}': {e}") from e

    async def process_iterations(self, messages: List[Dict[str, Any]]) -> Any:
        """
        Iteratively drives the agent conversation until a final answer or max iterations.

        Args:
            messages (List[Dict[str, Any]]): Initial conversation messages.

        Returns:
            Any: The final assistant message.

        Raises:
            AgentError: On chat failure or tool issues.
        """
        for iteration in range(self.max_iterations):
            logger.info(f"Iteration {iteration + 1}/{self.max_iterations} started.")

            messages += self.tool_history

            try:
                response: ChatCompletion = self.llm.generate(
                    messages=messages,
                    tools=self.tools,
                    tool_choice=self.tool_choice,
                )
                response_message = response.get_message()
                self.text_formatter.print_message(response_message)

                if response.get_reason() == "tool_calls":
                    self.tool_history.append(response_message)
                    await self.process_response(response.get_tool_calls())
                else:
                    self.memory.add_message(AssistantMessage(response.get_content()))
                    self.tool_history.clear()
                    return response.get_content()
            except Exception as e:
                logger.error(f"Error during chat generation: {e}")
                raise AgentError(f"Failed during chat generation: {e}") from e

        logger.info("Max iterations reached. Agent has stopped.")

    async def run_tool(self, tool_name: str, *args, **kwargs) -> Any:
        """
        Executes a registered tool by name, automatically handling sync or async tools.

        Args:
            tool_name (str): Name of the tool to run.
            *args: Positional arguments passed to the tool.
            **kwargs: Keyword arguments passed to the tool.

        Returns:
            Any: Result from the tool execution.

        Raises:
            AgentError: If the tool is not found or execution fails.
        """
        try:
            return await self.tool_executor.run_tool(tool_name, *args, **kwargs)
        except Exception as e:
            logger.error(f"Agent failed to run tool '{tool_name}': {e}")
            raise AgentError(f"Failed to run tool '{tool_name}': {e}") from e

================
File: agent/patterns/__init__.py
================
from .react import ReActAgent
from .toolcall import ToolCallAgent
from .openapi import OpenAPIReActAgent

================
File: agent/telemetry/__init__.py
================
from .otel import DaprAgentsOTel

================
File: agent/telemetry/otel.py
================
from logging import Logger
from typing import Union

from opentelemetry._logs import set_logger_provider
from opentelemetry.metrics import set_meter_provider
from opentelemetry.sdk._logs import LoggerProvider, LoggingHandler
from opentelemetry.sdk._logs.export import BatchLogRecordProcessor
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.sdk.resources import Resource, SERVICE_NAME
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.trace import set_tracer_provider
from opentelemetry.exporter.otlp.proto.http._log_exporter import OTLPLogExporter
from opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter


class DaprAgentsOTel:
    """
    OpenTelemetry configuration for Dapr agents.
    """

    def __init__(self, service_name: str = "", otlp_endpoint: str = ""):
        # Configure OpenTelemetry
        self.service_name = service_name
        self.otlp_endpoint = otlp_endpoint

        self.setup_resources()

    def setup_resources(self):
        """
        Set up the resource for OpenTelemetry.
        """

        self._resource = Resource.create(
            attributes={
                SERVICE_NAME: str(self.service_name),
            }
        )

    def create_and_instrument_meter_provider(
        self,
        otlp_endpoint: str = "",
    ) -> MeterProvider:
        """
        Returns a `MeterProvider` that is configured to export metrics using the `PeriodicExportingMetricReader`
        which means that metrics are exported periodically in the background. The interval can be set by
        the environment variable `OTEL_METRIC_EXPORT_INTERVAL`. The default value is 60000ms (1 minute).

        Also sets the global OpenTelemetry meter provider to the returned meter provider.
        """

        # Ensure the endpoint is set correctly
        endpoint = self._endpoint_validator(
            endpoint=self.otlp_endpoint if otlp_endpoint == "" else otlp_endpoint,
            telemetry_type="metrics",
        )

        metric_exporter = OTLPMetricExporter(endpoint=str(endpoint))
        metric_reader = PeriodicExportingMetricReader(metric_exporter)
        meter_provider = MeterProvider(
            resource=self._resource, metric_readers=[metric_reader]
        )
        set_meter_provider(meter_provider)
        return meter_provider

    def create_and_instrument_tracer_provider(
        self,
        otlp_endpoint: str = "",
    ) -> TracerProvider:
        """
        Returns a `TracerProvider` that is configured to export traces using the `BatchSpanProcessor`
        which means that traces are exported in batches. The batch size can be set by
        the environment variable `OTEL_TRACES_EXPORT_BATCH_SIZE`. The default value is 512.
        Also sets the global OpenTelemetry tracer provider to the returned tracer provider.
        """

        # Ensure the endpoint is set correctly
        endpoint = self._endpoint_validator(
            endpoint=self.otlp_endpoint if otlp_endpoint == "" else otlp_endpoint,
            telemetry_type="traces",
        )

        trace_exporter = OTLPSpanExporter(endpoint=str(endpoint))
        tracer_processor = BatchSpanProcessor(trace_exporter)
        tracer_provider = TracerProvider(resource=self._resource)
        tracer_provider.add_span_processor(tracer_processor)
        set_tracer_provider(tracer_provider)
        return tracer_provider

    def create_and_instrument_logging_provider(
        self,
        logger: Logger,
        otlp_endpoint: str = "",
    ) -> LoggerProvider:
        """
        Returns a `LoggingProvider` that is configured to export logs using the `BatchLogProcessor`
        which means that logs are exported in batches. The batch size can be set by
        the environment variable `OTEL_LOGS_EXPORT_BATCH_SIZE`. The default value is 512.
        Also sets the global OpenTelemetry logging provider to the returned logging provider.
        """

        # Ensure the endpoint is set correctly
        endpoint = self._endpoint_validator(
            endpoint=self.otlp_endpoint if otlp_endpoint == "" else otlp_endpoint,
            telemetry_type="logs",
        )

        log_exporter = OTLPLogExporter(endpoint=str(endpoint))
        logging_provider = LoggerProvider(resource=self._resource)
        logging_provider.add_log_record_processor(BatchLogRecordProcessor(log_exporter))
        set_logger_provider(logging_provider)

        handler = LoggingHandler(logger_provider=logging_provider)
        logger.addHandler(handler)
        return logging_provider

    def _endpoint_validator(
        self,
        endpoint: str,
        telemetry_type: str,
    ) -> Union[str | Exception]:
        """
        Validates the endpoint and method.
        """

        if endpoint == "":
            raise ValueError(
                "OTLP endpoint must be set either in the environment variable OTEL_EXPORTER_OTLP_ENDPOINT or in the constructor."
            )
        if endpoint.startswith("https://"):
            raise NotImplementedError(
                "OTLP over HTTPS is not supported. Please use HTTP."
            )

        endpoint = (
            endpoint
            if endpoint.endswith(f"/v1/{telemetry_type}")
            else f"{endpoint}/v1/{telemetry_type}"
        )
        endpoint = endpoint if endpoint.startswith("http://") else f"http://{endpoint}"

        return endpoint

================
File: agent/utils/auth.py
================
import requests
import os


def construct_auth_headers(auth_url, grant_type="client_credentials", **kwargs):
    """
    Construct authorization headers for API requests.

    :param auth_url: The authorization URL.
    :param grant_type: The type of OAuth grant (default is 'client_credentials').
    :param kwargs: Additional parameters for the POST request body.

    :return: A dictionary containing the Authorization header.
    """

    # Define default parameters based on the grant_type
    data = {
        "grant_type": grant_type,
    }

    # Defaults for client_credentials grant type
    if grant_type == "client_credentials":
        data.update(
            {
                "client_id": kwargs.get("client_id", os.getenv("CLIENT_ID")),
                "client_secret": kwargs.get(
                    "client_secret", os.getenv("CLIENT_SECRET")
                ),
            }
        )

    # Add any additional data passed in kwargs
    data.update(kwargs)

    # POST request to obtain the access token
    auth_response = requests.post(auth_url, data=data)

    # Check if the response was successful
    auth_response.raise_for_status()

    # Convert the response to JSON
    auth_response_data = auth_response.json()

    # Extract the access token
    access_token = auth_response_data.get("access_token")

    if not access_token:
        raise ValueError("No access token found in the response")

    return {"Authorization": f"Bearer {access_token}"}

================
File: agent/utils/factory.py
================
from dapr_agents.agent.patterns import ReActAgent, ToolCallAgent, OpenAPIReActAgent
from dapr_agents.tool.utils.openapi import OpenAPISpecParser
from dapr_agents.memory import ConversationListMemory
from dapr_agents.llm import OpenAIChatClient
from dapr_agents.agent.base import AgentBase
from dapr_agents.llm import LLMClientBase
from dapr_agents.memory import MemoryBase
from dapr_agents.tool import AgentTool
from typing import Optional, List, Union, Type, TypeVar

T = TypeVar("T", ToolCallAgent, ReActAgent, OpenAPIReActAgent)


class AgentFactory:
    """
    Returns agent classes based on the provided pattern.
    """

    AGENT_PATTERNS = {
        "react": ReActAgent,
        "toolcalling": ToolCallAgent,
        "openapireact": OpenAPIReActAgent,
    }

    @staticmethod
    def create_agent_class(pattern: str) -> Type[T]:
        """
        Selects the agent class based on the pattern.

        Args:
            pattern (str): Pattern type ('react', 'toolcalling', 'openapireact').

        Returns:
            Type: Corresponding agent class.

        Raises:
            ValueError: If the pattern is unsupported.
        """
        pattern = pattern.lower()
        agent_class = AgentFactory.AGENT_PATTERNS.get(pattern)
        if not agent_class:
            raise ValueError(f"Unsupported agent pattern: {pattern}")
        return agent_class


class Agent(AgentBase):
    """
    Dynamically creates an agent instance based on the specified pattern.
    """

    def __new__(
        cls,
        role: str,
        name: Optional[str] = None,
        pattern: str = "toolcalling",
        llm: Optional[LLMClientBase] = None,
        memory: Optional[MemoryBase] = None,
        tools: Optional[List[AgentTool]] = [],
        **kwargs,
    ) -> Union[ToolCallAgent, ReActAgent, OpenAPIReActAgent]:
        """
        Creates and returns an instance of the selected agent class.

        Args:
            role (str): Agent role.
            name (Optional[str]): Agent name.
            pattern (str): Agent pattern to use.
            llm (Optional[LLMClientBase]): LLM client for generating responses.
            memory (Optional[MemoryBase]): Memory for conversation history.
            tools (Optional[List[AgentTool]]): List of tools for task execution.

        Returns:
            Union[ToolCallAgent, ReActAgent, OpenAPIReActAgent]: The initialized agent instance.
        """
        agent_class = AgentFactory.create_agent_class(pattern)

        # Lazy initialization
        llm = llm or OpenAIChatClient()
        memory = memory or ConversationListMemory()

        if pattern == "openapireact":
            kwargs.update(
                {
                    "spec_parser": kwargs.get("spec_parser", OpenAPISpecParser()),
                    "auth_header": kwargs.get("auth_header", {}),
                }
            )

        instance = super().__new__(agent_class)
        agent_class.__init__(
            instance,
            role=role,
            name=name,
            llm=llm,
            memory=memory,
            tools=tools,
            **kwargs,
        )
        return instance

================
File: agent/utils/message_converter.py
================
from dapr_agents.types import BaseMessage
from typing import List
from pydantic import ValidationError


def messages_to_string(messages: List[BaseMessage]) -> str:
    """
    Converts messages into a single string with roles and content.

    Args:
        messages (List[BaseMessage]): List of message objects to convert.

    Returns:
        str: A formatted string representing the chat history.

    Raises:
        ValueError: If a message has an unknown role or is missing required fields.
    """
    valid_roles = {"user", "assistant", "system", "tool"}

    def format_message(message):
        if isinstance(message, dict):
            message = BaseMessage(**message)
        elif not isinstance(message, BaseMessage):
            raise ValueError(f"Invalid message type: {type(message)}")

        role = message.role
        content = message.content

        if role not in valid_roles:
            raise ValueError(f"Unknown role in chat history: {role}")

        return f"{role.capitalize()}: {content}"

    try:
        formatted_history = [format_message(message) for message in messages]
    except (ValidationError, ValueError) as e:
        raise ValueError(f"Invalid message in chat history. Error: {e}")

    return "\n".join(formatted_history)

================
File: agent/utils/text_printer.py
================
from dapr_agents.types.message import BaseMessage
from typing import Optional, Any, Union, Dict
from colorama import Style

# Define your custom colors as a dictionary
COLORS = {
    "dapr_agents_teal": "\033[38;2;147;191;183m",
    "dapr_agents_mustard": "\033[38;2;242;182;128m",
    "dapr_agents_red": "\033[38;2;217;95;118m",
    "dapr_agents_pink": "\033[38;2;191;69;126m",
    "dapr_agents_purple": "\033[38;2;146;94;130m",
    "reset": Style.RESET_ALL,
}


class ColorTextFormatter:
    """
    A flexible text formatter class to print colored text dynamically.
    Supports custom colors and text structures.
    """

    def __init__(self, default_color: Optional[str] = "reset"):
        """
        Initialize the formatter with a default color.

        Args:
            default_color (Optional[str]): Default color to use for text. Defaults to reset.
        """
        self.default_color = COLORS.get(default_color, COLORS["reset"])

    def format_text(self, text: str, color: Optional[str] = None) -> str:
        """
        Format text with the specified color.

        Args:
            text (str): The text to be formatted.
            color (Optional[str]): The color to apply (by name). Defaults to the default color.

        Returns:
            str: Colored text.
        """
        color_code = COLORS.get(color, self.default_color)
        return f"{color_code}{text}{COLORS['reset']}"

    def print_colored_text(self, text_blocks: list[tuple[str, Optional[str]]]):
        """
        Print multiple blocks of text in specified colors dynamically, ensuring that newlines
        are handled correctly.

        Args:
            text_blocks (list[tuple[str, Optional[str]]]): A list of text and color name pairs.
        """
        for text, color in text_blocks:
            # Split the text by \n to handle each line separately
            lines = text.split("\n")
            for i, line in enumerate(lines):
                formatted_line = self.format_text(line, color)
                print(formatted_line, end="\n" if i < len(lines) - 1 else "")

        print(COLORS["reset"])  # Ensure terminal color is reset at the end

    def print_separator(self):
        """
        Prints a separator line.
        """
        separator = "-" * 80
        self.print_colored_text([(f"\n{separator}\n", "reset")])

    def print_message(
        self,
        message: Union[BaseMessage, Dict[str, Any]],
        include_separator: bool = True,
    ):
        """
        Prints messages with colored formatting based on the role and message content.

        Args:
            message (Union[BaseMessage, Dict[str, Any]]): The message content, either as a BaseMessage object or
                                                        a dictionary. If a BaseMessage is provided, it will be
                                                        converted to a dictionary using its `model_dump` method.
            include_separator (bool): Whether to include a separator line after the message. Defaults to True.
        """
        # If message is a BaseMessage object, convert it to dict
        if isinstance(message, BaseMessage):
            message = message.model_dump()

        role = message.get("role", "unknown")
        name = message.get("name")

        # Format role as "role(name)" if name exists, otherwise just "role"
        formatted_role = f"{name}({role})" if name else role

        content = message.get("content", "")

        color_map = {
            "user": "dapr_agents_mustard",
            "assistant": "dapr_agents_teal",
            "tool_calls": "dapr_agents_red",
            "tool": "dapr_agents_pink",
        }

        # Handle tool calls
        if "tool_calls" in message and message["tool_calls"]:
            tool_calls = message["tool_calls"]
            for tool_call in tool_calls:
                function_name = tool_call["function"]["name"]
                arguments = tool_call["function"]["arguments"]
                tool_id = tool_call["id"]
                tool_call_text = [
                    (f"{formatted_role}:\n", color_map["tool_calls"]),
                    (
                        f"Function name: {function_name} (Call Id: {tool_id})\n",
                        color_map["tool_calls"],
                    ),
                    (f"Arguments: {arguments}", color_map["tool_calls"]),
                ]
                self.print_colored_text(tool_call_text)
                if include_separator:
                    self.print_separator()

        elif role == "tool":
            # Handle tool messages
            tool_call_id = message.get("tool_call_id", "Unknown")
            tool_message_text = [
                (f"{formatted_role} (Id: {tool_call_id}):\n", color_map["tool"]),
                (f"{content}", color_map["tool"]),
            ]
            self.print_colored_text(tool_message_text)
            if include_separator:
                self.print_separator()

        else:
            # Handle regular user or assistant messages
            regular_message_text = [
                (f"{formatted_role}:\n", color_map.get(role, "reset")),
                (f"{content}", color_map.get(role, "reset")),
            ]
            self.print_colored_text(regular_message_text)
            if include_separator:
                self.print_separator()

    def print_react_part(self, part_type: str, content: str):
        """
        Prints a part of the ReAct loop (Thought, Action, Observation) with the corresponding color.

        Args:
            part_type (str): The part of the loop being printed (e.g., 'Thought', 'Action', 'Observation').
            content (str): The content to print.
        """
        color_map = {
            "Thought": "dapr_agents_red",
            "Action": "dapr_agents_pink",
            "Observation": "dapr_agents_purple",
        }

        # Get the color for the part type, defaulting to reset if not found
        color = color_map.get(part_type, "reset")

        # Print the part with the specified color
        self.print_colored_text([(f"{part_type}: {content}", color)])

================
File: agent/__init__.py
================
from .base import AgentBase
from .utils.factory import Agent
from .actor import AgentActor
from .patterns import ReActAgent, ToolCallAgent, OpenAPIReActAgent

================
File: agent/base.py
================
from dapr_agents.memory import (
    MemoryBase,
    ConversationListMemory,
    ConversationVectorMemory,
)
from dapr_agents.agent.utils.text_printer import ColorTextFormatter
from dapr_agents.types import MessageContent, MessagePlaceHolder
from dapr_agents.tool.executor import AgentToolExecutor
from dapr_agents.prompt.base import PromptTemplateBase
from dapr_agents.llm import LLMClientBase, OpenAIChatClient
from dapr_agents.prompt import ChatPromptTemplate
from dapr_agents.tool.base import AgentTool
from typing import List, Optional, Dict, Any, Union, Callable, Literal
from pydantic import BaseModel, Field, PrivateAttr, model_validator, ConfigDict
from abc import ABC, abstractmethod
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class AgentBase(BaseModel, ABC):
    """
    Base class for agents that interact with language models and manage tools for task execution.
    """

    name: Optional[str] = Field(
        default=None,
        description="The agent's name, defaulting to the role if not provided.",
    )
    role: Optional[str] = Field(
        default="Assistant",
        description="The agent's role in the interaction (e.g., 'Weather Expert').",
    )
    goal: Optional[str] = Field(
        default="Help humans",
        description="The agent's main objective (e.g., 'Provide Weather information').",
    )
    instructions: Optional[List[str]] = Field(
        default=None, description="Instructions guiding the agent's tasks."
    )
    system_prompt: Optional[str] = Field(
        default=None,
        description="A custom system prompt, overriding name, role, goal, and instructions.",
    )
    llm: LLMClientBase = Field(
        default_factory=OpenAIChatClient,
        description="Language model client for generating responses.",
    )
    prompt_template: Optional[PromptTemplateBase] = Field(
        default=None, description="The prompt template for the agent."
    )
    tools: List[Union[AgentTool, Callable]] = Field(
        default_factory=list,
        description="Tools available for the agent to assist with tasks.",
    )
    max_iterations: int = Field(
        default=10, description="Max iterations for conversation cycles."
    )
    memory: MemoryBase = Field(
        default_factory=ConversationListMemory,
        description="Handles conversation history and context storage.",
    )
    template_format: Literal["f-string", "jinja2"] = Field(
        default="jinja2",
        description="The format used for rendering the prompt template.",
    )

    # Private attributes
    _tool_executor: AgentToolExecutor = PrivateAttr()
    _text_formatter: ColorTextFormatter = PrivateAttr(
        default_factory=ColorTextFormatter
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @model_validator(mode="before")
    def set_name_from_role(cls, values: dict):
        # Set name to role if name is not provided
        if not values.get("name") and values.get("role"):
            values["name"] = values["role"]
        return values

    @property
    def tool_executor(self) -> AgentToolExecutor:
        """Returns the tool executor, ensuring it's accessible but read-only."""
        return self._tool_executor

    @property
    def text_formatter(self) -> ColorTextFormatter:
        """Returns the text formatter for the agent."""
        return self._text_formatter

    @property
    def chat_history(self, task: str = None) -> List[MessageContent]:
        """
        Retrieves the chat history from memory based on the memory type.

        Args:
            task (str): The task or query provided by the user.

        Returns:
            List[MessageContent]: The chat history.
        """
        if isinstance(self.memory, ConversationVectorMemory) and task:
            query_embeddings = self.memory.vector_store.embed_documents([task])
            return self.memory.get_messages(query_embeddings=query_embeddings)
        return self.memory.get_messages()

    @abstractmethod
    def run(self, input_data: Union[str, Dict[str, Any]]) -> Any:
        """
        Executes the agent's main logic based on provided inputs.

        Args:
            inputs (Dict[str, Any]): A dictionary with dynamic input values for task execution.
        """
        pass

    def model_post_init(self, __context: Any) -> None:
        """
        Sets up the prompt template based on system_prompt or attributes like name, role, goal, and instructions.
        Confirms the source of prompt_template post-initialization.
        """
        # Initialize tool executor with provided tools
        self._tool_executor = AgentToolExecutor(tools=self.tools)

        # Check if both agent and LLM have a prompt template specified and raise an error if both exist
        if self.prompt_template and self.llm.prompt_template:
            raise ValueError(
                "Conflicting prompt templates: both an agent prompt_template and an LLM prompt_template are provided. "
                "Please set only one or ensure synchronization between the two."
            )

        # If the agent's prompt_template is provided, use it and skip further configuration
        if self.prompt_template:
            logger.info(
                "Using the provided agent prompt_template. Skipping system prompt construction."
            )
            self.llm.prompt_template = self.prompt_template

        # If the LLM client already has a prompt template, sync it and prefill/validate as needed
        elif self.llm.prompt_template:
            logger.info("Using existing LLM prompt_template. Synchronizing with agent.")
            self.prompt_template = self.llm.prompt_template

        else:
            if not self.system_prompt:
                logger.info("Constructing system_prompt from agent attributes.")
                self.system_prompt = self.construct_system_prompt()

            logger.info("Using system_prompt to create the prompt template.")
            self.prompt_template = self.construct_prompt_template()

        # Pre-fill Agent Attributes if needed
        self.prefill_agent_attributes()

        if not self.llm.prompt_template:
            # Assign the prompt template to the LLM client
            self.llm.prompt_template = self.prompt_template

        # Complete post-initialization
        super().model_post_init(__context)

    def prefill_agent_attributes(self) -> None:
        """
        Pre-fill prompt template with agent attributes if specified in `input_variables`.
        Logs any agent attributes set but not used by the template.
        """
        # Start with a dictionary for attributes
        prefill_data = {}

        # Check if each attribute is defined in input_variables before adding
        if "name" in self.prompt_template.input_variables and self.name:
            prefill_data["name"] = self.name

        if "role" in self.prompt_template.input_variables:
            prefill_data["role"] = self.role

        if "goal" in self.prompt_template.input_variables:
            prefill_data["goal"] = self.goal

        if "instructions" in self.prompt_template.input_variables and self.instructions:
            prefill_data["instructions"] = "\n".join(self.instructions)

        # Collect attributes set but not in input_variables for informational logging
        set_attributes = {
            "name": self.name,
            "role": self.role,
            "goal": self.goal,
            "instructions": self.instructions,
        }

        # Use Pydantic's model_fields_set to detect if attributes were user-set
        user_set_attributes = {
            attr for attr in set_attributes if attr in self.model_fields_set
        }

        ignored_attributes = [
            attr
            for attr in set_attributes
            if attr not in self.prompt_template.input_variables
            and set_attributes[attr] is not None
            and attr in user_set_attributes
        ]

        # Apply pre-filled data only for attributes that are in input_variables
        if prefill_data:
            self.prompt_template = self.prompt_template.pre_fill_variables(
                **prefill_data
            )
            logger.info(
                f"Pre-filled prompt template with attributes: {list(prefill_data.keys())}"
            )
        elif ignored_attributes:
            raise ValueError(
                f"The following agent attributes were explicitly set by the user but are not considered by the prompt template: {', '.join(ignored_attributes)}. "
                "Please ensure that these attributes are included in the prompt template's input variables if they are needed."
            )
        else:
            logger.info(
                "No agent attributes were pre-filled, as the template did not require any."
            )

    def construct_system_prompt(self) -> str:
        """
        Constructs a system prompt with agent attributes like `name`, `role`, `goal`, and `instructions`.
        Sets default values for `role` and `goal` if not provided.

        Returns:
            str: A system prompt template string.
        """
        # Initialize prompt parts with the current date as the first entry
        prompt_parts = [f"# Today's date is: {datetime.now().strftime('%B %d, %Y')}"]

        # Append name if provided
        if self.name:
            prompt_parts.append("## Name\nYour name is {{name}}.")

        # Append role and goal with default values if not set
        prompt_parts.append("## Role\nYour role is {{role}}.")
        prompt_parts.append("## Goal\n{{goal}}.")

        # Append instructions if provided
        if self.instructions:
            prompt_parts.append("## Instructions\n{{instructions}}")

        return "\n\n".join(prompt_parts)

    def construct_prompt_template(self) -> ChatPromptTemplate:
        """
        Constructs a ChatPromptTemplate that includes the system prompt and a placeholder for chat history.
        Ensures that the template is flexible and adaptable to dynamically handle pre-filled variables.

        Returns:
            ChatPromptTemplate: A formatted prompt template for the agent.
        """
        # Construct the system prompt if not provided
        system_prompt = self.system_prompt or self.construct_system_prompt()

        # Create the template with placeholders for system message and chat history
        return ChatPromptTemplate.from_messages(
            messages=[
                ("system", system_prompt),
                MessagePlaceHolder(variable_name="chat_history"),
            ],
            template_format=self.template_format,
        )

    def construct_messages(
        self, input_data: Union[str, Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Constructs and formats initial messages based on input type, pre-filling chat history as needed.

        Args:
            input_data (Union[str, Dict[str, Any]]): User input, either as a string or dictionary.

        Returns:
            List[Dict[str, Any]]: List of formatted messages, including the user message if input_data is a string.
        """
        # Pre-fill chat history in the prompt template
        chat_history = self.memory.get_messages()
        self.pre_fill_prompt_template(**{"chat_history": chat_history})

        # Handle string input by adding a user message
        if isinstance(input_data, str):
            formatted_messages = self.prompt_template.format_prompt()
            user_message = {"role": "user", "content": input_data}
            return formatted_messages + [user_message]

        # Handle dictionary input as dynamic variables for the template
        elif isinstance(input_data, dict):
            # Pass the dictionary directly, assuming it contains keys expected by the prompt template
            formatted_messages = self.prompt_template.format_prompt(**input_data)
            return formatted_messages

        else:
            raise ValueError("Input data must be either a string or dictionary.")

    def reset_memory(self):
        """Clears all messages stored in the agent's memory."""
        self.memory.reset_memory()

    def get_last_message(self) -> Optional[MessageContent]:
        """
        Retrieves the last message from the chat history.

        Returns:
            Optional[MessageContent]: The last message in the history, or None if none exist.
        """
        chat_history = self.chat_history
        return chat_history[-1] if chat_history else None

    def get_last_user_message(
        self, messages: List[Dict[str, Any]]
    ) -> Optional[MessageContent]:
        """
        Retrieves the last user message in a list of messages.

        Args:
            messages (List[Dict[str, Any]]): List of formatted messages to search.

        Returns:
            Optional[MessageContent]: The last user message with trimmed content, or None if no user message exists.
        """
        # Iterate in reverse to find the most recent 'user' role message
        for message in reversed(messages):
            if message.get("role") == "user":
                # Trim the content of the user message
                message["content"] = message["content"].strip()
                return message
        return None

    def pre_fill_prompt_template(self, **kwargs: Union[str, Callable[[], str]]) -> None:
        """
        Pre-fills the prompt template with specified variables, updating input variables if applicable.

        Args:
            **kwargs: Variables to pre-fill in the prompt template. These can be strings or callables
                    that return strings.

        Notes:
            - Existing pre-filled variables will be overwritten by matching keys in `kwargs`.
            - This method does not affect the `chat_history` which is dynamically updated.
        """
        if not self.prompt_template:
            raise ValueError(
                "Prompt template must be initialized before pre-filling variables."
            )

        self.prompt_template = self.prompt_template.pre_fill_variables(**kwargs)
        logger.debug(f"Pre-filled prompt template with variables: {kwargs.keys()}")

================
File: document/embedder/__init__.py
================
from .openai import OpenAIEmbedder
from .sentence import SentenceTransformerEmbedder
from .nvidia import NVIDIAEmbedder

================
File: document/embedder/base.py
================
from abc import ABC, abstractmethod
from pydantic import BaseModel
from typing import List, Any


class EmbedderBase(BaseModel, ABC):
    """
    Abstract base class for Embedders.
    """

    @abstractmethod
    def embed(self, query: str, **kwargs) -> List[Any]:
        """
        Search for content based on a query.

        Args:
            query (str): The search query.
            **kwargs: Additional search parameters.

        Returns:
            List[Any]: A list of results.
        """
        pass

================
File: document/embedder/nvidia.py
================
from dapr_agents.llm.nvidia.embeddings import NVIDIAEmbeddingClient
from dapr_agents.document.embedder.base import EmbedderBase
from typing import List, Union
from pydantic import Field
import numpy as np
import logging

logger = logging.getLogger(__name__)


class NVIDIAEmbedder(NVIDIAEmbeddingClient, EmbedderBase):
    """
    NVIDIA-based embedder for generating text embeddings with support for indexing (passage) and querying.
    Inherits functionality from NVIDIAEmbeddingClient for API interactions.

    Attributes:
        chunk_size (int): Batch size for embedding requests. Defaults to 1000.
        normalize (bool): Whether to normalize embeddings. Defaults to True.
    """

    chunk_size: int = Field(
        default=1000, description="Batch size for embedding requests."
    )
    normalize: bool = Field(
        default=True, description="Whether to normalize embeddings."
    )

    def embed(
        self, input: Union[str, List[str]]
    ) -> Union[List[float], List[List[float]]]:
        """
        Embeds input text(s) for indexing with default input_type set to 'passage'.

        Args:
            input (Union[str, List[str]]): Input text(s) to embed. Can be a single string or a list of strings.

        Returns:
            Union[List[float], List[List[float]]]: Embedding vector(s) for the input(s).
                - Returns a single list of floats for a single string input.
                - Returns a list of lists of floats for a list of string inputs.

        Raises:
            ValueError: If input is invalid or embedding generation fails.
        """
        return self._generate_embeddings(input, input_type="passage")

    def embed_query(
        self, input: Union[str, List[str]]
    ) -> Union[List[float], List[List[float]]]:
        """
        Embeds input text(s) for querying with input_type set to 'query'.

        Args:
            input (Union[str, List[str]]): Input text(s) to embed. Can be a single string or a list of strings.

        Returns:
            Union[List[float], List[List[float]]]: Embedding vector(s) for the input(s).
                - Returns a single list of floats for a single string input.
                - Returns a list of lists of floats for a list of string inputs.

        Raises:
            ValueError: If input is invalid or embedding generation fails.
        """
        return self._generate_embeddings(input, input_type="query")

    def _generate_embeddings(
        self, input: Union[str, List[str]], input_type: str
    ) -> Union[List[float], List[List[float]]]:
        """
        Helper function to generate embeddings for given input text(s) with specified input_type.

        Args:
            input (Union[str, List[str]]): Input text(s) to embed.
            input_type (str): The type of embedding operation ('query' or 'passage').

        Returns:
            Union[List[float], List[List[float]]]: Embedding vector(s) for the input(s).
        """
        # Validate input
        if not input or (isinstance(input, list) and all(not q for q in input)):
            raise ValueError("Input must contain valid text.")

        single_input = isinstance(input, str)
        input_list = [input] if single_input else input

        # Process input in chunks for efficiency
        chunk_embeddings = []
        for i in range(0, len(input_list), self.chunk_size):
            batch = input_list[i : i + self.chunk_size]
            response = self.create_embedding(input=batch, input_type=input_type)
            chunk_embeddings.extend(r.embedding for r in response.data)

        # Normalize embeddings if required
        if self.normalize:
            normalized_embeddings = [
                (embedding / np.linalg.norm(embedding)).tolist()
                for embedding in chunk_embeddings
            ]
        else:
            normalized_embeddings = chunk_embeddings

        # Return a single embedding if the input was a single string; otherwise, return a list
        return normalized_embeddings[0] if single_input else normalized_embeddings

    def __call__(
        self, input: Union[str, List[str]], query: bool = False
    ) -> Union[List[float], List[List[float]]]:
        """
        Allows the instance to be called directly to embed text(s).

        Args:
            input (Union[str, List[str]]): The input text(s) to embed.
            query (bool): If True, embeds for querying (input_type='query'). Otherwise, embeds for indexing (input_type='passage').

        Returns:
            Union[List[float], List[List[float]]]: Embedding vector(s) for the input(s).
        """
        if query:
            return self.embed_query(input)
        return self.embed(input)

================
File: document/embedder/openai.py
================
from dapr_agents.document.embedder.base import EmbedderBase
from dapr_agents.llm.openai.embeddings import OpenAIEmbeddingClient
from typing import List, Any, Union, Optional
from pydantic import Field, ConfigDict
import numpy as np
import logging

logger = logging.getLogger(__name__)


class OpenAIEmbedder(OpenAIEmbeddingClient, EmbedderBase):
    """
    OpenAI-based embedder for generating text embeddings with handling for long inputs.
    Inherits functionality from OpenAIEmbeddingClient for API interactions.
    """

    max_tokens: int = Field(
        default=8191, description="Maximum tokens allowed per input."
    )
    chunk_size: int = Field(
        default=1000, description="Batch size for embedding requests."
    )
    normalize: bool = Field(
        default=True, description="Whether to normalize embeddings."
    )
    encoding_name: Optional[str] = Field(
        default=None, description="Token encoding name (if provided)."
    )
    encoder: Optional[Any] = Field(
        default=None, init=False, description="TikToken Encoder"
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def model_post_init(self, __context: Any) -> None:
        """
        Initialize attributes after model validation.
        Automatically determines the appropriate encoding for the model.
        """
        super().model_post_init(__context)

        try:
            import tiktoken
            from tiktoken.core import Encoding
        except ImportError:
            raise ImportError(
                "The `tiktoken` library is required for tokenizing inputs. "
                "Install it using `pip install tiktoken`."
            )

        if self.encoding_name:
            # Use the explicitly provided encoding
            self.encoder: Encoding = tiktoken.get_encoding(self.encoding_name)
        else:
            # Automatically determine encoding based on model
            try:
                self.encoder: Encoding = tiktoken.encoding_for_model(self.model)
            except KeyError:
                # Fallback to default encoding and model
                logger.warning(
                    f"Model '{self.model}' not recognized. "
                    "Defaulting to 'cl100k_base' encoding and 'text-embedding-ada-002' model."
                )
                self.encoder = tiktoken.get_encoding("cl100k_base")
                self.model = "text-embedding-ada-002"

    def _tokenize_text(self, text: str) -> List[int]:
        """Tokenizes the input text using the specified encoding."""
        return self.encoder.encode(text)

    def _chunk_tokens(self, tokens: List[int], chunk_length: int) -> List[List[int]]:
        """Splits tokens into chunks of the specified length."""
        return [
            tokens[i : i + chunk_length] for i in range(0, len(tokens), chunk_length)
        ]

    def _process_embeddings(
        self, embeddings: List[List[float]], weights: List[int]
    ) -> List[float]:
        """Combines embeddings using weighted averaging."""
        weighted_avg = np.average(embeddings, axis=0, weights=weights)
        if self.normalize:
            norm = np.linalg.norm(weighted_avg)
            return (weighted_avg / norm).tolist()
        return weighted_avg.tolist()

    def embed(
        self, input: Union[str, List[str]]
    ) -> Union[List[float], List[List[float]]]:
        """
        Embeds input text(s) with support for both single and multiple inputs, handling long texts via chunking and batching.

        Args:
            input (Union[str, List[str]]): The input text(s) to embed. Can be a single string or a list of strings.

        Returns:
            Union[List[float], List[List[float]]]: Embedding vector(s) for the input(s).
                - Returns a single list of floats for a single string input.
                - Returns a list of lists of floats for a list of string inputs.

        Notes:
            - Handles long inputs by chunking them into smaller parts based on `max_tokens` and reassembling embeddings.
            - Batches API calls for efficiency using `chunk_size`.
            - Automatically combines chunk embeddings using weighted averaging for long inputs.
        """
        # Validate input
        if not input or (isinstance(input, list) and all(not q for q in input)):
            raise ValueError("Input must contain valid text.")

        # Check if the input is a single string or a list of strings
        single_input = isinstance(input, str)
        input_strings = [input] if single_input else input

        # Tokenize the input strings to check for long texts requiring chunking
        tokenized_inputs = [self._tokenize_text(q) for q in input_strings]
        chunks = []  # Holds text chunks for API calls
        chunk_indices = []  # Maps each chunk to its original input index

        # Handle tokenized inputs: Chunk long inputs and map chunks to their respective inputs
        for idx, tokens in enumerate(tokenized_inputs):
            if len(tokens) <= self.max_tokens:
                # Directly use the text if it's within max token limits
                chunks.append(self.encoder.decode(tokens))
                chunk_indices.append(idx)
            else:
                # Split long inputs into smaller chunks
                token_chunks = self._chunk_tokens(tokens, self.max_tokens)
                chunks.extend([self.encoder.decode(chunk) for chunk in token_chunks])
                chunk_indices.extend([idx] * len(token_chunks))

        # Process the chunks in batches for efficiency
        batch_size = self.chunk_size
        chunk_embeddings = []  # Holds embeddings for all chunks

        for i in range(0, len(chunks), batch_size):
            batch = chunks[i : i + batch_size]
            response = self.create_embedding(input=batch)  # Batch API call
            chunk_embeddings.extend(r.embedding for r in response.data)

        # Group chunk embeddings by their original query indices
        grouped_embeddings = [[] for _ in range(len(input_strings))]
        for idx, embedding in zip(chunk_indices, chunk_embeddings):
            grouped_embeddings[idx].append(embedding)

        # Combine chunk embeddings for each query
        results = []
        for embeddings, tokens in zip(grouped_embeddings, tokenized_inputs):
            if len(embeddings) == 1:
                # If only one chunk, use its embedding directly
                results.append(embeddings[0])
            else:
                # Combine chunk embeddings using weighted averaging
                weights = [
                    len(chunk) for chunk in self._chunk_tokens(tokens, self.max_tokens)
                ]
                results.append(self._process_embeddings(embeddings, weights))

        # Return a single embedding if the input was a single string; otherwise, return a list
        return results[0] if single_input else results

    def __call__(
        self, input: Union[str, List[str]]
    ) -> Union[List[float], List[List[float]]]:
        """
        Allows the instance to be called directly to embed text(s).

        Args:
            input (Union[str, List[str]]): The input text(s) to embed.

        Returns:
            Union[List[float], List[List[float]]]: Embedding vector(s) for the input(s).
        """
        return self.embed(input)

================
File: document/embedder/sentence.py
================
from dapr_agents.document.embedder.base import EmbedderBase
from typing import List, Any, Optional, Union, Literal
from pydantic import Field
import logging
import os

logger = logging.getLogger(__name__)


class SentenceTransformerEmbedder(EmbedderBase):
    """
    SentenceTransformer-based embedder for generating text embeddings.
    Supports multi-process encoding for large datasets.
    """

    model: str = Field(
        default="all-MiniLM-L6-v2",
        description="Name of the SentenceTransformer model to use.",
    )
    device: Literal["cpu", "cuda", "mps", "npu"] = Field(
        default="cpu", description="Device for computation."
    )
    normalize_embeddings: bool = Field(
        default=False, description="Whether to normalize embeddings."
    )
    multi_process: bool = Field(
        default=False, description="Whether to use multi-process encoding."
    )
    cache_dir: Optional[str] = Field(
        default=None, description="Directory to cache or load the model."
    )

    client: Optional[Any] = Field(
        default=None, init=False, description="Loaded SentenceTransformer model."
    )

    def model_post_init(self, __context: Any) -> None:
        """
        Initialize the SentenceTransformer model after validation.
        """
        super().model_post_init(__context)

        try:
            from sentence_transformers import SentenceTransformer
        except ImportError:
            raise ImportError(
                "The `sentence-transformers` library is required for this embedder. "
                "Install it using `pip install sentence-transformers`."
            )

        # Determine whether to load from cache or download
        model_path = (
            self.cache_dir
            if self.cache_dir and os.path.exists(self.cache_dir)
            else self.model
        )
        # Attempt to load the model
        try:
            if os.path.exists(model_path):
                logger.info(
                    f"Loading SentenceTransformer model from local path: {model_path}"
                )
            else:
                logger.info(f"Downloading SentenceTransformer model: {self.model}")
                if self.cache_dir:
                    logger.info(f"Model will be cached to: {self.cache_dir}")
            self.client: SentenceTransformer = SentenceTransformer(
                model_name_or_path=model_path, device=self.device
            )
            logger.info("Model loaded successfully.")
        except Exception as e:
            logger.error(f"Failed to load SentenceTransformer model: {e}")
            raise
        # Save to cache directory if downloaded
        if (
            model_path == self.model
            and self.cache_dir
            and not os.path.exists(self.cache_dir)
        ):
            logger.info(f"Saving the downloaded model to: {self.cache_dir}")
            self.client.save(self.cache_dir)

    def embed(
        self, input: Union[str, List[str]]
    ) -> Union[List[float], List[List[float]]]:
        """
        Generate embeddings for input text(s).

        Args:
            input (Union[str, List[str]]): Input text(s) to embed. Can be a single string or a list of strings.

        Returns:
            Union[List[float], List[List[float]]]: Embedding vector(s) for the input(s).
                - A single embedding vector (list of floats) for a single string input.
                - A list of embedding vectors for a list of string inputs.
        """
        if not input or (isinstance(input, list) and all(not q for q in input)):
            raise ValueError("Input must contain valid text.")

        single_input = isinstance(input, str)
        input_strings = [input] if single_input else input

        logger.info(f"Generating embeddings for {len(input_strings)} input(s).")

        if self.multi_process:
            logger.info("Starting multi-process pool for encoding.")
            pool = self.client.start_multi_process_pool()

            try:
                embeddings = self.client.encode_multi_process(
                    input_strings,
                    pool=pool,
                    normalize_embeddings=self.normalize_embeddings,
                )
            finally:
                logger.info("Stopping multi-process pool.")
                self.client.stop_multi_process_pool(pool)
        else:
            embeddings = self.client.encode(
                input_strings,
                convert_to_numpy=True,
                normalize_embeddings=self.normalize_embeddings,
            )

        if single_input:
            return embeddings[0].tolist()
        return embeddings.tolist()

    def __call__(
        self, input: Union[str, List[str]]
    ) -> Union[List[float], List[List[float]]]:
        """
        Allows the instance to be called directly to embed text(s).

        Args:
            input (Union[str, List[str]]): The input text(s) to embed.

        Returns:
            Union[List[float], List[List[float]]]: Embedding vector(s) for the input(s).
        """
        return self.embed(input)

================
File: document/fetcher/__init__.py
================
from .arxiv import ArxivFetcher

================
File: document/fetcher/arxiv.py
================
from dapr_agents.document.fetcher.base import FetcherBase
from dapr_agents.types.document import Document
from typing import List, Dict, Optional, Union, Any
from datetime import datetime
from pathlib import Path
import re
import logging

logger = logging.getLogger(__name__)


class ArxivFetcher(FetcherBase):
    """
    Fetcher for interacting with the arXiv API.
    """

    max_results: int = 10
    include_full_metadata: bool = False

    def search(
        self,
        query: str,
        from_date: Union[str, datetime, None] = None,
        to_date: Union[str, datetime, None] = None,
        download: bool = False,
        dirpath: Path = Path("./"),
        include_summary: bool = False,
        **kwargs,
    ) -> Union[List[Dict], List["Document"]]:
        """
        Search for papers on arXiv and optionally download them.

        Args:
            query (str): The search query.
            from_date (Union[str, datetime, None]): Start date for the search in 'YYYYMMDD' format or as a datetime object.
            to_date (Union[str, datetime, None]): End date for the search in 'YYYYMMDD' format or as a datetime object.
            download (bool): Whether to download the papers as PDFs.
            dirpath (Path): Directory path for the downloads (used if download=True).
            include_summary (bool): Whether to include the paper summary in the returned metadata or documents. Defaults to False.
            **kwargs: Additional search parameters (e.g., sort_by).

        Returns:
            Union[List[Dict], List[Document]]: A list of metadata dictionaries if `download=True`,
            otherwise a list of `Document` objects.

        Examples:
            >>> fetcher = ArxivFetcher()
            >>> fetcher.search("quantum computing")
            # Searches for papers related to "quantum computing".

            >>> fetcher.search("machine learning", from_date="20240101", to_date="20240131")
            # Searches for papers on "machine learning" submitted in January 2024.

            >>> fetcher.search("cybersecurity", from_date=datetime(2024, 1, 1), to_date=datetime(2024, 1, 31))
            # Same as above but using datetime objects for date filtering.

            >>> fetcher.search("artificial intelligence", download=True, dirpath=Path("./downloads"))
            # Searches for papers on "artificial intelligence" and downloads the PDFs to "./downloads".
        """
        try:
            import arxiv
        except ImportError:
            raise ImportError(
                "The `arxiv` library is required to use the ArxivFetcher. "
                "Install it with `pip install arxiv`."
            )

        logger.info(f"Searching for query: {query}")

        # Enforce that both from_date and to_date are provided if one is specified
        if (from_date and not to_date) or (to_date and not from_date):
            raise ValueError(
                "Both 'from_date' and 'to_date' must be specified if one is provided."
            )

        # Add date filter if both from_date and to_date are provided
        if from_date and to_date:
            from_date_str = self._format_date(from_date)
            to_date_str = self._format_date(to_date)
            date_filter = f"submittedDate:[{from_date_str} TO {to_date_str}]"
            query = f"{query} AND {date_filter}"

        search = arxiv.Search(
            query=query,
            max_results=kwargs.get("max_results", self.max_results),
            sort_by=kwargs.get("sort_by", arxiv.SortCriterion.SubmittedDate),
            sort_order=kwargs.get("sort_order", arxiv.SortOrder.Descending),
        )
        results = list(search.results())
        logger.info(f"Found {len(results)} results for query: {query}")

        return self._process_results(results, download, dirpath, include_summary)

    def search_by_id(
        self,
        content_id: str,
        download: bool = False,
        dirpath: Path = Path("./"),
        include_summary: bool = False,
    ) -> Union[Optional[Dict], Optional[Document]]:
        """
        Search for a specific paper by its arXiv ID and optionally download it.

        Args:
            content_id (str): The arXiv ID of the paper.
            download (bool): Whether to download the paper.
            dirpath (Path): Directory path for the download (used if download=True).
            include_summary (bool): Whether to include the paper summary in the returned metadata or document. Defaults to False.

        Returns:
            Union[Optional[Dict], Optional[Document]]: Metadata dictionary if `download=True`,
            otherwise a `Document` object.

        Examples:
            >>> fetcher = ArxivFetcher()
            >>> fetcher.search_by_id("1234.5678")
            # Searches for the paper with arXiv ID "1234.5678".

            >>> fetcher.search_by_id("1234.5678", download=True, dirpath=Path("./downloads"))
            # Searches for the paper with arXiv ID "1234.5678" and downloads it to "./downloads".
        """
        try:
            import arxiv
        except ImportError:
            raise ImportError(
                "The `arxiv` library is required to use the ArxivFetcher. "
                "Install it with `pip install arxiv`."
            )

        logger.info(f"Searching for paper by ID: {content_id}")
        try:
            search = arxiv.Search(id_list=[content_id])
            result = next(search.results(), None)
            if not result:
                logger.warning(f"No result found for ID: {content_id}")
                return None

            return self._process_results([result], download, dirpath, include_summary)[
                0
            ]
        except Exception as e:
            logger.error(f"Error fetching result for ID {content_id}: {e}")
            return None

    def _process_results(
        self, results: List[Any], download: bool, dirpath: Path, include_summary: bool
    ) -> Union[List[Dict], List["Document"]]:
        """
        Process arXiv search results.

        Args:
            results (List[Any]): The list of arXiv result objects.
            download (bool): Whether to download the papers as PDFs.
            dirpath (Path): Directory path for the downloads (used if download=True).
            include_summary (bool): Whether to include the paper summary in the returned metadata or documents.

        Returns:
            Union[List[Dict], List[Document]]: A list of metadata dictionaries if `download=True`,
            otherwise a list of `Document` objects.
        """
        if download:
            metadata_list = []
            for result in results:
                file_path = self._download_result(result, dirpath)
                metadata_list.append(
                    self._format_result_metadata(
                        result, file_path=file_path, include_summary=include_summary
                    )
                )
            return metadata_list
        else:
            documents = []
            for result in results:
                metadata = self._format_result_metadata(
                    result, include_summary=include_summary
                )
                text = result.summary.strip()
                documents.append(Document(text=text, metadata=metadata))
            return documents

    def _download_result(self, result: Any, dirpath: Path) -> Optional[str]:
        """
        Download a paper from an arXiv result object.

        Args:
            result (Any): The arXiv result object.
            dirpath (Path): Directory path for the download.

        Returns:
            Optional[str]: Path to the downloaded file, or None if the download failed.
        """
        try:
            dirpath.mkdir(parents=True, exist_ok=True)
            filename = result._get_default_filename()
            file_path = dirpath / filename
            logger.info(f"Downloading paper to {file_path}")
            result.download_pdf(dirpath=str(dirpath), filename=filename)
            return str(file_path)
        except Exception as e:
            logger.error(f"Failed to download paper {result.title}: {e}")
            return None

    def _format_result_metadata(
        self,
        result: Any,
        file_path: Optional[str] = None,
        include_summary: bool = False,
    ) -> Dict:
        """
        Format metadata from an arXiv result, optionally including file path and summary.

        Args:
            result (Any): The arXiv result object.
            file_path (Optional[str]): Path to the downloaded file.
            include_summary (bool): Whether to include the summary in the metadata.

        Returns:
            Dict: A dictionary containing formatted metadata.
        """
        metadata = {
            "entry_id": result.entry_id,
            "title": result.title,
            "authors": [author.name for author in result.authors],
            "published": result.published.strftime("%Y-%m-%d"),
            "updated": result.updated.strftime("%Y-%m-%d"),
            "primary_category": result.primary_category,
            "categories": result.categories,
            "pdf_url": result.pdf_url,
            "file_path": file_path,
        }

        if self.include_full_metadata:
            metadata.update(
                {
                    "links": result.links,
                    "authors_comment": result.comment,
                    "DOI": result.doi,
                    "journal_reference": result.journal_ref,
                }
            )

        if include_summary:
            metadata["summary"] = result.summary.strip()

        return {key: value for key, value in metadata.items() if value is not None}

    def _format_date(self, date: Union[str, datetime]) -> str:
        """
        Format a date into the 'YYYYMMDDHHMM' format required by the arXiv API.

        Args:
            date (Union[str, datetime]): The date to format. Can be a string in 'YYYYMMDD' or
                'YYYYMMDDHHMM' format, or a datetime object.

        Returns:
            str: The formatted date string.

        Raises:
            ValueError: If the provided date string is not in the correct format or invalid.

        Examples:
            >>> fetcher = ArxivFetcher()
            >>> fetcher._format_date("20240101")
            '202401010000'

            >>> fetcher._format_date("202401011200")
            '202401011200'

            >>> fetcher._format_date(datetime(2024, 1, 1, 12, 0))
            '202401011200'

            >>> fetcher._format_date("invalid_date")
            # Raises ValueError: Invalid date format: invalid_date. Use 'YYYYMMDD' or 'YYYYMMDDHHMM'.
        """
        if isinstance(date, str):
            # Check if the string matches the basic format
            if not re.fullmatch(r"^\d{8}(\d{4})?$", date):
                raise ValueError(
                    f"Invalid date format: {date}. Use 'YYYYMMDD' or 'YYYYMMDDHHMM'."
                )

            # Validate that it is a real date
            try:
                if len(date) == 8:  # 'YYYYMMDD'
                    datetime.strptime(date, "%Y%m%d")
                elif len(date) == 12:  # 'YYYYMMDDHHMM'
                    datetime.strptime(date, "%Y%m%d%H%M")
            except ValueError as e:
                raise ValueError(f"Invalid date value: {date}. {str(e)}")

            return date
        elif isinstance(date, datetime):
            return date.strftime("%Y%m%d%H%M")
        else:
            raise ValueError(
                "Invalid date input. Provide a string in 'YYYYMMDD', 'YYYYMMDDHHMM' format, or a datetime object."
            )

================
File: document/fetcher/base.py
================
from abc import ABC, abstractmethod
from pydantic import BaseModel
from typing import List, Any


class FetcherBase(BaseModel, ABC):
    """
    Abstract base class for fetchers.
    """

    @abstractmethod
    def search(self, query: str, **kwargs) -> List[Any]:
        """
        Search for content based on a query.

        Args:
            query (str): The search query.
            **kwargs: Additional search parameters.

        Returns:
            List[Any]: A list of results.
        """
        pass

================
File: document/reader/pdf/__init__.py
================
from .pymupdf import PyMuPDFReader
from .pypdf import PyPDFReader

================
File: document/reader/pdf/pymupdf.py
================
from dapr_agents.document.reader.base import ReaderBase
from dapr_agents.types.document import Document
from typing import List, Dict, Optional
from pathlib import Path


class PyMuPDFReader(ReaderBase):
    """
    Reader for PDF documents using PyMuPDF.
    """

    def load(
        self, file_path: Path, additional_metadata: Optional[Dict] = None
    ) -> List[Document]:
        """
        Load content from a PDF file using PyMuPDF.

        Args:
            file_path (Path): Path to the PDF file.
            additional_metadata (Optional[Dict]): Additional metadata to include.

        Returns:
            List[Document]: A list of Document objects.
        """
        try:
            import pymupdf
        except ImportError:
            raise ImportError(
                "PyMuPDF library is not installed. Install it using `pip install pymupdf`."
            )

        file_path = str(file_path)
        doc = pymupdf.open(file_path)
        total_pages = len(doc)
        documents = []

        for page_num, page in enumerate(doc.pages):
            text = page.get_text()
            metadata = {
                "file_path": file_path,
                "page_number": page_num + 1,
                "total_pages": total_pages,
            }
            if additional_metadata:
                metadata.update(additional_metadata)

            documents.append(Document(text=text.strip(), metadata=metadata))

        doc.close()
        return documents

================
File: document/reader/pdf/pypdf.py
================
from dapr_agents.types.document import Document
from dapr_agents.document.reader.base import ReaderBase
from typing import List, Dict, Optional
from pathlib import Path


class PyPDFReader(ReaderBase):
    """
    Reader for PDF documents using PyPDF.
    """

    def load(
        self, file_path: Path, additional_metadata: Optional[Dict] = None
    ) -> List[Document]:
        """
        Load content from a PDF file using PyPDF.

        Args:
            file_path (Path): Path to the PDF file.
            additional_metadata (Optional[Dict]): Additional metadata to include.

        Returns:
            List[Document]: A list of Document objects.
        """
        try:
            from pypdf import PdfReader
        except ImportError:
            raise ImportError(
                "PyPDF library is not installed. Install it using `pip install pypdf`."
            )

        reader = PdfReader(file_path)
        total_pages = len(reader.pages)
        documents = []

        for page_num, page in enumerate(reader.pages):
            text = page.extract_text()
            metadata = {
                "file_path": str(file_path),
                "page_number": page_num + 1,
                "total_pages": total_pages,
            }
            if additional_metadata:
                metadata.update(additional_metadata)

            documents.append(Document(text=text.strip(), metadata=metadata))

        return documents

================
File: document/reader/__init__.py
================
from .pdf import PyMuPDFReader, PyPDFReader

================
File: document/reader/base.py
================
from dapr_agents.types.document import Document
from abc import ABC, abstractmethod
from pydantic import BaseModel
from pathlib import Path
from typing import List


class ReaderBase(BaseModel, ABC):
    """
    Abstract base class for file readers.
    """

    @abstractmethod
    def load(self, file_path: Path) -> List[Document]:
        """
        Load content from a file.

        Args:
            file_path (Path): Path to the file.

        Returns:
            List[Document]: A list of Document objects.
        """
        pass

================
File: document/reader/text.py
================
from dapr_agents.document.reader.base import ReaderBase
from dapr_agents.types.document import Document
from pathlib import Path
from typing import List
from pydantic import Field


class TextLoader(ReaderBase):
    """
    Loader for plain text files.

    Attributes:
        encoding (str): The text file encoding. Defaults to 'utf-8'.
    """

    encoding: str = Field(default="utf-8", description="Encoding of the text file.")

    def load(self, file_path: Path) -> List[Document]:
        """
        Load content from a plain text file.

        Args:
            file_path (Path): Path to the text file.

        Returns:
            List[Document]: A list containing one Document object.
        """
        if not file_path.is_file():
            raise FileNotFoundError(f"File not found: {file_path}")

        content = file_path.read_text(encoding=self.encoding).strip()
        metadata = {
            "file_path": str(file_path),
            "file_type": "text",
        }
        return [Document(text=content, metadata=metadata)]

================
File: document/splitter/__init__.py
================
from .base import SplitterBase
from .text import TextSplitter

================
File: document/splitter/base.py
================
from pydantic import BaseModel, ConfigDict, Field
from abc import ABC, abstractmethod
from typing import List, Optional, Callable
from dapr_agents.types.document import Document
import re
import logging

try:
    from nltk.tokenize import sent_tokenize

    NLTK_AVAILABLE = True
except ImportError:
    sent_tokenize = None
    NLTK_AVAILABLE = False

logger = logging.getLogger(__name__)


class SplitterBase(BaseModel, ABC):
    """
    Base class for defining text splitting strategies.
    Provides common utilities for breaking text into smaller chunks
    based on separators, regex patterns, or sentence-based splitting.
    """

    chunk_size: int = Field(
        default=4000,
        description="Maximum size of chunks (in characters or tokens).",
        gt=0,
    )
    chunk_overlap: int = Field(
        default=200,
        description="Overlap size between chunks for context continuity.",
        ge=0,
    )
    chunk_size_function: Callable[[str], int] = Field(
        default=len,
        description="Function to calculate chunk size (e.g., by characters or tokens).",
    )
    separator: Optional[str] = Field(
        default="\n\n", description="Primary separator for splitting text."
    )
    fallback_separators: List[str] = Field(
        default_factory=lambda: ["\n", " "],
        description="Fallback separators if the primary separator fails.",
    )
    fallback_regex: str = Field(
        default=r"[^,.;。？！]+[,.;。？！]",
        description="Improved regex pattern for fallback splitting.",
    )
    reserved_metadata_size: int = Field(
        default=0, description="Tokens reserved for metadata.", ge=0
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @abstractmethod
    def split(self, text: str) -> List[str]:
        """
        Abstract method to be implemented by child classes for splitting text.

        Args:
            text (str): The text to be split.

        Returns:
            List[str]: List of text chunks.
        """
        pass

    def _get_chunk_size(self, text: str) -> int:
        """
        Calculate the size of a chunk based on the provided chunk_size_function.

        Args:
            text (str): The input text.

        Returns:
            int: The size of the text chunk.
        """
        return self.chunk_size_function(text)

    def _merge_splits(self, splits: List[str], max_size: int) -> List[str]:
        """
        Merge splits into chunks while ensuring size constraints and meaningful overlaps.

        Unlike other implementations, this method prioritizes sentence boundaries
        when creating overlaps, ensuring that each chunk remains contextually meaningful.

        Args:
            splits (List[str]): The text segments to be merged.
            max_size (int): Maximum allowed size for each chunk.

        Returns:
            List[str]: The resulting merged chunks.
        """
        if not splits:
            return []

        chunks = []  # Store finalized chunks
        current_chunk = []  # Collect splits for the current chunk
        current_size = 0  # Track the size of the current chunk

        for split in splits:
            split_size = self._get_chunk_size(split)

            # If adding the current split exceeds max_size, finalize the current chunk
            if current_size + split_size > max_size:
                if current_chunk:
                    # Finalize the current chunk
                    full_chunk = "".join(current_chunk)
                    chunks.append(full_chunk)

                    # Logging information for overlap and chunk size
                    logger.debug(
                        f"Chunk {len(chunks)} finalized. Size: {current_size}. Overlap size: {self.chunk_overlap}"
                    )

                    # Create an overlap using sentences from the current chunk
                    overlap = []
                    overlap_size = 0
                    for sentence in reversed(current_chunk):
                        sentence_size = self._get_chunk_size(sentence)
                        if overlap_size + sentence_size > self.chunk_overlap:
                            break
                        overlap.insert(0, sentence)
                        overlap_size += sentence_size

                    # Logging information for overlap content
                    logger.debug(f"Chunk {len(chunks)} overlap: {''.join(overlap)}")

                    # Start the new chunk with the overlap
                    current_chunk = overlap
                    current_size = overlap_size
                else:
                    # If a single split exceeds max_size, treat it as a standalone chunk
                    chunks.append(split)
                    current_chunk = []
                    current_size = 0
            else:
                # Add the current split to the ongoing chunk
                current_chunk.append(split)
                current_size += split_size

        # Finalize the last chunk
        if current_chunk:
            chunks.append("".join(current_chunk))
            logger.debug(f"Chunk {len(chunks)} finalized. Size: {current_size}.")

        return chunks

    def _split_by_separators(self, text: str, separators: List[str]) -> List[str]:
        """
        Split text using a prioritized list of separators while keeping separators in chunks.

        For each separator in the provided list, attempt to split the text. The separator
        is appended to each split except the last one to preserve structure.

        Args:
            text (str): The input text to split.
            separators (List[str]): List of separators in order of priority.

        Returns:
            List[str]: A list of non-empty splits with separators retained.
        """
        for separator in separators:
            if separator in text:
                parts = text.split(separator)
                result = []
                for i, part in enumerate(parts):
                    # Add separator to all splits except the last one
                    if i < len(parts) - 1:
                        result.append(part + separator)
                    else:
                        result.append(part)
                # Return non-empty chunks only
                return [chunk for chunk in result if chunk.strip()]
        return [text.strip()]

    def _split_by_sentences(self, text: str) -> List[str]:
        """
        Split text into sentences using NLTK if available, or fallback to regex.

        Args:
            text (str): The input text to split.

        Returns:
            List[str]: List of sentences split from the text.
        """
        if NLTK_AVAILABLE:
            return sent_tokenize(text)
        return self._regex_split(text)

    def _regex_split(self, text: str) -> List[str]:
        """
        Split text using the fallback regex, retaining separators.

        Args:
            text (str): The input text to split.

        Returns:
            List[str]: List of text segments split using regex.
        """
        matches = re.findall(self.fallback_regex, text)
        return [match for match in matches if match.strip()]

    def _split_adaptively(self, text: str) -> List[str]:
        """
        Adaptively split text using separators, fallback methods, and regex.

        Args:
            text (str): The input text to split.

        Returns:
            List[str]: List of adaptively split text segments.
        """
        # Try primary separator first
        chunks = self._split_by_separators(text, [self.separator])

        # Use fallback separators if the primary separator fails
        if len(chunks) <= 1:
            chunks = self._split_by_separators(text, self.fallback_separators)

        # Finally, fallback to sentence-based or regex splitting
        if len(chunks) <= 1:
            chunks = self._split_by_sentences(text)

        return chunks

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        Split documents into smaller chunks while retaining metadata.

        Args:
            documents (List[Document]): List of documents to be split.

        Returns:
            List[Document]: List of chunked documents with updated metadata.
        """
        chunked_documents = []
        for doc in documents:
            text_chunks = self.split(doc.text)

            previous_end = 0
            for chunk_num, chunk in enumerate(text_chunks):
                start_index = doc.text.find(chunk, previous_end)
                if start_index == -1:
                    start_index = previous_end
                end_index = start_index + self._get_chunk_size(chunk)

                metadata = doc.metadata.copy() if doc.metadata else {}
                metadata.update(
                    {
                        "chunk_number": chunk_num + 1,
                        "total_chunks": len(text_chunks),
                        "start_index": start_index,
                        "end_index": end_index,
                        "chunk_length": self._get_chunk_size(chunk),
                    }
                )
                chunked_documents.append(Document(metadata=metadata, text=chunk))
                previous_end = end_index

        return chunked_documents

================
File: document/splitter/text.py
================
from dapr_agents.document.splitter.base import SplitterBase
from typing import List
import logging

logger = logging.getLogger(__name__)


class TextSplitter(SplitterBase):
    """
    Concrete implementation of the SplitterBase class.
    Splits text using a primary separator, fallback strategies,
    and applies size limits and overlap logic.
    """

    def split(self, text: str) -> List[str]:
        """
        Splits input text into chunks using a hierarchical strategy.

        Steps:
        1. Adjusts the effective chunk size to account for metadata space.
        2. If the text is smaller than the adjusted chunk size, returns it as a single chunk.
        3. Splits text adaptively using primary and fallback methods.
        4. Merges smaller chunks into valid sizes while maintaining overlap.

        Args:
            text (str): The text to be split.

        Returns:
            List[str]: List of merged text chunks.
        """
        # Step 1: Adjust effective chunk size
        effective_chunk_size = self.chunk_size - self.reserved_metadata_size
        logger.debug(f"Effective chunk size: {effective_chunk_size}")

        # Step 2: Short-circuit for small texts
        if self._get_chunk_size(text) <= effective_chunk_size:
            logger.debug(
                "Text size is smaller than effective chunk size. Returning as a single chunk."
            )
            return [text]

        # Step 3: Use adaptive splitting strategy
        chunks = self._split_adaptively(text)
        logger.debug(f"Initial split into {len(chunks)} chunks.")

        # Step 4: Merge smaller chunks into valid sizes with overlap
        merged_chunks = self._merge_splits(chunks, effective_chunk_size)
        logger.debug(f"Merged into {len(merged_chunks)} chunks with overlap.")

        return merged_chunks

================
File: document/__init__.py
================
from .fetcher import ArxivFetcher
from .reader import PyMuPDFReader, PyPDFReader
from .splitter import TextSplitter
from .embedder import OpenAIEmbedder, SentenceTransformerEmbedder, NVIDIAEmbedder

================
File: executors/utils/package_manager.py
================
from __future__ import annotations
import shutil
from pathlib import Path
from typing import List, Optional, Set
from functools import lru_cache
from enum import Enum


class PackageManagerType(str, Enum):
    """Types of package managers that can be detected."""

    PIP = "pip"
    POETRY = "poetry"
    PIPENV = "pipenv"
    CONDA = "conda"
    NPM = "npm"
    YARN = "yarn"
    PNPM = "pnpm"
    BUN = "bun"
    CARGO = "cargo"
    GO = "go"
    MAVEN = "maven"
    GRADLE = "gradle"
    COMPOSER = "composer"
    UNKNOWN = "unknown"


class ProjectType(str, Enum):
    """Types of projects that can be detected."""

    PYTHON = "python"
    NODE = "node"
    RUST = "rust"
    GO = "go"
    JAVA = "java"
    PHP = "php"
    UNKNOWN = "unknown"


class PackageManager:
    """Information about a package manager and its commands."""

    def __init__(
        self,
        name: PackageManagerType,
        project_type: ProjectType,
        install_cmd: str,
        add_cmd: Optional[str] = None,
        remove_cmd: Optional[str] = None,
        update_cmd: Optional[str] = None,
        markers: Optional[List[str]] = None,
    ) -> None:
        """
        Initialize a package manager.

        Args:
            name: Package manager identifier.
            project_type: Type of project this manager serves.
            install_cmd: Command to install project dependencies.
            add_cmd: Command to add a single package.
            remove_cmd: Command to remove a package.
            update_cmd: Command to update packages.
            markers: Filenames indicating this manager in a project.
        """
        self.name = name
        self.project_type = project_type
        self.install_cmd = install_cmd
        self.add_cmd = add_cmd or f"{name.value} install"
        self.remove_cmd = remove_cmd or f"{name.value} remove"
        self.update_cmd = update_cmd or f"{name.value} update"
        self.markers = markers or []

    def __str__(self) -> str:
        return self.name.value


# Known package managers
PACKAGE_MANAGERS: dict[PackageManagerType, PackageManager] = {
    # Python package managers
    PackageManagerType.PIP: PackageManager(
        name=PackageManagerType.PIP,
        project_type=ProjectType.PYTHON,
        install_cmd="pip install -r requirements.txt",
        add_cmd="pip install",
        remove_cmd="pip uninstall",
        update_cmd="pip install --upgrade",
        markers=["requirements.txt", "setup.py", "setup.cfg"],
    ),
    PackageManagerType.POETRY: PackageManager(
        name=PackageManagerType.POETRY,
        project_type=ProjectType.PYTHON,
        install_cmd="poetry install",
        add_cmd="poetry add",
        remove_cmd="poetry remove",
        update_cmd="poetry update",
        markers=["pyproject.toml", "poetry.lock"],
    ),
    PackageManagerType.PIPENV: PackageManager(
        name=PackageManagerType.PIPENV,
        project_type=ProjectType.PYTHON,
        install_cmd="pipenv install",
        add_cmd="pipenv install",
        remove_cmd="pipenv uninstall",
        update_cmd="pipenv update",
        markers=["Pipfile", "Pipfile.lock"],
    ),
    PackageManagerType.CONDA: PackageManager(
        name=PackageManagerType.CONDA,
        project_type=ProjectType.PYTHON,
        install_cmd="conda env update -f environment.yml",
        add_cmd="conda install",
        remove_cmd="conda remove",
        update_cmd="conda update",
        markers=["environment.yml", "environment.yaml"],
    ),
    # JavaScript package managers
    PackageManagerType.NPM: PackageManager(
        name=PackageManagerType.NPM,
        project_type=ProjectType.NODE,
        install_cmd="npm install",
        add_cmd="npm install",
        remove_cmd="npm uninstall",
        update_cmd="npm update",
        markers=["package.json", "package-lock.json"],
    ),
    PackageManagerType.YARN: PackageManager(
        name=PackageManagerType.YARN,
        project_type=ProjectType.NODE,
        install_cmd="yarn install",
        add_cmd="yarn add",
        remove_cmd="yarn remove",
        update_cmd="yarn upgrade",
        markers=["package.json", "yarn.lock"],
    ),
    PackageManagerType.PNPM: PackageManager(
        name=PackageManagerType.PNPM,
        project_type=ProjectType.NODE,
        install_cmd="pnpm install",
        add_cmd="pnpm add",
        remove_cmd="pnpm remove",
        update_cmd="pnpm update",
        markers=["package.json", "pnpm-lock.yaml"],
    ),
    PackageManagerType.BUN: PackageManager(
        name=PackageManagerType.BUN,
        project_type=ProjectType.NODE,
        install_cmd="bun install",
        add_cmd="bun add",
        remove_cmd="bun remove",
        update_cmd="bun update",
        markers=["package.json", "bun.lockb"],
    ),
}


@lru_cache(maxsize=None)
def is_installed(name: str) -> bool:
    """
    Check if a given command exists on PATH.

    Args:
        name: Command name to check.

    Returns:
        True if the command is available, False otherwise.
    """
    return shutil.which(name) is not None


@lru_cache(maxsize=None)
def detect_package_managers(directory: str) -> List[PackageManager]:
    """
    Detect all installed package managers by looking for marker files.

    Args:
        directory: Path to the project root.

    Returns:
        A list of PackageManager instances found in the directory.
    """
    dir_path = Path(directory)
    if not dir_path.is_dir():
        return []

    found: List[PackageManager] = []
    for pm in PACKAGE_MANAGERS.values():
        for marker in pm.markers:
            if (dir_path / marker).exists() and is_installed(pm.name.value):
                found.append(pm)
                break
    return found


@lru_cache(maxsize=None)
def get_primary_package_manager(directory: str) -> Optional[PackageManager]:
    """
    Determine the primary package manager using lockfile heuristics.

    Args:
        directory: Path to the project root.

    Returns:
        The chosen PackageManager or None if none detected.
    """
    managers = detect_package_managers(directory)
    if not managers:
        return None
    if len(managers) == 1:
        return managers[0]

    dir_path = Path(directory)
    # Prefer lockfiles over others
    lock_priority = [
        (PackageManagerType.POETRY, "poetry.lock"),
        (PackageManagerType.PIPENV, "Pipfile.lock"),
        (PackageManagerType.PNPM, "pnpm-lock.yaml"),
        (PackageManagerType.YARN, "yarn.lock"),
        (PackageManagerType.BUN, "bun.lockb"),
        (PackageManagerType.NPM, "package-lock.json"),
    ]
    for pm_type, lock in lock_priority:
        if (dir_path / lock).exists() and PACKAGE_MANAGERS.get(pm_type) in managers:
            return PACKAGE_MANAGERS[pm_type]

    return managers[0]


def get_install_command(directory: str) -> Optional[str]:
    """
    Get the shell command to install project dependencies.

    Args:
        directory: Path to the project root.

    Returns:
        A shell command string or None if no manager detected.
    """
    pm = get_primary_package_manager(directory)
    return pm.install_cmd if pm else None


def get_add_command(directory: str, package: str, dev: bool = False) -> Optional[str]:
    """
    Get the shell command to add a package to the project.

    Args:
        directory: Path to the project root.
        package: Package name to add.
        dev: Whether to add as a development dependency.

    Returns:
        A shell command string or None if no manager detected.
    """
    pm = get_primary_package_manager(directory)
    if not pm:
        return None
    base = pm.add_cmd
    if dev and pm.name in {
        PackageManagerType.PIP,
        PackageManagerType.POETRY,
        PackageManagerType.NPM,
        PackageManagerType.YARN,
        PackageManagerType.PNPM,
        PackageManagerType.BUN,
        PackageManagerType.COMPOSER,
    }:
        flag = (
            "--dev"
            if pm.name in {PackageManagerType.PIP, PackageManagerType.POETRY}
            else "--save-dev"
        )
        return f"{base} {package} {flag}"
    return f"{base} {package}"


def get_project_type(directory: str) -> ProjectType:
    """
    Infer project type from the primary package manager or file extensions.

    Args:
        directory: Path to the project root.

    Returns:
        The detected ProjectType.
    """
    pm = get_primary_package_manager(directory)
    if pm:
        return pm.project_type

    # Fallback by extension scanning
    exts: Set[str] = set()
    for path in Path(directory).rglob("*"):
        if path.is_file():
            exts.add(path.suffix.lower())
            if len(exts) > 50:
                break
    if ".py" in exts:
        return ProjectType.PYTHON
    if {".js", ".ts"} & exts:
        return ProjectType.NODE
    if ".rs" in exts:
        return ProjectType.RUST
    if ".go" in exts:
        return ProjectType.GO
    if ".java" in exts:
        return ProjectType.JAVA
    if ".php" in exts:
        return ProjectType.PHP
    return ProjectType.UNKNOWN

================
File: executors/__init__.py
================
from .base import CodeExecutorBase
from .local import LocalCodeExecutor
from .docker import DockerCodeExecutor

================
File: executors/base.py
================
from dapr_agents.types.executor import ExecutionRequest, CodeSnippet, ExecutionResult
from abc import ABC, abstractmethod
from pydantic import BaseModel
from typing import List, ClassVar


class CodeExecutorBase(BaseModel, ABC):
    """Abstract base class for executing code in different environments."""

    SUPPORTED_LANGUAGES: ClassVar[set] = {"python", "sh", "bash"}

    @abstractmethod
    async def execute(self, request: ExecutionRequest) -> List[ExecutionResult]:
        """Executes the provided code snippets and returns results."""
        pass

    def validate_snippets(self, snippets: List[CodeSnippet]) -> bool:
        """Ensures all code snippets are valid before execution."""
        for snippet in snippets:
            if snippet.language not in self.SUPPORTED_LANGUAGES:
                raise ValueError(f"Unsupported language: {snippet.language}")
        return True

================
File: executors/docker.py
================
from dapr_agents.types.executor import ExecutionRequest, ExecutionResult
from typing import List, Any, Optional, Union, Literal
from dapr_agents.executors import CodeExecutorBase
from pydantic import Field
import tempfile
import logging
import asyncio
import shutil
import ast
import os

logger = logging.getLogger(__name__)


class DockerCodeExecutor(CodeExecutorBase):
    """Executes code securely inside a persistent Docker container with dynamic volume updates."""

    image: Optional[str] = Field(
        "python:3.9", description="Docker image used for execution."
    )
    container_name: Optional[str] = Field(
        "dapr_agents_code_executor", description="Name of the Docker container."
    )
    disable_network_access: bool = Field(
        default=True, description="Disable network access inside the container."
    )
    execution_timeout: int = Field(
        default=60, description="Max execution time (seconds)."
    )
    execution_mode: str = Field(
        "detached", description="Execution mode: 'interactive' or 'detached'."
    )
    restart_policy: str = Field(
        "no", description="Container restart policy: 'no', 'on-failure', 'always'."
    )
    max_memory: str = Field("500m", description="Max memory for execution.")
    cpu_quota: int = Field(50000, description="CPU quota limit.")
    runtime: Optional[str] = Field(
        default=None, description="Container runtime (e.g., 'nvidia')."
    )
    auto_remove: bool = Field(
        default=False, description="Keep container running to reuse it."
    )
    auto_cleanup: bool = Field(
        default=False,
        description="Automatically clean up the workspace after execution.",
    )
    volume_access_mode: Literal["ro", "rw"] = Field(
        default="ro", description="Access mode for the workspace volume."
    )
    host_workspace: Optional[str] = Field(
        default=None,
        description="Custom workspace on host. If None, defaults to system temp dir.",
    )

    docker_client: Optional[Any] = Field(
        default=None, init=False, description="Docker client instance."
    )
    execution_container: Optional[Any] = Field(
        default=None, init=False, description="Persistent Docker container."
    )
    container_workspace: Optional[str] = Field(
        default="/workspace", init=False, description="Mounted workspace in container."
    )

    def model_post_init(self, __context: Any) -> None:
        """Initializes the Docker client and ensures a reusable execution container is ready."""
        try:
            from docker import DockerClient
            from docker.errors import DockerException
        except ImportError as e:
            raise ImportError(
                "Install 'docker' package with 'pip install docker'."
            ) from e

        try:
            self.docker_client: DockerClient = DockerClient.from_env()
        except DockerException as e:
            raise RuntimeError("Docker not running or unreachable.") from e

        # Validate or Set the Host Workspace
        if self.host_workspace:
            self.host_workspace = os.path.abspath(
                self.host_workspace
            )  # Ensure absolute path
        else:
            self.host_workspace = os.path.join(
                tempfile.gettempdir(), "dapr_agents_executor_workspace"
            )

        # Ensure the directory exists
        os.makedirs(self.host_workspace, exist_ok=True)

        # Log the workspace path for visibility
        logger.info(f"Using host workspace: {self.host_workspace}")

        self.ensure_container()

        super().model_post_init(__context)

    def ensure_container(self) -> None:
        """Ensures that the execution container exists. If not, it creates and starts one."""
        try:
            from docker.errors import NotFound
        except ImportError as e:
            raise ImportError(
                "Install 'docker' package with 'pip install docker'."
            ) from e

        try:
            self.execution_container = self.docker_client.containers.get(
                self.container_name
            )
            logger.info(f"Reusing existing container: {self.container_name}")
        except NotFound:
            logger.info(f"Creating a new container: {self.container_name}")
            self.create_container()
            self.execution_container.start()
            logger.info(f"Started container: {self.container_name}")

    def create_container(self) -> None:
        """Creates a reusable Docker container."""
        try:
            from docker.errors import DockerException, APIError
        except ImportError as e:
            raise ImportError(
                "Install 'docker' package with 'pip install docker'."
            ) from e
        try:
            self.execution_container = self.docker_client.containers.create(
                self.image,
                name=self.container_name,
                command="/bin/sh -c 'while true; do sleep 30; done'",
                detach=True,
                stdin_open=True,
                tty=(self.execution_mode == "interactive"),
                auto_remove=False,
                network_disabled=self.disable_network_access,
                mem_limit=self.max_memory,
                cpu_quota=self.cpu_quota,
                security_opt=["no-new-privileges"],
                restart_policy={"Name": self.restart_policy},
                runtime=self.runtime,
                working_dir=self.container_workspace,
                volumes={
                    self.host_workspace: {
                        "bind": self.container_workspace,
                        "mode": self.volume_access_mode,
                    }
                },
            )
        except (DockerException, APIError) as e:
            logger.error(f"Failed to create the execution container: {str(e)}")
            raise RuntimeError(
                f"Failed to create the execution container: {str(e)}"
            ) from e

    async def execute(
        self, request: Union[ExecutionRequest, dict]
    ) -> List[ExecutionResult]:
        """
        Executes code inside the persistent Docker container.
        The code is written to a shared volume instead of stopping & starting the container.

        Args:
            request (Union[ExecutionRequest, dict]): The execution request containing code snippets.

        Returns:
            List[ExecutionResult]: A list of execution results.
        """
        if isinstance(request, dict):
            request = ExecutionRequest(**request)

        self.validate_snippets(request.snippets)
        results = []

        try:
            for snippet in request.snippets:
                if snippet.language == "python":
                    required_packages = self._extract_imports(snippet.code)
                    if required_packages:
                        logger.info(
                            f"Installing missing dependencies: {required_packages}"
                        )
                        await self._install_missing_packages(required_packages)

                script_filename = f"script.{snippet.language}"
                script_path_host = os.path.join(self.host_workspace, script_filename)
                script_path_container = f"{self.container_workspace}/{script_filename}"

                # Write the script dynamically
                with open(script_path_host, "w", encoding="utf-8") as script_file:
                    script_file.write(snippet.code)

                cmd = (
                    f"timeout {self.execution_timeout} python3 {script_path_container}"
                    if snippet.language == "python"
                    else f"timeout {self.execution_timeout} sh {script_path_container}"
                )

                # Run command dynamically inside the running container
                exec_result = await asyncio.to_thread(
                    self.execution_container.exec_run, cmd
                )

                exit_code = exec_result.exit_code
                logs = exec_result.output.decode("utf-8", errors="ignore").strip()
                status = "success" if exit_code == 0 else "error"

                results.append(
                    ExecutionResult(status=status, output=logs, exit_code=exit_code)
                )

        except Exception as e:
            logs = self.get_container_logs()
            logger.error(f"Execution error: {str(e)}\nContainer logs:\n{logs}")
            results.append(ExecutionResult(status="error", output=str(e), exit_code=1))

        finally:
            if self.auto_cleanup:
                if os.path.exists(self.host_workspace):
                    shutil.rmtree(self.host_workspace, ignore_errors=True)
                    logger.info(
                        f"Temporary workspace {self.host_workspace} cleaned up."
                    )

            if self.auto_remove:
                self.execution_container.stop()
                logger.info(f"Container {self.execution_container.id} stopped.")

        return results

    def _extract_imports(self, code: str) -> List[str]:
        """
        Parses a Python script and extracts the top-level imported modules.

        Args:
            code (str): The Python code to analyze.

        Returns:
            List[str]: A list of unique top-level module names imported in the script.

        Raises:
            SyntaxError: If the provided code is not valid Python, an error is logged,
                        and an empty list is returned.
        """
        try:
            parsed_code = ast.parse(code)
        except SyntaxError as e:
            logger.error(f"Syntax error in code: {e}")
            return []

        modules = set()
        for node in ast.walk(parsed_code):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    modules.add(alias.name.split(".")[0])
            elif isinstance(node, ast.ImportFrom) and node.module:
                modules.add(node.module.split(".")[0])

        return list(modules)

    async def _install_missing_packages(self, packages: List[str]) -> None:
        """
        Installs missing Python dependencies inside the execution container.

        Args:
            packages (List[str]): A list of package names to install.

        Raises:
            RuntimeError: If the package installation fails.
        """
        if not packages:
            return

        command = f"python3 -m pip install {' '.join(packages)}"
        result = await asyncio.to_thread(self.execution_container.exec_run, command)

        if result.exit_code != 0:
            error_msg = result.output.decode().strip()
            logger.error(f"Dependency installation failed: {error_msg}")
            raise RuntimeError(f"Dependency installation failed: {error_msg}")

        logger.info(f"Dependencies installed: {', '.join(packages)}")

    def get_container_logs(self) -> str:
        """
        Retrieves and returns the logs from the execution container.

        Returns:
            str: The container logs as a string.

        Raises:
            Exception: If log retrieval fails, an error message is logged.
        """
        try:
            logs = self.execution_container.logs(stdout=True, stderr=True).decode(
                "utf-8"
            )
            return logs
        except Exception as e:
            logger.error(f"Failed to retrieve container logs: {str(e)}")
            return ""

================
File: executors/local.py
================
"""Local executor that runs Python or shell snippets in cached virtual-envs."""

import asyncio
import ast
import hashlib
import inspect
import logging
import time
import venv
from pathlib import Path
from typing import Any, Callable, List, Sequence, Union

from pydantic import Field, PrivateAttr

from dapr_agents.executors import CodeExecutorBase
from dapr_agents.executors.sandbox import detect_backend, wrap_command, SandboxType
from dapr_agents.executors.utils.package_manager import (
    get_install_command,
    get_project_type,
)
from dapr_agents.types.executor import ExecutionRequest, ExecutionResult

logger = logging.getLogger(__name__)


class LocalCodeExecutor(CodeExecutorBase):
    """
    Run snippets locally with **optional OS-level sandboxing** and
    per-snippet virtual-env caching.
    """

    cache_dir: Path = Field(
        default_factory=lambda: Path.cwd() / ".dapr_agents_cached_envs",
        description="Directory that stores cached virtual environments.",
    )
    user_functions: List[Callable] = Field(
        default_factory=list,
        description="Functions whose source is prepended to every Python snippet.",
    )
    sandbox: SandboxType = Field(
        default="auto",
        description="'seatbelt' | 'firejail' | 'none' | 'auto' (best available)",
    )
    writable_paths: List[Path] = Field(
        default_factory=list,
        description="Extra paths the sandboxed process may write to.",
    )
    cleanup_threshold: int = Field(
        default=604_800,  # one week
        description="Seconds before a cached venv is considered stale.",
    )

    _env_lock: asyncio.Lock = PrivateAttr(default_factory=asyncio.Lock)
    _bootstrapped_root: Path | None = PrivateAttr(default=None)

    def model_post_init(self, __context: Any) -> None:  # noqa: D401
        """Create ``cache_dir`` after pydantic instantiation."""
        super().model_post_init(__context)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        logger.debug("venv cache directory: %s", self.cache_dir)

    async def execute(
        self, request: Union[ExecutionRequest, dict]
    ) -> List[ExecutionResult]:
        """
        Run the snippets in *request* and return their results.

        Args:
            request: ``ExecutionRequest`` instance or a raw mapping that can
                be unpacked into one.

        Returns:
            A list with one ``ExecutionResult`` for every snippet in the
            original request.
        """
        if isinstance(request, dict):
            request = ExecutionRequest(**request)

        await self._bootstrap_project()
        self.validate_snippets(request.snippets)

        #  Resolve sandbox once
        eff_backend: SandboxType = (
            detect_backend() if self.sandbox == "auto" else self.sandbox
        )
        if eff_backend != "none":
            logger.info(
                "Sandbox backend enabled: %s%s",
                eff_backend,
                f" (writable: {', '.join(map(str, self.writable_paths))})"
                if self.writable_paths
                else "",
            )
        else:
            logger.info("Sandbox disabled - running commands directly.")

        # Main loop
        results: list[ExecutionResult] = []
        for snip_idx, snippet in enumerate(request.snippets, start=1):
            start = time.perf_counter()

            # Assemble the *raw* command
            if snippet.language == "python":
                env = await self._prepare_python_env(snippet.code)
                python_bin = env / "bin" / "python3"
                prelude = "\n".join(inspect.getsource(fn) for fn in self.user_functions)
                script = f"{prelude}\n{snippet.code}" if prelude else snippet.code
                raw_cmd: Sequence[str] = [str(python_bin), "-c", script]
            else:
                raw_cmd = ["sh", "-c", snippet.code]

            # Wrap for sandbox
            final_cmd = wrap_command(raw_cmd, eff_backend, self.writable_paths)
            logger.debug(
                "Snippet %s - launch command: %s",
                snip_idx,
                " ".join(final_cmd),
            )

            # Run it
            snip_timeout = getattr(snippet, "timeout", request.timeout)
            results.append(await self._run_subprocess(final_cmd, snip_timeout))

            logger.info(
                "Snippet %s finished in %.3fs",
                snip_idx,
                time.perf_counter() - start,
            )

        return results

    async def _bootstrap_project(self) -> None:
        """Install top-level dependencies once per executor instance."""
        cwd = Path.cwd().resolve()
        if self._bootstrapped_root == cwd:
            return

        install_cmd = get_install_command(str(cwd))
        if install_cmd:
            logger.info(
                "bootstrapping %s project with '%s'",
                get_project_type(str(cwd)).value,
                install_cmd,
            )

            proc = await asyncio.create_subprocess_shell(
                install_cmd,
                cwd=cwd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            _, err = await proc.communicate()
            if proc.returncode:
                logger.warning(
                    "bootstrap failed (%d): %s", proc.returncode, err.decode().strip()
                )

        self._bootstrapped_root = cwd

    async def _prepare_python_env(self, code: str) -> Path:
        """
        Ensure a virtual-env exists that satisfies *code* imports.

        Args:
            code: User-supplied Python source.

        Returns:
            Path to the virtual-env directory.
        """
        imports = self._extract_imports(code)
        env = await self._get_or_create_cached_env(imports)
        missing = await self._get_missing_packages(imports, env)
        if missing:
            await self._install_missing_packages(missing, env)
        return env

    @staticmethod
    def _extract_imports(code: str) -> List[str]:
        """
        Return all top-level imported module names in *code*.

        Args:
            code: Python source to scan.

        Returns:
            Unique list of first-segment module names.

        Raises:
            SyntaxError: If *code* cannot be parsed.
        """
        try:
            tree = ast.parse(code)
        except SyntaxError:
            logger.error("cannot parse user code, assuming no imports")
            return []

        names = {
            alias.name.partition(".")[0]
            for node in ast.walk(tree)
            for alias in getattr(node, "names", [])
            if isinstance(node, (ast.Import, ast.ImportFrom))
        }
        if any(
            isinstance(node, ast.ImportFrom) and node.module for node in ast.walk(tree)
        ):
            names |= {
                node.module.partition(".")[0]
                for node in ast.walk(tree)
                if isinstance(node, ast.ImportFrom) and node.module
            }
        return sorted(names)

    async def _get_missing_packages(
        self, packages: List[str], env_path: Path
    ) -> List[str]:
        """
        Identify which *packages* are not importable from *env_path*.

        Args:
            packages: Candidate import names.
            env_path: Path to the virtual-env.

        Returns:
            Subset of *packages* that need installation.
        """
        python = env_path / "bin" / "python3"

        async def probe(pkg: str) -> str | None:
            proc = await asyncio.create_subprocess_exec(
                str(python),
                "- <<PY\nimport importlib.util, sys;"
                f"sys.exit(importlib.util.find_spec('{pkg}') is None)\nPY",
                stdout=asyncio.subprocess.DEVNULL,
                stderr=asyncio.subprocess.DEVNULL,
            )
            await proc.wait()
            return pkg if proc.returncode else None

        missing = await asyncio.gather(*(probe(p) for p in packages))
        return [m for m in missing if m]

    async def _get_or_create_cached_env(self, deps: List[str]) -> Path:
        """
        Return a cached venv path keyed by the sorted list *deps*.

        Args:
            deps: Import names required by user code.

        Returns:
            Path to the virtual-env directory.

        Raises:
            RuntimeError: If venv creation fails.
        """
        digest = hashlib.sha1(",".join(sorted(deps)).encode()).hexdigest()
        env_path = self.cache_dir / f"env_{digest}"

        async with self._env_lock:
            if env_path.exists():
                logger.info("Reusing cached virtual environment.")
            else:
                try:
                    venv.create(env_path, with_pip=True)
                    logger.info("Created a new virtual environment")
                    logger.debug("venv %s created", env_path)
                except Exception as exc:  # noqa: BLE001
                    raise RuntimeError("virtual-env creation failed") from exc
        return env_path

    async def _install_missing_packages(
        self, packages: List[str], env_dir: Path
    ) -> None:
        """
        ``pip install`` *packages* inside *env_dir*.

        Args:
            packages: Package names to install.
            env_dir: Target virtual-env directory.

        Raises:
            RuntimeError: If installation returns non-zero exit code.
        """
        python = env_dir / "bin" / "python3"
        cmd = [str(python), "-m", "pip", "install", *packages]
        logger.info("Installing %s", ", ".join(packages))

        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        _, err = await proc.communicate()
        if proc.returncode != 0:
            msg = err.decode().strip()
            logger.error("pip install failed: %s", msg)
            raise RuntimeError(msg)
        logger.debug("Installed %d package(s)", len(packages))

    async def _run_subprocess(
        self, cmd: Sequence[str], timeout: int
    ) -> ExecutionResult:
        """
        Run *cmd* with *timeout* seconds.

        Args:
            cmd: Command list to execute.
            timeout: Maximum runtime in seconds.

        Returns:
            ``ExecutionResult`` with captured output.
        """
        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
            )
            out, err = await asyncio.wait_for(proc.communicate(), timeout)
            status = "success" if proc.returncode == 0 else "error"
            if err:
                logger.debug("stderr: %s", err.decode().strip())
            return ExecutionResult(
                status=status, output=out.decode(), exit_code=proc.returncode
            )

        except asyncio.TimeoutError:
            proc.kill()
            await proc.wait()
            return ExecutionResult(
                status="error", output="execution timed out", exit_code=1
            )

        except Exception as exc:
            return ExecutionResult(status="error", output=str(exc), exit_code=1)

================
File: executors/sandbox.py
================
"""Light-weight cross-platform sandbox helpers."""

import platform
import shutil
from pathlib import Path
from typing import List, Literal, Sequence

SandboxType = Literal["none", "seatbelt", "firejail", "auto"]

_READ_ONLY_SEATBELT_POLICY = r"""
(version 1)

; ---------------- default = deny everything -----------------
(deny default)

; ---------------- read-only FS access -----------------------
(allow file-read*)

; ---------------- minimal process mgmt ----------------------
(allow process-exec)
(allow process-fork)
(allow signal (target self))

; ---------------- write-only to /dev/null -------------------
(allow file-write-data
  (require-all
    (path "/dev/null")
    (vnode-type CHARACTER-DEVICE)))

; ---------------- harmless sysctls --------------------------
(allow sysctl-read
  (sysctl-name "hw.activecpu")
  (sysctl-name "hw.busfrequency_compat")
  (sysctl-name "hw.byteorder")
  (sysctl-name "hw.cacheconfig")
  (sysctl-name "hw.cachelinesize_compat")
  (sysctl-name "hw.cpufamily")
  (sysctl-name "hw.cpufrequency_compat")
  (sysctl-name "hw.cputype")
  (sysctl-name "hw.l1dcachesize_compat")
  (sysctl-name "hw.l1icachesize_compat")
  (sysctl-name "hw.l2cachesize_compat")
  (sysctl-name "hw.l3cachesize_compat")
  (sysctl-name "hw.logicalcpu_max")
  (sysctl-name "hw.machine")
  (sysctl-name "hw.ncpu")
  (sysctl-name "hw.nperflevels")
  (sysctl-name "hw.memsize")
  (sysctl-name "hw.pagesize")
  (sysctl-name "hw.packages")
  (sysctl-name "hw.physicalcpu_max")
  (sysctl-name "kern.hostname")
  (sysctl-name "kern.osrelease")
  (sysctl-name "kern.ostype")
  (sysctl-name "kern.osversion")
  (sysctl-name "kern.version")
  (sysctl-name-prefix "hw.perflevel")
)
"""


def detect_backend() -> SandboxType:  # noqa: D401
    """Return the best-effort sandbox backend for the current host."""
    system = platform.system()
    if system == "Darwin" and shutil.which("sandbox-exec"):
        return "seatbelt"
    if system == "Linux" and shutil.which("firejail"):
        return "firejail"
    return "none"


def _seatbelt_cmd(cmd: Sequence[str], writable_paths: List[Path]) -> List[str]:
    """
    Construct a **macOS seatbelt** command line.

    The resulting list can be passed directly to `asyncio.create_subprocess_exec`.
    It launches the target *cmd* under **sandbox-exec** with an
    *initially-read-only* profile; every directory in *writable_paths* is added
    as an explicit “write-allowed sub-path”.

    Args:
        cmd:
            The *raw* command (program + args) that should run inside the sandbox.
        writable_paths:
            Absolute paths that the child process must be able to modify
            (e.g. a temporary working directory).
            Each entry becomes a param `-D WR<i>=<path>` and a corresponding
            ``file-write*`` rule in the generated profile.

    Returns:
        list[str]
            A fully-assembled ``sandbox-exec`` invocation:
            ``['sandbox-exec', '-p', <profile>, …, '--', *cmd]``.
    """
    policy = _READ_ONLY_SEATBELT_POLICY
    params: list[str] = []

    if writable_paths:
        # Build parameter substitutions and the matching `(allow file-write*)` stanza.
        write_terms: list[str] = []
        for idx, path in enumerate(writable_paths):
            param = f"WR{idx}"
            params.extend(["-D", f"{param}={path}"])
            write_terms.append(f'(subpath (param "{param}"))')

        policy += f"\n(allow file-write*\n  {' '.join(write_terms)}\n)"

    return [
        "sandbox-exec",
        "-p",
        policy,
        *params,
        "--",
        *cmd,
    ]


def _firejail_cmd(cmd: Sequence[str], writable_paths: List[Path]) -> List[str]:
    """
    Build a **Firejail** command line (Linux only).

    The wrapper enables seccomp, disables sound and networking, and whitelists
    the provided *writable_paths* so the child process can persist data there.

    Args:
        cmd:
            The command (program + args) to execute.
        writable_paths:
            Directories that must remain writable inside the Firejail sandbox.

    Returns:
        list[str]
            A Firejail-prefixed command suitable for
            ``asyncio.create_subprocess_exec``.

    Raises:
        ValueError
            If *writable_paths* contains non-absolute paths.
    """
    for p in writable_paths:
        if not p.is_absolute():
            raise ValueError(f"Firejail whitelist paths must be absolute: {p}")

    rw_flags = sum([["--whitelist", str(p)] for p in writable_paths], [])
    return [
        "firejail",
        "--quiet",  # suppress banner
        "--seccomp",  # enable seccomp filter
        "--nosound",
        "--net=none",
        *rw_flags,
        "--",
        *cmd,
    ]


def wrap_command(
    cmd: Sequence[str],
    backend: SandboxType,
    writable_paths: List[Path] | None = None,
) -> List[str]:
    """
    Produce a sandbox-wrapped command according to *backend*.

    This is the single public helper used by the executors: it hides the
    platform-specific details of **seatbelt** and **Firejail** while providing
    a graceful fallback to “no sandbox”.

    Args:
        cmd:
            The raw command (program + args) to execute.
        backend:
            One of ``'seatbelt'``, ``'firejail'``, ``'none'`` or ``'auto'``.
            When ``'auto'`` is supplied the caller should already have resolved the
            platform with :func:`detect_backend`; the value is treated as ``'none'``.
        writable_paths:
            Extra directories that must remain writable inside the sandbox.
            Ignored when *backend* is ``'none'`` / ``'auto'``.

    Returns:
        list[str]
            The command list ready for ``asyncio.create_subprocess_exec``.
            If sandboxing is disabled, this is simply ``list(cmd)``.

    Raises:
        ValueError
            If an unrecognised *backend* value is given.
    """
    if backend in ("none", "auto"):
        return list(cmd)

    writable_paths = writable_paths or []

    if backend == "seatbelt":
        return _seatbelt_cmd(cmd, writable_paths)

    if backend == "firejail":
        return _firejail_cmd(cmd, writable_paths)

    raise ValueError(f"Unknown sandbox backend: {backend!r}")

================
File: llm/dapr/__init__.py
================
from .chat import DaprChatClient
from .client import DaprInferenceClientBase

================
File: llm/dapr/chat.py
================
from dapr_agents.llm.dapr.client import DaprInferenceClientBase
from dapr_agents.llm.utils import RequestHandler, ResponseHandler
from dapr_agents.prompt.prompty import Prompty
from dapr_agents.types.message import BaseMessage
from dapr_agents.llm.chat import ChatClientBase
from dapr_agents.tool import AgentTool
from dapr.clients.grpc._request import ConversationInput
from typing import (
    Union,
    Optional,
    Iterable,
    Dict,
    Any,
    List,
    Iterator,
    Type,
    Literal,
    ClassVar,
)
from pydantic import BaseModel
from pathlib import Path
import logging
import os
import time

logger = logging.getLogger(__name__)


class DaprChatClient(DaprInferenceClientBase, ChatClientBase):
    """
    Concrete class for Dapr's chat completion API using the Inference API.
    This class extends the ChatClientBase.
    """

    SUPPORTED_STRUCTURED_MODES: ClassVar[set] = {"function_call"}

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes private attributes for provider, api, config, and client after validation.
        """
        # Set the private provider and api attributes
        self._api = "chat"
        self._llm_component = os.environ["DAPR_LLM_COMPONENT_DEFAULT"]

        return super().model_post_init(__context)

    @classmethod
    def from_prompty(
        cls,
        prompty_source: Union[str, Path],
        timeout: Union[int, float, Dict[str, Any]] = 1500,
    ) -> "DaprChatClient":
        """
        Initializes an DaprChatClient client using a Prompty source, which can be a file path or inline content.

        Args:
            prompty_source (Union[str, Path]): The source of the Prompty file, which can be a path to a file
                or inline Prompty content as a string.
            timeout (Union[int, float, Dict[str, Any]], optional): Timeout for requests, defaults to 1500 seconds.

        Returns:
            DaprChatClient: An instance of DaprChatClient configured with the model settings from the Prompty source.
        """
        # Load the Prompty instance from the provided source
        prompty_instance = Prompty.load(prompty_source)

        # Generate the prompt template from the Prompty instance
        prompt_template = Prompty.to_prompt_template(prompty_instance)

        # Initialize the DaprChatClient based on the Prompty model configuration
        return cls.model_validate(
            {
                "timeout": timeout,
                "prompty": prompty_instance,
                "prompt_template": prompt_template,
            }
        )

    def translate_response(self, response: dict, model: str) -> dict:
        """Converts a Dapr response dict into a structure compatible with Choice and ChatCompletion."""
        choices = [
            {
                "finish_reason": "stop",
                "index": i,
                "message": {"content": output["result"], "role": "assistant"},
                "logprobs": None,
            }
            for i, output in enumerate(response.get("outputs", []))
        ]

        return {
            "choices": choices,
            "created": int(time.time()),
            "model": model,
            "object": "chat.completion",
            "usage": {"total_tokens": "-1"},
        }

    def convert_to_conversation_inputs(
        self, inputs: List[Dict[str, Any]]
    ) -> List[ConversationInput]:
        return [
            ConversationInput(
                content=item["content"],
                role=item.get("role"),
                scrub_pii=item.get("scrubPII") == "true",
            )
            for item in inputs
        ]

    def generate(
        self,
        messages: Union[
            str,
            Dict[str, Any],
            BaseMessage,
            Iterable[Union[Dict[str, Any], BaseMessage]],
        ] = None,
        input_data: Optional[Dict[str, Any]] = None,
        llm_component: Optional[str] = None,
        tools: Optional[List[Union[AgentTool, Dict[str, Any]]]] = None,
        response_format: Optional[Type[BaseModel]] = None,
        structured_mode: Literal["function_call"] = "function_call",
        scrubPII: Optional[bool] = False,
        temperature: Optional[float] = None,
        stream: Optional[bool] = False,
        context_id: Optional[str] = None,
        **kwargs,
    ) -> Union[Iterator[Dict[str, Any]], Dict[str, Any]]:
        """
        Generate chat completions based on provided messages or input_data for prompt templates.

        Args:
            messages (Optional): Either pre-set messages or None if using input_data.
            input_data (Optional[Dict[str, Any]]): Input variables for prompt templates.
            llm_component (str): Name of the LLM component to use for the request.
            tools (List[Union[AgentTool, Dict[str, Any]]]): List of tools for the request.
            response_format (Type[BaseModel]): Optional Pydantic model for structured response parsing.
            structured_mode (Literal["function_call"]): Mode for structured output: "function_call" (Limited Support).
            scrubPII (Type[bool]): Optional flag to obfuscate any sensitive information coming back from the LLM.
            temperature (Optional[float]): Temperature setting for the LLM to optimize for creativity or predictability.
            stream (Optional[bool]): Whether to stream the response. Defaults to False.
            context_id (Optional[str]): Optional context ID for continuing an existing conversation.
            **kwargs: Additional parameters for the language model.

        Returns:
            Union[Iterator[Dict[str, Any]], Dict[str, Any]]: The chat completion response(s).
        """
        if structured_mode not in self.SUPPORTED_STRUCTURED_MODES:
            raise ValueError(
                f"Invalid structured_mode '{structured_mode}'. Must be one of {self.SUPPORTED_STRUCTURED_MODES}."
            )

        # If input_data is provided, check for a prompt_template
        if input_data:
            if not self.prompt_template:
                raise ValueError(
                    "Inputs are provided but no 'prompt_template' is set. Please set a 'prompt_template' to use the input_data."
                )

            logger.info("Using prompt template to generate messages.")
            messages = self.prompt_template.format_prompt(**input_data)

        # Ensure we have messages at this point
        if not messages:
            raise ValueError("Either 'messages' or 'input_data' must be provided.")

        # Process and normalize the messages
        params = {"inputs": RequestHandler.normalize_chat_messages(messages)}
        # Merge Prompty parameters if available, then override with any explicit kwargs
        if self.prompty:
            params = {**self.prompty.model.parameters.model_dump(), **params, **kwargs}
        else:
            params.update(kwargs)

        # Prepare request parameters
        params = RequestHandler.process_params(
            params,
            llm_provider=self.provider,
            tools=tools,
            response_format=response_format,
            structured_mode=structured_mode,
        )
        
        # Override stream parameter if explicitly provided
        if stream is not None:
            params["stream"] = stream
            
        inputs = self.convert_to_conversation_inputs(params["inputs"])

        try:
            # Use streaming or non-streaming API based on the stream parameter
            if params.get("stream", False):
                logger.info("Invoking the Dapr Streaming Conversation API.")
                response_stream = self.client.chat_completion_stream(
                    llm=llm_component or self._llm_component,
                    conversation_inputs=inputs,
                    context_id=context_id,
                    scrub_pii=scrubPII,
                    temperature=temperature,
                )
                logger.info("Streaming chat completion started successfully.")

                return ResponseHandler.process_response(
                    response_stream,
                    llm_provider=self.provider,
                    response_format=response_format,
                    structured_mode=structured_mode,
                    stream=True,
                )
            else:
                logger.info("Invoking the Dapr Conversation API.")
                response = self.client.chat_completion(
                    llm=llm_component or self._llm_component,
                    conversation_inputs=inputs,
                    scrub_pii=scrubPII,
                    temperature=temperature,
                )
                transposed_response = self.translate_response(response, self._llm_component)
                logger.info("Chat completion retrieved successfully.")

                return ResponseHandler.process_response(
                    transposed_response,
                    llm_provider=self.provider,
                    response_format=response_format,
                    structured_mode=structured_mode,
                    stream=False,
                )
        except Exception as e:
            logger.error(
                f"An error occurred during the Dapr Conversation API call: {e}"
            )
            raise

================
File: llm/dapr/client.py
================
from dapr_agents.types.llm import DaprInferenceClientConfig
from dapr_agents.llm.base import LLMClientBase
from dapr.clients import DaprClient
from dapr.clients.grpc._request import ConversationInput
from dapr.clients.grpc._response import ConversationResponse
from typing import Dict, Any, List, Iterator, Optional
from pydantic import model_validator

import logging

logger = logging.getLogger(__name__)


class DaprInferenceClient:
    def __init__(self):
        self.dapr_client = DaprClient()

    def translate_to_json(self, response: ConversationResponse) -> dict:
        response_dict = {
            "outputs": [
                {
                    "result": output.result,
                }
                for output in response.outputs
            ]
        }

        return response_dict



    def chat_completion(
        self,
        llm: str,
        conversation_inputs: List[ConversationInput],
        scrub_pii: bool | None = None,
        temperature: float | None = None,
    ) -> Any:
        response = self.dapr_client.converse_alpha1(
            name=llm,
            inputs=conversation_inputs,
            scrub_pii=scrub_pii,
            temperature=temperature,
        )
        output = self.translate_to_json(response)

        return output

    def chat_completion_stream(
        self,
        llm: str,
        conversation_inputs: List[ConversationInput],
        context_id: str | None = None,
        scrub_pii: bool | None = None,
        temperature: float | None = None,
    ) -> Iterator[Dict[str, Any]]:
        """
        Stream chat completion responses using Dapr's converse_stream_alpha1 API.

        Args:
            llm: Name of the LLM component to use
            conversation_inputs: List of conversation inputs
            context_id: Optional context ID for continuing conversation
            scrub_pii: Optional flag to scrub PII from inputs and outputs
            temperature: Optional temperature setting for the LLM

        Yields:
            Dict[str, Any]: JSON-formatted streaming response chunks compatible with common LLM APIs
        """
        logger.info(f"Starting streaming conversation with LLM component: {llm}")
        
        try:
            # Use converse_stream_alpha1 and transform to JSON format
            for chunk in self.dapr_client.converse_stream_alpha1(
                name=llm,
                inputs=conversation_inputs,
                context_id=context_id,
                scrub_pii=scrub_pii,
                temperature=temperature,
            ):
                # Transform the chunk to JSON format compatible with common LLM APIs
                chunk_dict = {
                    'choices': [],
                    'context_id': None,
                    'usage': None,
                }

                # Handle streaming result chunks
                if hasattr(chunk, 'result') and chunk.result and hasattr(chunk.result, 'result'):
                    chunk_dict['choices'] = [
                        {
                            'delta': {
                                'content': chunk.result.result,
                                'role': 'assistant'
                            },
                            'index': 0,
                            'finish_reason': None
                        }
                    ]

                # Handle context ID
                if hasattr(chunk, 'context_id') and chunk.context_id:
                    chunk_dict['context_id'] = chunk.context_id

                # Handle usage information (typically in the final chunk)
                if hasattr(chunk, 'usage') and chunk.usage:
                    chunk_dict['usage'] = {
                        'prompt_tokens': getattr(chunk.usage, 'prompt_tokens', 0),
                        'completion_tokens': getattr(chunk.usage, 'completion_tokens', 0),
                        'total_tokens': getattr(chunk.usage, 'total_tokens', 0),
                    }

                yield chunk_dict

        except Exception as e:
            logger.error(f"Error during streaming conversation: {e}")
            raise


class DaprInferenceClientBase(LLMClientBase):
    """
    Base class for managing Dapr Inference API clients.
    Handles client initialization, configuration, and shared logic.
    """

    @model_validator(mode="before")
    def validate_and_initialize(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        return values

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes private attributes after validation.
        """
        self._provider = "dapr"

        # Set up the private config and client attributes
        self._config = self.get_config()
        self._client = self.get_client()
        return super().model_post_init(__context)

    def get_config(self) -> DaprInferenceClientConfig:
        """
        Returns the appropriate configuration for the Dapr Conversation API.
        """
        return DaprInferenceClientConfig()

    def get_client(self) -> DaprInferenceClient:
        """
        Initializes and returns the Dapr Inference client.
        """
        return DaprInferenceClient()

    @classmethod
    def from_config(
        cls, client_options: DaprInferenceClientConfig, timeout: float = 1500
    ):
        """
        Initializes the DaprInferenceClientBase using DaprInferenceClientConfig.

        Args:
            client_options: The configuration options for the client.
            timeout: Timeout for requests (default is 1500 seconds).

        Returns:
            DaprInferenceClientBase: The initialized client instance.
        """
        return cls()

    @property
    def config(self) -> Dict[str, Any]:
        return self._config

    @property
    def client(self) -> DaprInferenceClient:
        return self._client

================
File: llm/elevenlabs/__init__.py
================
from .speech import ElevenLabsSpeechClient

================
File: llm/elevenlabs/client.py
================
from dapr_agents.types.llm import ElevenLabsClientConfig
from dapr_agents.llm.base import LLMClientBase
from typing import Any, Optional
from pydantic import Field
import os
import logging

logger = logging.getLogger(__name__)


class ElevenLabsClientBase(LLMClientBase):
    """
    Base class for managing ElevenLabs LLM clients.
    Handles client initialization, configuration, and shared logic specific to the ElevenLabs API.
    """

    api_key: Optional[str] = Field(
        default=None,
        description="API key for authenticating with the ElevenLabs API. Defaults to environment variables 'ELEVENLABS_API_KEY' or 'ELEVEN_API_KEY'.",
    )
    base_url: Optional[str] = Field(
        default="https://api.elevenlabs.io",
        description="Base URL for the ElevenLabs API endpoints.",
    )

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes private attributes and performs any post-validation setup.

        This includes setting up provider-specific attributes such as configuration and client instances.
        """
        self._provider = "elevenlabs"

        # Use environment variable if `api_key` is not explicitly provided
        if self.api_key is None:
            self.api_key = os.getenv("ELEVENLABS_API_KEY") or os.getenv(
                "ELEVEN_API_KEY"
            )

        if self.api_key is None:
            raise ValueError(
                "API key is required. Set it explicitly or in the 'ELEVENLABS_API_KEY' or 'ELEVEN_API_KEY' environment variable."
            )

        # Initialize configuration and client
        self._config = self.get_config()
        self._client = self.get_client()
        logger.info("ElevenLabs client initialized successfully.")

        return super().model_post_init(__context)

    def get_config(self) -> ElevenLabsClientConfig:
        """
        Returns the configuration object for the ElevenLabs API client.
        """
        return ElevenLabsClientConfig(api_key=self.api_key, base_url=self.base_url)

    def get_client(self) -> Any:
        """
        Initializes and returns the ElevenLabs API client.

        This method sets up the client using the provided configuration.
        """
        try:
            from elevenlabs import ElevenLabs
        except ImportError as e:
            raise ImportError(
                "The 'elevenlabs' package is required but not installed. Install it with 'pip install elevenlabs'."
            ) from e

        config = self.config

        logger.info("Initializing ElevenLabs API client...")
        return ElevenLabs(api_key=config.api_key, base_url=config.base_url)

    @property
    def config(self) -> ElevenLabsClientConfig:
        """
        Provides access to the ElevenLabs API client configuration.
        """
        return self._config

    @property
    def client(self) -> Any:
        """
        Provides access to the ElevenLabs API client instance.
        """
        return self._client

================
File: llm/elevenlabs/speech.py
================
from dapr_agents.llm.elevenlabs.client import ElevenLabsClientBase
from typing import Optional, Union, Any
from pydantic import Field
import logging

logger = logging.getLogger(__name__)


class ElevenLabsSpeechClient(ElevenLabsClientBase):
    """
    Client for ElevenLabs speech generation functionality.
    Handles text-to-speech conversions with customizable options.
    """

    voice: Optional[Any] = Field(
        default=None,
        description="Default voice (ID, name, or object) for speech generation.",
    )
    model: Optional[str] = Field(
        default="eleven_multilingual_v2",
        description="Default model for speech generation.",
    )
    output_format: Optional[str] = Field(
        default="mp3_44100_128", description="Default audio output format."
    )
    optimize_streaming_latency: Optional[int] = Field(
        default=0,
        description="Default latency optimization level (0 means no optimizations).",
    )
    voice_settings: Optional[Any] = Field(
        default=None,
        description="Default voice settings (stability, similarity boost, etc.).",
    )

    def model_post_init(self, __context: Any) -> None:
        """
        Post-initialization logic for the ElevenLabsSpeechClient.
        Dynamically imports ElevenLabs components and validates voice attributes.
        """
        super().model_post_init(__context)

        from elevenlabs import VoiceSettings
        from elevenlabs.client import DEFAULT_VOICE

        # Set default voice settings if not already set
        if self.voice_settings is None:
            self.voice_settings = VoiceSettings(
                stability=DEFAULT_VOICE.settings.stability,
                similarity_boost=DEFAULT_VOICE.settings.similarity_boost,
                style=DEFAULT_VOICE.settings.style,
                use_speaker_boost=DEFAULT_VOICE.settings.use_speaker_boost,
            )

        # Set default voice if not provided
        if self.voice is None:
            self.voice = DEFAULT_VOICE

    def create_speech(
        self,
        text: str,
        file_name: Optional[str] = None,
        voice: Optional[Union[str, Any]] = None,
        model: Optional[str] = None,
        output_format: Optional[str] = None,
        optimize_streaming_latency: Optional[int] = None,
        voice_settings: Optional[Any] = None,
        overwrite_file: bool = True,
    ) -> Union[bytes, None]:
        """
        Generate speech audio from text and optionally save it to a file.

        Args:
            text (str): The text to convert to speech.
            file_name (Optional[str]): Optional file name to save the generated audio.
            voice (Optional[Union[str, Voice]]): Override default voice for this request (ID, name, or object).
            model (Optional[str]): Override default model for this request.
            output_format (Optional[str]): Override default output format for this request.
            optimize_streaming_latency (Optional[int]): Override default latency optimization level.
            voice_settings (Optional[VoiceSettings]): Override default voice settings (stability, similarity boost, etc.).
            overwrite_file (bool): Whether to overwrite the file if it exists. Defaults to True.

        Returns:
            Union[bytes, None]: The generated audio as bytes if no `file_name` is provided; otherwise, None.
        """
        # Apply defaults if arguments are not provided
        voice = voice or self.voice
        model = model or self.model
        output_format = output_format or self.output_format
        optimize_streaming_latency = (
            optimize_streaming_latency or self.optimize_streaming_latency
        )
        voice_settings = voice_settings or self.voice_settings

        logger.info(f"Generating speech with voice '{voice}', model '{model}'.")

        try:
            audio_chunks = self.client.generate(
                text=text,
                voice=voice,
                model=model,
                output_format=output_format,
                optimize_streaming_latency=optimize_streaming_latency,
                voice_settings=voice_settings,
            )

            if file_name:
                file_mode = "wb" if overwrite_file else "ab"
                logger.info(f"Saving audio to file: {file_name} (mode: {file_mode})")
                with open(file_name, file_mode) as audio_file:
                    for chunk in audio_chunks:
                        audio_file.write(chunk)
                logger.info(f"Audio saved to {file_name}")
                return None
            else:
                logger.info("Collecting audio bytes.")
                return b"".join(audio_chunks)

        except Exception as e:
            logger.error(f"Failed to generate speech: {e}")
            raise ValueError(f"An error occurred during speech generation: {e}")

================
File: llm/huggingface/__init__.py
================
from .chat import HFHubChatClient
from .client import HFHubInferenceClientBase

================
File: llm/huggingface/chat.py
================
from dapr_agents.llm.huggingface.client import HFHubInferenceClientBase
from dapr_agents.llm.utils import RequestHandler, ResponseHandler
from dapr_agents.prompt.prompty import Prompty
from dapr_agents.types.message import BaseMessage
from dapr_agents.llm.chat import ChatClientBase
from dapr_agents.tool import AgentTool
from typing import (
    Union,
    Optional,
    Iterable,
    Dict,
    Any,
    List,
    Iterator,
    Type,
    Literal,
    ClassVar,
)
from pydantic import BaseModel
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class HFHubChatClient(HFHubInferenceClientBase, ChatClientBase):
    """
    Concrete class for the Hugging Face Hub's chat completion API using the Inference API.
    This class extends the ChatClientBase and provides the necessary configurations for Hugging Face models.
    """

    SUPPORTED_STRUCTURED_MODES: ClassVar[set] = {"function_call"}

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes private attributes for provider, api, config, and client after validation.
        """
        # Set the private provider and api attributes
        self._api = "chat"
        return super().model_post_init(__context)

    @classmethod
    def from_prompty(
        cls,
        prompty_source: Union[str, Path],
        timeout: Union[int, float, Dict[str, Any]] = 1500,
    ) -> "HFHubChatClient":
        """
        Initializes an HFHubChatClient client using a Prompty source, which can be a file path or inline content.

        Args:
            prompty_source (Union[str, Path]): The source of the Prompty file, which can be a path to a file
                or inline Prompty content as a string.
            timeout (Union[int, float, Dict[str, Any]], optional): Timeout for requests, defaults to 1500 seconds.

        Returns:
            HFHubChatClient: An instance of HFHubChatClient configured with the model settings from the Prompty source.
        """
        # Load the Prompty instance from the provided source
        prompty_instance = Prompty.load(prompty_source)

        # Generate the prompt template from the Prompty instance
        prompt_template = Prompty.to_prompt_template(prompty_instance)

        # Extract the model configuration from Prompty
        model_config = prompty_instance.model

        # Initialize the HFHubChatClient based on the Prompty model configuration
        return cls.model_validate(
            {
                "model": model_config.configuration.name,
                "api_key": model_config.configuration.api_key,
                "base_url": model_config.configuration.base_url,
                "headers": model_config.configuration.headers,
                "cookies": model_config.configuration.cookies,
                "proxies": model_config.configuration.proxies,
                "timeout": timeout,
                "prompty": prompty_instance,
                "prompt_template": prompt_template,
            }
        )

    def generate(
        self,
        messages: Union[
            str,
            Dict[str, Any],
            BaseMessage,
            Iterable[Union[Dict[str, Any], BaseMessage]],
        ] = None,
        input_data: Optional[Dict[str, Any]] = None,
        model: Optional[str] = None,
        tools: Optional[List[Union[AgentTool, Dict[str, Any]]]] = None,
        response_format: Optional[Type[BaseModel]] = None,
        structured_mode: Literal["function_call"] = "function_call",
        **kwargs,
    ) -> Union[Iterator[Dict[str, Any]], Dict[str, Any]]:
        """
        Generate chat completions based on provided messages or input_data for prompt templates.

        Args:
            messages (Optional): Either pre-set messages or None if using input_data.
            input_data (Optional[Dict[str, Any]]): Input variables for prompt templates.
            model (str): Specific model to use for the request, overriding the default.
            tools (List[Union[AgentTool, Dict[str, Any]]]): List of tools for the request.
            response_format (Type[BaseModel]): Optional Pydantic model for structured response parsing.
            structured_mode (Literal["function_call"]): Mode for structured output: "function_call" (Limited Support).
            **kwargs: Additional parameters for the language model.

        Returns:
            Union[Iterator[Dict[str, Any]], Dict[str, Any]]: The chat completion response(s).
        """

        if structured_mode not in self.SUPPORTED_STRUCTURED_MODES:
            raise ValueError(
                f"Invalid structured_mode '{structured_mode}'. Must be one of {self.SUPPORTED_STRUCTURED_MODES}."
            )

        # If input_data is provided, check for a prompt_template
        if input_data:
            if not self.prompt_template:
                raise ValueError(
                    "Inputs are provided but no 'prompt_template' is set. Please set a 'prompt_template' to use the input_data."
                )

            logger.info("Using prompt template to generate messages.")
            messages = self.prompt_template.format_prompt(**input_data)

        # Ensure we have messages at this point
        if not messages:
            raise ValueError("Either 'messages' or 'input_data' must be provided.")

        # Process and normalize the messages
        params = {"messages": RequestHandler.normalize_chat_messages(messages)}

        # Merge Prompty parameters if available, then override with any explicit kwargs
        if self.prompty:
            params = {**self.prompty.model.parameters.model_dump(), **params, **kwargs}
        else:
            params.update(kwargs)

        # If a model is provided, override the default model
        params["model"] = model or self.model

        # Prepare request parameters
        params = RequestHandler.process_params(
            params,
            llm_provider=self.provider,
            tools=tools,
            response_format=response_format,
            structured_mode=structured_mode,
        )

        try:
            logger.info("Invoking Hugging Face ChatCompletion API.")
            response = self.client.chat_completion(**params)
            logger.info("Chat completion retrieved successfully.")

            return ResponseHandler.process_response(
                response,
                llm_provider=self.provider,
                response_format=response_format,
                structured_mode=structured_mode,
                stream=params.get("stream", False),
            )
        except Exception as e:
            logger.error(f"An error occurred during the ChatCompletion API call: {e}")
            raise

================
File: llm/huggingface/client.py
================
from dapr_agents.types.llm import HFInferenceClientConfig
from dapr_agents.llm.base import LLMClientBase
from typing import Optional, Dict, Any, Union
from huggingface_hub import InferenceClient
from pydantic import Field, model_validator
import os
import logging

logger = logging.getLogger(__name__)


class HFHubInferenceClientBase(LLMClientBase):
    """
    Base class for managing Hugging Face Inference API clients.
    Handles client initialization, configuration, and shared logic.
    """

    model: Optional[str] = Field(
        default=None,
        description="Model ID or URL for the Hugging Face API. Cannot be used with `base_url`. If set, the client will infer a model-specific endpoint.",
    )
    token: Optional[Union[str, bool]] = Field(
        default=None,
        description="Hugging Face token. Defaults to the locally saved token if not provided. Pass `False` to disable authentication.",
    )
    api_key: Optional[Union[str, bool]] = Field(
        default=None,
        description="Alias for `token` for compatibility with OpenAI's client. Cannot be used if `token` is set.",
    )
    base_url: Optional[str] = Field(
        default=None,
        description="Base URL to run inference. Alias for `model`. Cannot be used if `model` is set.",
    )
    headers: Optional[Dict[str, str]] = Field(
        default=None,
        description="Additional headers to send to the server. Overrides the default authorization and user-agent headers.",
    )
    cookies: Optional[Dict[str, str]] = Field(
        default=None, description="Additional cookies to send to the server."
    )
    proxies: Optional[Any] = Field(
        default=None, description="Proxies to use for the request."
    )
    timeout: Optional[float] = Field(
        default=None,
        description="The maximum number of seconds to wait for a response from the server. Loading a new model in Inference. API can take up to several minutes. Defaults to None, meaning it will loop until the server is available.",
    )

    @model_validator(mode="before")
    def validate_and_initialize(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        """
        Ensures consistency for 'api_key' and 'token' fields before initialization.
        - Normalizes 'token' and 'api_key' to a single field.
        - Validates exclusivity of 'model' and 'base_url'.
        """
        token = values.get("token")
        api_key = values.get("api_key")
        model = values.get("model")
        base_url = values.get("base_url")

        # Ensure mutual exclusivity of `token` and `api_key`
        if token is not None and api_key is not None:
            raise ValueError(
                "Provide only one of 'api_key' or 'token'. They are aliases and cannot coexist."
            )

        # Normalize `token` to `api_key`
        if token is not None:
            values["api_key"] = token
            values.pop("token", None)  # Remove `token` for consistency

        # Use environment variable if `api_key` is not explicitly provided
        if api_key is None:
            api_key = os.environ.get("HUGGINGFACE_API_KEY")

        if api_key is None:
            raise ValueError(
                "API key is required. Set it explicitly or in the 'HUGGINGFACE_API_KEY' environment variable."
            )

        values["api_key"] = api_key

        # mutual‑exclusivity
        if model is not None and base_url is not None:
            raise ValueError("Cannot provide both 'model' and 'base_url'.")

        # require at least one
        if model is None and base_url is None:
            raise ValueError(
                "HF Inference needs either `model` or `base_url`. "
                "E.g. model='gpt2' or base_url='https://…/models/gpt2'."
            )

        # auto‑derive model from base_url
        if model is None:
            derived = base_url.rstrip("/").split("/")[-1]
            values["model"] = derived

        return values

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes private attributes after validation.
        """
        self._provider = "huggingface"

        # Set up the private config and client attributes
        self._config = self.get_config()
        self._client = self.get_client()
        return super().model_post_init(__context)

    def get_config(self) -> HFInferenceClientConfig:
        """
        Returns the appropriate configuration for the Hugging Face Inference API.
        """
        return HFInferenceClientConfig(
            model=self.model,
            api_key=self.api_key,
            base_url=self.base_url,
            headers=self.headers,
            cookies=self.cookies,
            proxies=self.proxies,
            timeout=self.timeout,
        )

    def get_client(self) -> InferenceClient:
        """
        Initializes and returns the Hugging Face Inference client.
        """
        config: HFInferenceClientConfig = self.config
        return InferenceClient(
            model=config.model,
            api_key=config.api_key,
            base_url=config.base_url,
            headers=config.headers,
            cookies=config.cookies,
            proxies=config.proxies,
            timeout=self.timeout,
        )

    @classmethod
    def from_config(
        cls, client_options: HFInferenceClientConfig, timeout: float = 1500
    ):
        """
        Initializes the HFHubInferenceClientBase using HFInferenceClientConfig.

        Args:
            client_options: The configuration options for the client.
            timeout: Timeout for requests (default is 1500 seconds).

        Returns:
            HFHubInferenceClientBase: The initialized client instance.
        """
        return cls(
            model=client_options.model,
            api_key=client_options.api_key,
            token=client_options.token,
            base_url=client_options.base_url,
            headers=client_options.headers,
            cookies=client_options.cookies,
            proxies=client_options.proxies,
            timeout=timeout,
        )

    @property
    def config(self) -> Dict[str, Any]:
        return self._config

    @property
    def client(self) -> InferenceClient:
        return self._client

================
File: llm/nvidia/__init__.py
================
from .client import NVIDIAClientBase
from .chat import NVIDIAChatClient
from .embeddings import NVIDIAEmbeddingClient

================
File: llm/nvidia/chat.py
================
from dapr_agents.llm.utils import RequestHandler, ResponseHandler
from dapr_agents.llm.nvidia.client import NVIDIAClientBase
from dapr_agents.types.message import BaseMessage
from dapr_agents.llm.chat import ChatClientBase
from dapr_agents.prompt.prompty import Prompty
from dapr_agents.tool import AgentTool
from typing import (
    Union,
    Optional,
    Iterable,
    Dict,
    Any,
    List,
    Iterator,
    Type,
    Literal,
    ClassVar,
)
from openai.types.chat import ChatCompletionMessage
from pydantic import BaseModel, Field
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class NVIDIAChatClient(NVIDIAClientBase, ChatClientBase):
    """
    Chat client for NVIDIA chat models.
    Combines NVIDIA client management with Prompty-specific functionality for handling chat completions.
    """

    model: str = Field(
        default="meta/llama3-8b-instruct",
        description="Model name to use. Defaults to 'meta/llama3-8b-instruct'.",
    )
    max_tokens: Optional[int] = Field(
        default=1024,
        description=(
            "The maximum number of tokens to generate in any given call. Must be an integer ≥ 1. Defaults to 1024."
        ),
    )

    SUPPORTED_STRUCTURED_MODES: ClassVar[set] = {"function_call"}

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes chat-specific attributes after validation.

        Args:
            __context (Any): Additional context for post-initialization (not used here).
        """
        self._api = "chat"
        super().model_post_init(__context)

    @classmethod
    def from_prompty(cls, prompty_source: Union[str, Path]) -> "NVIDIAChatClient":
        """
        Initializes an NVIDIAChatClient client using a Prompty source, which can be a file path or inline content.

        Args:
            prompty_source (Union[str, Path]): The source of the Prompty file, which can be a path to a file
                or inline Prompty content as a string.

        Returns:
            NVIDIAChatClient: An instance of NVIDIAChatClient configured with the model settings from the Prompty source.
        """
        # Load the Prompty instance from the provided source
        prompty_instance = Prompty.load(prompty_source)

        # Generate the prompt template from the Prompty instance
        prompt_template = Prompty.to_prompt_template(prompty_instance)

        # Extract the model configuration from Prompty
        model_config = prompty_instance.model

        # Initialize the NVIDIAChatClient instance using model_validate
        return cls.model_validate(
            {
                "model": model_config.configuration.name,
                "api_key": model_config.configuration.api_key,
                "base_url": model_config.configuration.base_url,
                "prompty": prompty_instance,
                "prompt_template": prompt_template,
            }
        )

    def generate(
        self,
        messages: Union[
            str,
            Dict[str, Any],
            BaseMessage,
            Iterable[Union[Dict[str, Any], BaseMessage]],
        ] = None,
        input_data: Optional[Dict[str, Any]] = None,
        model: Optional[str] = None,
        tools: Optional[List[Union[AgentTool, Dict[str, Any]]]] = None,
        response_format: Optional[Type[BaseModel]] = None,
        max_tokens: Optional[int] = None,
        structured_mode: Literal["function_call"] = "function_call",
        **kwargs,
    ) -> Union[Iterator[Dict[str, Any]], Dict[str, Any]]:
        """
        Generate chat completions based on provided messages or input_data for prompt templates.

        Args:
            messages (Optional): Either pre-set messages or None if using input_data.
            input_data (Optional[Dict[str, Any]]): Input variables for prompt templates.
            model (str): Specific model to use for the request, overriding the default.
            tools (List[Union[AgentTool, Dict[str, Any]]]): List of tools for the request.
            response_format (Type[BaseModel]): Optional Pydantic model for structured response parsing.
            max_tokens (Optional[int]): The maximum number of tokens to generate. Defaults to the instance setting.
            structured_mode (Literal["function_call"]): Mode for structured output: "function_call" (Limited Support).
            **kwargs: Additional parameters for the language model.

        Returns:
            Union[Iterator[Dict[str, Any]], Dict[str, Any]]: The chat completion response(s).
        """

        if structured_mode not in self.SUPPORTED_STRUCTURED_MODES:
            raise ValueError(
                f"Invalid structured_mode '{structured_mode}'. Must be one of {self.SUPPORTED_STRUCTURED_MODES}."
            )

        # If input_data is provided, check for a prompt_template
        if input_data:
            if not self.prompt_template:
                raise ValueError(
                    "Inputs are provided but no 'prompt_template' is set. Please set a 'prompt_template' to use the input_data."
                )

            logger.info("Using prompt template to generate messages.")
            messages = self.prompt_template.format_prompt(**input_data)

        # Ensure we have messages at this point
        if not messages:
            raise ValueError("Either 'messages' or 'input_data' must be provided.")

        # Process and normalize the messages
        params = {"messages": RequestHandler.normalize_chat_messages(messages)}

        # Merge prompty parameters if available, then override with any explicit kwargs
        if self.prompty:
            params = {**self.prompty.model.parameters.model_dump(), **params, **kwargs}
        else:
            params.update(kwargs)

        # If a model is provided, override the default model
        params["model"] = model or self.model

        # Apply max_tokens if provided
        params["max_tokens"] = max_tokens or self.max_tokens

        # Prepare request parameters
        params = RequestHandler.process_params(
            params,
            llm_provider=self.provider,
            tools=tools,
            response_format=response_format,
            structured_mode=structured_mode,
        )

        try:
            logger.info("Invoking ChatCompletion API.")
            logger.debug(f"ChatCompletion API Parameters:{params}")
            response: ChatCompletionMessage = self.client.chat.completions.create(
                **params
            )
            logger.info("Chat completion retrieved successfully.")

            return ResponseHandler.process_response(
                response,
                llm_provider=self.provider,
                response_format=response_format,
                structured_mode=structured_mode,
                stream=params.get("stream", False),
            )
        except Exception as e:
            logger.error(f"An error occurred during the ChatCompletion API call: {e}")
            raise

================
File: llm/nvidia/client.py
================
from dapr_agents.types.llm import NVIDIAClientConfig
from dapr_agents.llm.base import LLMClientBase
from typing import Any, Optional
from pydantic import Field
from openai import OpenAI
import os
import logging

logger = logging.getLogger(__name__)


class NVIDIAClientBase(LLMClientBase):
    """
    Base class for managing NVIDIA LLM clients.
    Handles client initialization, configuration, and shared logic specific to NVIDIA's API.
    """

    api_key: Optional[str] = Field(
        default=None,
        description="API key for authenticating with the NVIDIA LLM API. If not provided, it will be sourced from the 'NVIDIA_API_KEY' environment variable.",
    )
    base_url: Optional[str] = Field(
        default="https://integrate.api.nvidia.com/v1",
        description="Base URL for the NVIDIA LLM API endpoints.",
    )

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes private attributes and performs any post-validation setup.

        This includes setting up provider-specific attributes such as configuration and client instances.

        Args:
            __context (Any): Additional context for post-initialization (not used here).
        """
        self._provider = "nvidia"

        # Use environment variable if `api_key` is not explicitly provided
        if self.api_key is None:
            self.api_key = os.environ.get("NVIDIA_API_KEY")

        if self.api_key is None:
            raise ValueError(
                "API key is required. Set it explicitly or in the 'NVIDIA_API_KEY' environment variable."
            )

        # Set up the private config and client attributes
        self._config: NVIDIAClientConfig = self.get_config()
        self._client: OpenAI = self.get_client()
        return super().model_post_init(__context)

    def get_config(self) -> NVIDIAClientConfig:
        """
        Returns the configuration object for the NVIDIA LLM API client.

        This configuration includes the API key and base URL, ensuring the client can communicate with the API.

        Returns:
            NVIDIAClientConfig: Configuration object containing API credentials and endpoint details.
        """
        return NVIDIAClientConfig(api_key=self.api_key, base_url=self.base_url)

    def get_client(self) -> OpenAI:
        """
        Initializes and returns the NVIDIA LLM API client.

        This method sets up the client using the provided configuration.

        Returns:
            OpenAI: The initialized NVIDIA API client instance.
        """
        config = self.config

        logger.info("Initializing NVIDIA API client...")
        return OpenAI(api_key=config.api_key, base_url=config.base_url)

    @property
    def config(self) -> NVIDIAClientConfig:
        """
        Provides access to the NVIDIA API client configuration.

        Returns:
            NVIDIAClientConfig: Configuration object for the NVIDIA API client.
        """
        return self._config

    @property
    def client(self) -> OpenAI:
        """
        Provides access to the NVIDIA API client instance.

        Returns:
            OpenAI: The NVIDIA API client instance.
        """
        return self._client

================
File: llm/nvidia/embeddings.py
================
from openai.types.create_embedding_response import CreateEmbeddingResponse
from dapr_agents.llm.nvidia.client import NVIDIAClientBase
from typing import Union, Dict, Any, Literal, List, Optional
from pydantic import Field
import logging

logger = logging.getLogger(__name__)


class NVIDIAEmbeddingClient(NVIDIAClientBase):
    """
    Client for handling NVIDIA's embedding functionalities.

    Attributes:
        model (str): The ID of the model to use for embedding. Required for NVIDIA embeddings.
        encoding_format (Optional[Literal["float", "base64"]]): The format of the embeddings. Defaults to 'float'.
        dimensions (Optional[int]): Number of dimensions for the output embeddings. Not supported by all models.
        input_type (Optional[Literal["query", "passage"]]): Specifies the mode of operation for embeddings.
            'query' for generating embeddings during querying.
            'passage' for generating embeddings during indexing.
        truncate (Optional[Literal["NONE", "START", "END"]]): Specifies handling for inputs exceeding the model's max token length. Defaults to 'NONE'.
    """

    model: str = Field(
        "nvidia/nv-embedqa-e5-v5", description="ID of the model to use for embedding."
    )
    encoding_format: Optional[Literal["float", "base64"]] = Field(
        "float", description="Format for the embeddings. Defaults to 'float'."
    )
    dimensions: Optional[int] = Field(
        None,
        description="Number of dimensions for the output embeddings. Not supported by all models.",
    )
    input_type: Optional[Literal["query", "passage"]] = Field(
        "passage", description="Mode of operation: 'query' or 'passage'."
    )
    truncate: Optional[Literal["NONE", "START", "END"]] = Field(
        "NONE",
        description="Handling for inputs exceeding max token length. Defaults to 'NONE'.",
    )

    def model_post_init(self, __context: Any) -> None:
        """
        Post-initialization setup for private attributes.

        This method configures the API endpoint for embedding operations.

        Args:
            __context (Any): Context provided during model initialization.
        """
        self._api = "embeddings"
        return super().model_post_init(__context)

    def create_embedding(
        self,
        input: Union[str, List[str]],
        model: Optional[str] = None,
        input_type: Optional[Literal["query", "passage"]] = None,
        truncate: Optional[Literal["NONE", "START", "END"]] = None,
        encoding_format: Optional[Literal["float", "base64"]] = None,
        dimensions: Optional[int] = None,
        extra_body: Optional[Dict[str, Any]] = None,
    ) -> CreateEmbeddingResponse:
        """
        Generate embeddings for the given input text(s).

        Args:
            input (Union[str, List[str]]): Input text(s) to generate embeddings for.
                - A single string for one input.
                - A list of strings for multiple inputs.
            model (Optional[str]): Model to use for embedding. Overrides the default model if provided.
            input_type (Optional[Literal["query", "passage"]]): Specifies the mode of operation. Overrides the default if provided.
            truncate (Optional[Literal["NONE", "START", "END"]]): Handling for inputs exceeding max token length.
            encoding_format (Optional[Literal["float", "base64"]]): Format for the embeddings. Defaults to the instance setting.
            dimensions (Optional[int]): Number of dimensions for the embeddings. Only supported by certain models.
            extra_body (Optional[Dict[str, Any]]): Additional parameters to pass in the request body.

        Returns:
            Dict[str, Any]: A response object containing the generated embeddings and associated metadata.

        Raises:
            ValueError: If the client fails to generate embeddings.
        """
        logger.info(f"Using model '{self.model}' for embedding generation.")

        # If a model is provided, override the default model
        model = model or self.model

        # Prepare request parameters
        body = {
            "model": model,
            "input": input,
            "encoding_format": encoding_format or self.encoding_format,
            "extra_body": extra_body or {},
        }

        # Add optional parameters if provided
        if input_type:
            body["extra_body"]["input_type"] = input_type
        if truncate:
            body["extra_body"]["truncate"] = truncate
        if dimensions:
            body["dimensions"] = dimensions

        logger.debug(f"Embedding request payload: {body}")

        # Send the request to the NVIDIA embeddings endpoint
        try:
            response = self.client.embeddings.create(**body)
            logger.info("Embedding generation successful.")
            return response
        except Exception as e:
            logger.error(f"An error occurred while generating embeddings: {e}")
            raise ValueError(f"Failed to generate embeddings: {e}")

================
File: llm/openai/client/__init__.py
================
from .openai import OpenAIClient
from .azure import AzureOpenAIClient
from .base import OpenAIClientBase

================
File: llm/openai/client/azure.py
================
from azure.identity import (
    DefaultAzureCredential,
    ManagedIdentityCredential,
    get_bearer_token_provider,
)
from dapr_agents.types.llm import AzureOpenAIClientConfig
from dapr_agents.llm.utils import HTTPHelper
from openai import AzureOpenAI
from typing import Union, Optional
import logging
import os

logger = logging.getLogger(__name__)


class AzureOpenAIClient:
    """
    Client for Azure OpenAI language models, handling API communication and authentication.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        azure_ad_token: Optional[str] = None,
        organization: Optional[str] = None,
        project: Optional[str] = None,
        api_version: Optional[str] = None,
        azure_endpoint: Optional[str] = None,
        azure_deployment: Optional[str] = None,
        azure_client_id: Optional[str] = None,
        timeout: Union[int, float, dict] = 1500,
    ):
        """
        Initializes the client with API key or Azure AD credentials.

        Args:
            api_key: Azure OpenAI API key (inferred from env variable if not provided).
            azure_ad_token: Azure AD token (inferred from env variable if not provided).
            organization: Organization name (optional).
            project: Project name (optional).
            api_version: API version (inferred from env variable if not provided).
            azure_endpoint: Azure endpoint (inferred from env variable if not provided).
            azure_deployment: Deployment name (inferred from env variable if not provided).
            azure_client_id: Managed Identity client ID (optional).
            timeout: Request timeout in seconds (default: 1500).
        """
        # Use provided values or fallback to environment variables
        self.api_key = api_key or os.getenv("AZURE_OPENAI_API_KEY")
        self.azure_ad_token = azure_ad_token or os.getenv("AZURE_OPENAI_AD_TOKEN")
        self.organization = organization or os.getenv("OPENAI_ORG_ID")
        self.project = project or os.getenv("OPENAI_PROJECT_ID")
        self.api_version = api_version or os.getenv("AZURE_OPENAI_API_VERSION")
        self.azure_endpoint = azure_endpoint or os.getenv("AZURE_OPENAI_ENDPOINT")
        self.azure_deployment = azure_deployment or os.getenv("AZURE_OPENAI_DEPLOYMENT")
        self.azure_client_id = azure_client_id or os.getenv("AZURE_CLIENT_ID")

        if not self.azure_endpoint or not self.azure_deployment:
            raise ValueError(
                "Azure OpenAI endpoint and deployment must be provided, either via arguments or environment variables."
            )

        self.timeout = HTTPHelper.configure_timeout(timeout)

    def get_client(self) -> AzureOpenAI:
        """
        Returns the Azure OpenAI client.

        Returns:
            AzureOpenAI: The initialized Azure OpenAI client.
        """
        # Authentication: API Key, Azure AD Token, or Azure Identity
        # The api_key, azure_ad_token, and azure_ad_token_provider arguments are mutually exclusive.
        # Case 1: Use API Key
        if self.api_key:
            logger.info("Using API key for authentication.")
            return self._create_client(api_key=self.api_key)

        # Case 2: Use Azure AD Token
        if self.azure_ad_token:
            logger.info("Using Azure AD token for authentication.")
            return self._create_client(azure_ad_token=self.azure_ad_token)

        # Case 3: Use Azure Identity Credentials
        logger.info(
            "No API key or Azure AD token provided, attempting to use Azure Identity credentials."
        )
        try:
            credential = (
                ManagedIdentityCredential(client_id=self.azure_client_id)
                if self.azure_client_id
                else DefaultAzureCredential(exclude_shared_token_cache_credential=True)
            )
            azure_ad_token_provider = get_bearer_token_provider(
                credential, "https://cognitiveservices.azure.com/.default"
            )
            return self._create_client(azure_ad_token_provider=azure_ad_token_provider)
        except Exception as e:
            logger.error(f"Failed to initialize Azure Identity credentials: {e}")
            raise ValueError(
                "Unable to authenticate using Azure Identity credentials. Check your setup."
            ) from e

    def _create_client(self, **kwargs) -> AzureOpenAI:
        """
        Helper method to create and return an Azure OpenAI client.
        """
        return AzureOpenAI(
            azure_endpoint=self.azure_endpoint,
            azure_deployment=self.azure_deployment,
            api_version=self.api_version,
            timeout=self.timeout,
            **kwargs,
        )

    @classmethod
    def from_config(
        cls,
        client_options: AzureOpenAIClientConfig,
        azure_client_id: Optional[str] = None,
        timeout: Union[int, float, dict] = 1500,
    ):
        """
        Initialize AzureOpenAIClient using AzureOpenAIClientOptions.

        Args:
            client_options: An instance of AzureOpenAIClientOptions containing configuration details.
            azure_client_id: Optional Azure client ID for Managed Identity authentication.
            timeout: Optional timeout value for requests (default is 1500 seconds).

        Returns:
            AzureOpenAIClient: An initialized instance of AzureOpenAIClient.
        """
        return cls(
            api_key=client_options.api_key,
            azure_ad_token=client_options.azure_ad_token,
            organization=client_options.organization,
            project=client_options.project,
            api_version=client_options.api_version,
            azure_endpoint=client_options.azure_endpoint,
            azure_deployment=client_options.azure_deployment,
            azure_client_id=azure_client_id,
            timeout=timeout,
        )

================
File: llm/openai/client/base.py
================
from dapr_agents.types.llm import OpenAIClientConfig, AzureOpenAIClientConfig
from dapr_agents.llm.openai.client import AzureOpenAIClient, OpenAIClient
from dapr_agents.llm.base import LLMClientBase
from openai import OpenAI, AzureOpenAI
from typing import Any, Optional, Union, Dict
from pydantic import Field
import logging

logger = logging.getLogger(__name__)


class OpenAIClientBase(LLMClientBase):
    """
    Base class for managing OpenAI and Azure OpenAI clients.
    Handles client initialization, configuration, and shared logic.
    """

    api_key: Optional[str] = Field(
        default=None, description="API key for OpenAI or Azure OpenAI."
    )
    base_url: Optional[str] = Field(
        default=None, description="Base URL for OpenAI API (OpenAI-specific)."
    )
    azure_endpoint: Optional[str] = Field(
        default=None, description="Azure endpoint URL (Azure OpenAI-specific)."
    )
    azure_deployment: Optional[str] = Field(
        default=None, description="Azure deployment name (Azure OpenAI-specific)."
    )
    api_version: Optional[str] = Field(
        default=None, description="Azure API version (Azure OpenAI-specific)."
    )
    organization: Optional[str] = Field(
        default=None, description="Organization for OpenAI or Azure OpenAI."
    )
    project: Optional[str] = Field(
        default=None, description="Project for OpenAI or Azure OpenAI."
    )
    azure_ad_token: Optional[str] = Field(
        default=None, description="Azure AD token for authentication (Azure-specific)."
    )
    azure_client_id: Optional[str] = Field(
        default=None, description="Client ID for Managed Identity (Azure-specific)."
    )
    timeout: Union[int, float, Dict[str, Any]] = Field(
        default=1500, description="Timeout for requests in seconds."
    )

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes private attributes after validation.
        """
        self._provider = "openai"

        # Set up the private config and client attributes
        self._config: Union[
            AzureOpenAIClientConfig, OpenAIClientConfig
        ] = self.get_config()
        self._client: Union[AzureOpenAI, OpenAI] = self.get_client()
        return super().model_post_init(__context)

    def get_config(self) -> Union[OpenAIClientConfig, AzureOpenAIClientConfig]:
        """
        Returns the appropriate configuration for OpenAI or Azure OpenAI.
        """
        is_azure = self.azure_endpoint or self.azure_deployment

        if is_azure:
            return AzureOpenAIClientConfig(
                api_key=self.api_key,
                organization=self.organization,
                project=self.project,
                azure_ad_token=self.azure_ad_token,
                azure_endpoint=self.azure_endpoint,
                azure_deployment=self.azure_deployment,
                api_version=self.api_version,
            )
        else:
            return OpenAIClientConfig(
                api_key=self.api_key,
                base_url=self.base_url,
                organization=self.organization,
                project=self.project,
            )

    def get_client(self) -> Union[AzureOpenAI, OpenAI]:
        """
        Initialize and return the appropriate client (OpenAI or Azure OpenAI).
        """
        config = self.config
        timeout = self.timeout

        if isinstance(config, AzureOpenAIClientConfig):
            logger.info("Initializing Azure OpenAI client...")
            return AzureOpenAIClient(
                api_key=config.api_key,
                azure_ad_token=config.azure_ad_token,
                azure_endpoint=config.azure_endpoint,
                azure_deployment=config.azure_deployment,
                api_version=config.api_version,
                organization=config.organization,
                project=config.project,
                azure_client_id=self.azure_client_id,
                timeout=timeout,
            ).get_client()

        logger.info("Initializing OpenAI client...")
        return OpenAIClient(
            api_key=config.api_key,
            base_url=config.base_url,
            organization=config.organization,
            project=config.project,
            timeout=timeout,
        ).get_client()

    @property
    def config(self) -> Union[AzureOpenAIClientConfig, OpenAIClientConfig]:
        return self._config

    @property
    def client(self) -> Union[OpenAI, AzureOpenAI]:
        return self._client

================
File: llm/openai/client/openai.py
================
from dapr_agents.types.llm import OpenAIClientConfig
from dapr_agents.llm.utils import HTTPHelper
from typing import Union, Optional
from openai import OpenAI
import logging

logger = logging.getLogger(__name__)


class OpenAIClient:
    """
    Client for interfacing with OpenAI's language models.
    This client handles API communication, including sending requests and processing responses.
    """

    def __init__(
        self,
        api_key: str,
        base_url: Optional[str] = None,
        organization: Optional[str] = None,
        project: Optional[str] = None,
        timeout: Union[int, float, dict] = 1500,
    ):
        """
        Initializes the OpenAI client with API key, base URL, and organization.

        Args:
            api_key: The OpenAI API key.
            base_url: The base URL for OpenAI API (defaults to https://api.openai.com/v1).
            organization: The OpenAI organization (optional).
            project: The OpenAI Project name (optional).
            timeout: Timeout for requests (default is 1500 seconds).
        """
        self.api_key = api_key  # or inferred from OPENAI_API_KEY env variable.
        self.base_url = base_url  # or set to "https://api.openai.com/v1" by default.
        self.organization = organization  # or inferred from OPENAI_ORG_ID env variable.
        self.project = project  # or inferred from OPENAI_PROJECT_ID env variable.
        self.timeout = HTTPHelper.configure_timeout(timeout)

    def get_client(self) -> OpenAI:
        """
        Returns the OpenAI client.

        Returns:
            OpenAI: The initialized OpenAI client.
        """
        return OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            organization=self.organization,
            project=self.project,
            timeout=self.timeout,
        )

    @classmethod
    def from_config(
        cls, client_options: OpenAIClientConfig, timeout: Union[int, float, dict] = 1500
    ):
        """
        Initialize OpenAIBaseClient using OpenAIClientConfig.

        Args:
            client_options: The client options containing the configuration.
            timeout: Timeout for requests (default is 1500 seconds).

        Returns:
            OpenAIBaseClient: An initialized instance.
        """
        return cls(
            api_key=client_options.api_key,
            base_url=client_options.base_url,
            organization=client_options.organization,
            project=client_options.project,
            timeout=timeout,
        )

================
File: llm/openai/__init__.py
================
from .client import OpenAIClient, AzureOpenAIClient
from .chat import OpenAIChatClient
from .audio import OpenAIAudioClient
from .embeddings import OpenAIEmbeddingClient

================
File: llm/openai/audio.py
================
from dapr_agents.llm.openai.client.base import OpenAIClientBase
from dapr_agents.llm.utils import RequestHandler
from dapr_agents.types.llm import (
    AudioSpeechRequest,
    AudioTranscriptionRequest,
    AudioTranslationRequest,
    AudioTranscriptionResponse,
    AudioTranslationResponse,
)
from typing import Union, Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)


class OpenAIAudioClient(OpenAIClientBase):
    """
    Client for handling OpenAI's audio functionalities, including speech generation, transcription, and translation.
    Inherits shared logic and configuration from OpenAIClientBase.
    """

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes the private attributes specific to the audio client.
        """
        self._api = "audio"
        super().model_post_init(__context)

    def create_speech(
        self,
        request: Union[AudioSpeechRequest, Dict[str, Any]],
        file_name: Optional[str] = None,
    ) -> Union[bytes, None]:
        """
        Generate speech audio from text and optionally save it to a file.

        Args:
            request (Union[AudioSpeechRequest, Dict[str, Any]]): The request parameters for speech generation.
            file_name (Optional[str]): Optional file name to save the generated audio.

        Returns:
            Union[bytes, None]: The generated audio content as bytes if no file_name is provided, otherwise None.
        """
        # Transform dictionary to Pydantic object if needed
        validated_request: AudioSpeechRequest = RequestHandler.validate_request(
            request, AudioSpeechRequest
        )

        logger.info(f"Using model '{validated_request.model}' for speech generation.")

        input_text = validated_request.input

        max_chunk_size = 4096

        if len(input_text) > max_chunk_size:
            logger.info(
                f"Input exceeds {max_chunk_size} characters. Splitting into smaller chunks."
            )

        # Split input text into manageable chunks
        def split_text(text, max_size):
            chunks = []
            while len(text) > max_size:
                split_index = text.rfind(". ", 0, max_size) + 1 or max_size
                chunks.append(text[:split_index].strip())
                text = text[split_index:].strip()
            chunks.append(text)
            return chunks

        text_chunks = split_text(input_text, max_chunk_size)

        audio_chunks = []

        try:
            for chunk in text_chunks:
                validated_request.input = chunk
                with self.client.with_streaming_response.audio.speech.create(
                    **validated_request.model_dump()
                ) as response:
                    if file_name:
                        # Write each chunk incrementally to the file
                        logger.info(f"Saving audio chunk to file: {file_name}")
                        with open(file_name, "ab") as audio_file:
                            for chunk in response.iter_bytes():
                                audio_file.write(chunk)
                    else:
                        # Collect all chunks in memory for combining
                        audio_chunks.extend(response.iter_bytes())

            if file_name:
                return None
            else:
                # Combine all chunks into one bytes object
                return b"".join(audio_chunks)

        except Exception as e:
            logger.error(f"Failed to create or save speech: {e}")
            raise ValueError(f"An error occurred during speech generation: {e}")

    def create_transcription(
        self, request: Union[AudioTranscriptionRequest, Dict[str, Any]]
    ) -> AudioTranscriptionResponse:
        """
        Transcribe audio to text.

        Args:
            request (Union[AudioTranscriptionRequest, Dict[str, Any]]): The request parameters for transcription.

        Returns:
            AudioTranscriptionResponse: The transcription result.
        """
        validated_request: AudioTranscriptionRequest = RequestHandler.validate_request(
            request, AudioTranscriptionRequest
        )

        logger.info(f"Using model '{validated_request.model}' for transcription.")

        response = self.client.audio.transcriptions.create(
            file=validated_request.file,
            **validated_request.model_dump(exclude={"file"}),
        )
        return response

    def create_translation(
        self, request: Union[AudioTranslationRequest, Dict[str, Any]]
    ) -> AudioTranslationResponse:
        """
        Translate audio to English.

        Args:
            request (Union[AudioTranslationRequest, Dict[str, Any]]): The request parameters for translation.

        Returns:
            AudioTranslationResponse: The translation result.
        """
        validated_request: AudioTranslationRequest = RequestHandler.validate_request(
            request, AudioTranslationRequest
        )

        logger.info(f"Using model '{validated_request.model}' for translation.")

        response = self.client.audio.translations.create(
            file=validated_request.file,
            **validated_request.model_dump(exclude={"file"}),
        )
        return response

================
File: llm/openai/chat.py
================
from dapr_agents.types.llm import AzureOpenAIModelConfig, OpenAIModelConfig
from dapr_agents.llm.utils import RequestHandler, ResponseHandler
from dapr_agents.llm.openai.client.base import OpenAIClientBase
from dapr_agents.types.message import BaseMessage
from dapr_agents.llm.chat import ChatClientBase
from dapr_agents.prompt.prompty import Prompty
from dapr_agents.tool import AgentTool
from typing import (
    Union,
    Optional,
    Iterable,
    Dict,
    Any,
    List,
    Iterator,
    Type,
    Literal,
    ClassVar,
)
from openai.types.chat import ChatCompletionMessage
from pydantic import BaseModel, Field, model_validator
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class OpenAIChatClient(OpenAIClientBase, ChatClientBase):
    """
    Chat client for OpenAI models.
    Combines OpenAI client management with Prompty-specific functionality.
    """

    model: str = Field(default=None, description="Model name to use, e.g., 'gpt-4'.")

    SUPPORTED_STRUCTURED_MODES: ClassVar[set] = {"json", "function_call"}

    @model_validator(mode="before")
    def validate_and_initialize(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        """
        Ensures the 'model' is set during validation.
        Uses 'azure_deployment' if no model is specified, defaults to 'gpt-4o'.
        """
        if "model" not in values or values["model"] is None:
            values["model"] = values.get("azure_deployment", "gpt-4o")
        return values

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes chat-specific attributes after validation.
        """
        self._api = "chat"
        super().model_post_init(__context)

    @classmethod
    def from_prompty(
        cls,
        prompty_source: Union[str, Path],
        timeout: Union[int, float, Dict[str, Any]] = 1500,
    ) -> "OpenAIChatClient":
        """
        Initializes an OpenAIChatClient client using a Prompty source, which can be a file path or inline content.

        Args:
            prompty_source (Union[str, Path]): The source of the Prompty file, which can be a path to a file
                or inline Prompty content as a string.
            timeout (Union[int, float, Dict[str, Any]], optional): Timeout for requests, defaults to 1500 seconds.

        Returns:
            OpenAIChatClient: An instance of OpenAIChatClient configured with the model settings from the Prompty source.
        """
        # Load the Prompty instance from the provided source
        prompty_instance = Prompty.load(prompty_source)

        # Generate the prompt template from the Prompty instance
        prompt_template = Prompty.to_prompt_template(prompty_instance)

        # Extract the model configuration from Prompty
        model_config = prompty_instance.model

        # Initialize the OpenAIChatClient instance using model_validate
        if isinstance(model_config.configuration, OpenAIModelConfig):
            return cls.model_validate(
                {
                    "model": model_config.configuration.name,
                    "api_key": model_config.configuration.api_key,
                    "base_url": model_config.configuration.base_url,
                    "organization": model_config.configuration.organization,
                    "project": model_config.configuration.project,
                    "timeout": timeout,
                    "prompty": prompty_instance,
                    "prompt_template": prompt_template,
                }
            )
        elif isinstance(model_config.configuration, AzureOpenAIModelConfig):
            return cls.model_validate(
                {
                    "model": model_config.configuration.azure_deployment,
                    "api_key": model_config.configuration.api_key,
                    "azure_endpoint": model_config.configuration.azure_endpoint,
                    "azure_deployment": model_config.configuration.azure_deployment,
                    "api_version": model_config.configuration.api_version,
                    "organization": model_config.configuration.organization,
                    "project": model_config.configuration.project,
                    "azure_ad_token": model_config.configuration.azure_ad_token,
                    "azure_client_id": model_config.configuration.azure_client_id,
                    "timeout": timeout,
                    "prompty": prompty_instance,
                    "prompt_template": prompt_template,
                }
            )
        else:
            raise ValueError(
                f"Unsupported model configuration type: {type(model_config.configuration)}"
            )

    def generate(
        self,
        messages: Union[
            str,
            Dict[str, Any],
            BaseMessage,
            Iterable[Union[Dict[str, Any], BaseMessage]],
        ] = None,
        input_data: Optional[Dict[str, Any]] = None,
        model: Optional[str] = None,
        tools: Optional[List[Union[AgentTool, Dict[str, Any]]]] = None,
        response_format: Optional[Type[BaseModel]] = None,
        structured_mode: Literal["json", "function_call"] = "json",
        **kwargs,
    ) -> Union[Iterator[Dict[str, Any]], Dict[str, Any]]:
        """
        Generate chat completions based on provided messages or input_data for prompt templates.

        Args:
            messages (Optional): Either pre-set messages or None if using input_data.
            input_data (Optional[Dict[str, Any]]): Input variables for prompt templates.
            model (str): Specific model to use for the request, overriding the default.
            tools (List[Union[AgentTool, Dict[str, Any]]]): List of tools for the request.
            response_format (Type[BaseModel]): Optional Pydantic model for structured response parsing.
            structured_mode (Literal["json", "function_call"]): Mode for structured output: "json" or "function_call".
            **kwargs: Additional parameters for the language model.

        Returns:
            Union[Iterator[Dict[str, Any]], Dict[str, Any]]: The chat completion response(s).
        """

        if structured_mode not in self.SUPPORTED_STRUCTURED_MODES:
            raise ValueError(
                f"Invalid structured_mode '{structured_mode}'. Must be one of {self.SUPPORTED_STRUCTURED_MODES}."
            )

        # If input_data is provided, check for a prompt_template
        if input_data:
            if not self.prompt_template:
                raise ValueError(
                    "Inputs are provided but no 'prompt_template' is set. Please set a 'prompt_template' to use the input_data."
                )

            logger.info("Using prompt template to generate messages.")
            messages = self.prompt_template.format_prompt(**input_data)

        # Ensure we have messages at this point
        if not messages:
            raise ValueError("Either 'messages' or 'input_data' must be provided.")

        # Process and normalize the messages
        params = {"messages": RequestHandler.normalize_chat_messages(messages)}

        # Merge prompty parameters if available, then override with any explicit kwargs
        if self.prompty:
            params = {**self.prompty.model.parameters.model_dump(), **params, **kwargs}
        else:
            params.update(kwargs)

        # If a model is provided, override the default model
        params["model"] = model or self.model

        # Prepare request parameters
        params = RequestHandler.process_params(
            params,
            llm_provider=self.provider,
            tools=tools,
            response_format=response_format,
            structured_mode=structured_mode,
        )

        try:
            logger.info("Invoking ChatCompletion API.")
            logger.debug(f"ChatCompletion API Parameters: {params}")
            response: ChatCompletionMessage = self.client.chat.completions.create(
                **params, timeout=self.timeout
            )
            logger.info("Chat completion retrieved successfully.")

            return ResponseHandler.process_response(
                response,
                llm_provider=self.provider,
                response_format=response_format,
                structured_mode=structured_mode,
                stream=params.get("stream", False),
            )
        except Exception as e:
            logger.error(f"An error occurred during the ChatCompletion API call: {e}")
            raise

================
File: llm/openai/embeddings.py
================
from openai.types.create_embedding_response import CreateEmbeddingResponse
from dapr_agents.llm.openai.client.base import OpenAIClientBase
from typing import Union, Dict, Any, Literal, List, Optional
from pydantic import Field, model_validator
import logging

logger = logging.getLogger(__name__)


class OpenAIEmbeddingClient(OpenAIClientBase):
    """
    Client for handling OpenAI's embedding functionalities, supporting both OpenAI and Azure OpenAI configurations.

    Attributes:
        model (str): The ID of the model to use for embedding. Defaults to `text-embedding-ada-002` if not specified.
        encoding_format (Optional[Literal["float", "base64"]]): The format of the embeddings. Defaults to 'float'.
        dimensions (Optional[int]): Number of dimensions for the output embeddings. Only supported in specific models like `text-embedding-3`.
        user (Optional[str]): A unique identifier representing the end-user.
    """

    model: str = Field(
        default=None, description="ID of the model to use for embedding."
    )
    encoding_format: Optional[Literal["float", "base64"]] = Field(
        "float", description="Format for the embeddings. Defaults to 'float'."
    )
    dimensions: Optional[int] = Field(
        None,
        description="Number of dimensions for the output embeddings. Supported in text-embedding-3 and later models.",
    )
    user: Optional[str] = Field(
        None, description="Unique identifier representing the end-user."
    )

    @model_validator(mode="before")
    def validate_and_initialize(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        """
        Ensures that the 'model' attribute is set during validation.
        If 'model' is not provided, defaults to 'text-embedding-ada-002' or uses 'azure_deployment' if available.

        Args:
            values (Dict[str, Any]): Dictionary of model attributes to validate and initialize.

        Returns:
            Dict[str, Any]: Updated dictionary of validated attributes.
        """
        if "model" not in values or values["model"] is None:
            values["model"] = values.get("azure_deployment", "text-embedding-ada-002")
        return values

    def model_post_init(self, __context: Any) -> None:
        """
        Post-initialization setup for private attributes.

        This method configures the API endpoint for embedding operations.

        Args:
            __context (Any): Context provided during model initialization.
        """
        self._api = "embeddings"
        return super().model_post_init(__context)

    def create_embedding(
        self,
        input: Union[str, List[Union[str, List[int]]]],
        model: Optional[str] = None,
    ) -> CreateEmbeddingResponse:
        """
        Generate embeddings for the given input text(s).

        Args:
            input (Union[str, List[Union[str, List[int]]]]): Input text(s) or tokenized input(s) to generate embeddings for.
                - A single string for one input.
                - A list of strings or tokenized lists for multiple inputs.
            model (Optional[str]): Model to use for embedding. Overrides the default model if provided.

        Returns:
            CreateEmbeddingResponse: A response object containing the generated embeddings and associated metadata.

        Raises:
            ValueError: If the client fails to generate embeddings.
        """
        logger.info(f"Using model '{self.model}' for embedding generation.")

        # If a model is provided, override the default model
        model = model or self.model

        response = self.client.embeddings.create(
            model=model,
            input=input,
            encoding_format=self.encoding_format,
            dimensions=self.dimensions,
            user=self.user,
        )
        return response

================
File: llm/utils/__init__.py
================
from .structure import StructureHandler
from .stream import StreamHandler
from .request import RequestHandler
from .response import ResponseHandler
from .http import HTTPHelper

================
File: llm/utils/http.py
================
from typing import Union
import httpx


class HTTPHelper:
    """
    HTTP operations helper.
    """

    @staticmethod
    def configure_timeout(timeout: Union[int, float, dict]) -> httpx.Timeout:
        """
        Configure the timeout setting for the HTTP client.
        :param timeout: Timeout in seconds or a dictionary of timeout configurations.
        :return: An httpx.Timeout instance configured with the provided timeout.
        """
        if isinstance(timeout, (int, float)):
            return httpx.Timeout(timeout)
        elif isinstance(timeout, dict):
            return httpx.Timeout(**timeout)
        else:
            return httpx.Timeout(30)

================
File: llm/utils/request.py
================
from typing import Dict, Any, Optional, List, Type, Union, Iterable, Literal
from dapr_agents.prompt.prompty import Prompty, PromptyHelper
from dapr_agents.types.message import BaseMessage
from dapr_agents.llm.utils import StructureHandler
from dapr_agents.tool.utils.tool import ToolHelper
from pydantic import BaseModel, ValidationError

import logging

logger = logging.getLogger(__name__)


class RequestHandler:
    """
    Handles the preparation of requests for language models.
    """

    @staticmethod
    def process_prompty_messages(
        prompty: Prompty, inputs: Dict[str, Any] = {}
    ) -> List[Dict[str, Any]]:
        """
        Process and format messages based on Prompty template and provided inputs.

        Args:
            prompty (Prompty): The Prompty instance containing the template and settings.
            inputs (Dict[str, Any]): Input variables for the Prompty template (default is an empty dictionary).

        Returns:
            List[Dict[str, Any]]: Processed and prepared messages.
        """
        # Prepare inputs and generate messages from Prompty content
        api_type = prompty.model.api
        prepared_inputs = PromptyHelper.prepare_inputs(
            inputs, prompty.inputs, prompty.sample
        )
        messages = PromptyHelper.to_prompt(
            prompty.content, prepared_inputs, api_type=api_type
        )

        return messages

    @staticmethod
    def normalize_chat_messages(
        messages: Union[
            str,
            Dict[str, Any],
            BaseMessage,
            Iterable[Union[Dict[str, Any], BaseMessage]],
        ],
    ) -> List[Dict[str, Any]]:
        """
        Normalize and validate the input messages into a list of dictionaries.

        Args:
            messages (Union[str, Dict[str, Any], BaseMessage, Iterable[Union[Dict[str, Any], BaseMessage]]]):
                Input messages in various formats (string, dict, BaseMessage, or an iterable).

        Returns:
            List[Dict[str, Any]]: A list of normalized message dictionaries with keys 'role' and 'content'.

        Raises:
            ValueError: If the input format is unsupported or if required fields are missing in a dictionary.
        """
        # Initialize an empty list to store the normalized messages
        normalized_messages = []

        # Use a queue to process messages iteratively and handle nested structures
        queue = [messages]

        while queue:
            msg = queue.pop(0)
            if isinstance(msg, str):
                normalized_messages.append({"role": "user", "content": msg})
            elif isinstance(msg, BaseMessage):
                normalized_messages.append(msg.model_dump())
            elif isinstance(msg, dict):
                role = msg.get("role")
                if role not in {"user", "assistant", "tool", "system"}:
                    raise ValueError(
                        f"Unrecognized role '{role}'. Supported roles are 'user', 'assistant', 'tool', or 'system'."
                    )
                normalized_messages.append(msg)
            elif isinstance(msg, Iterable) and not isinstance(msg, (str, dict)):
                queue.extend(msg)
            else:
                raise ValueError(f"Unsupported message format: {type(msg)}")
        return normalized_messages

    @staticmethod
    def process_params(
        params: Dict[str, Any],
        llm_provider: str,
        tools: Optional[List[Dict[str, Any]]] = None,
        response_format: Optional[Union[Type[BaseModel], Dict[str, Any]]] = None,
        structured_mode: Literal["json", "function_call"] = "json",
    ) -> Dict[str, Any]:
        """
        Prepare request parameters for the language model.

        Args:
            params: Parameters for the request.
            llm_provider: The LLM provider to use (e.g., 'openai').
            tools: List of tools to include in the request.
            response_format: Either a Pydantic model (for function calling)
                            or a JSON Schema definition/dict (for raw JSON structured output).
            structured_mode: The mode of structured output: 'json' or 'function_call'.
                            Defaults to 'json'.

        Returns:
            Dict[str, Any]: Prepared request parameters.
        """
        if tools:
            logger.info("Tools are available in the request.")
            params["tools"] = [
                ToolHelper.format_tool(tool, tool_format=llm_provider) for tool in tools
            ]

        if response_format:
            logger.info(f"Structured Mode Activated! Mode={structured_mode}.")
            params = StructureHandler.generate_request(
                response_format=response_format,
                llm_provider=llm_provider,
                structured_mode=structured_mode,
                **params,
            )

        return params

    @staticmethod
    def validate_request(
        request: Union[BaseModel, Dict[str, Any]], request_class: Type[BaseModel]
    ) -> BaseModel:
        """
        Validate and transform a dictionary into a Pydantic object.

        Args:
            request (Union[BaseModel, Dict[str, Any]]): The request data as a dictionary or a Pydantic object.
            request_class (Type[BaseModel]): The Pydantic model class for validation.

        Returns:
            BaseModel: A validated Pydantic object.

        Raises:
            ValueError: If validation fails.
        """
        if isinstance(request, dict):
            try:
                request = request_class(**request)
            except ValidationError as e:
                raise ValueError(f"Validation error: {e}")

        try:
            validated_request = request_class.model_validate(request)
        except ValidationError as e:
            raise ValueError(f"Validation error: {e}")

        return validated_request

================
File: llm/utils/response.py
================
import logging
from dataclasses import asdict, is_dataclass
from typing import Any, Dict, Iterator, Literal, Optional, Type, Union

from pydantic import BaseModel

from dapr_agents.llm.utils import StreamHandler, StructureHandler
from dapr_agents.types import ChatCompletion

logger = logging.getLogger(__name__)


class ResponseHandler:
    """
    Handles the processing of responses from language models.
    """

    @staticmethod
    def process_response(
        response: Any,
        llm_provider: str,
        response_format: Optional[Type[BaseModel]] = None,
        structured_mode: Literal["json", "function_call"] = "json",
        stream: bool = False,
    ) -> Union[Iterator[Dict[str, Any]], Dict[str, Any]]:
        """
        Process the response from the language model.

        Args:
            response: The response object from the language model.
            llm_provider: The LLM provider (e.g., 'openai').
            response_format: A pydantic model to parse and validate the structured response.
            structured_mode: The mode of the structured response: 'json' or 'function_call'.
            stream: Whether the response is a stream.

        Returns:
            Union[Iterator[Dict[str, Any]], Dict[str, Any]]: The processed response.
        """
        if stream:
            return StreamHandler.process_stream(
                stream=response,
                llm_provider=llm_provider,
                response_format=response_format,
            )
        else:
            if response_format:
                structured_response_json = StructureHandler.extract_structured_response(
                    response=response,
                    llm_provider=llm_provider,
                    structured_mode=structured_mode,
                )

                # Normalize format and resolve actual model class
                normalized_format = StructureHandler.normalize_iterable_format(
                    response_format
                )
                model_cls = StructureHandler.resolve_response_model(normalized_format)

                if not model_cls:
                    raise TypeError(
                        f"Could not resolve a valid Pydantic model from response_format: {response_format}"
                    )

                structured_response_instance = StructureHandler.validate_response(
                    structured_response_json, normalized_format
                )

                logger.info("Structured output was successfully validated.")
                if hasattr(structured_response_instance, "objects"):
                    return structured_response_instance.objects
                return structured_response_instance

            # Convert response to dictionary
            if isinstance(response, dict):
                # Already a dictionary
                response_dict = response
            elif is_dataclass(response):
                # Dataclass instance
                response_dict = asdict(response)
            elif isinstance(response, BaseModel):
                # Pydantic object
                response_dict = response.model_dump()
            else:
                raise ValueError(f"Unsupported response type: {type(response)}")

            completion = ChatCompletion(**response_dict)
            logger.debug(f"Chat completion response: {completion}")
            return completion

================
File: llm/utils/stream.py
================
from typing import (
    Dict,
    Any,
    Iterator,
    Type,
    TypeVar,
    Union,
    Optional,
    Iterable,
    get_args,
)
from dapr_agents.llm.utils import StructureHandler
from dapr_agents.types import ToolCall
from openai.types.chat import ChatCompletionChunk
from pydantic import BaseModel, ValidationError
import logging

logger = logging.getLogger(__name__)

T = TypeVar("T", bound=BaseModel)


class StreamHandler:
    """
    Handles streaming of chat completion responses, processing tool calls and content responses.
    """

    @staticmethod
    def process_stream(
        stream: Iterator[Dict[str, Any]],
        llm_provider: str,
        response_format: Optional[Union[Type[T], Type[Iterable[T]]]] = None,
    ) -> Iterator[Dict[str, Any]]:
        """
        Stream chat completion responses.

        Args:
            stream: The response stream from the API.
            llm_provider: The LLM provider to use (e.g., 'openai').
            response_format: The optional Pydantic model or iterable model for validating the response.

        Yields:
            dict: Each processed and validated chunk from the chat completion response.
        """
        logger.info("Streaming response enabled.")

        try:
            if llm_provider == "openai":
                yield from StreamHandler._process_openai_stream(stream, response_format)
            elif llm_provider == "dapr":
                yield from StreamHandler._process_dapr_stream(stream, response_format)
            else:
                yield from stream
        except Exception as e:
            logger.error(f"An error occurred during streaming: {e}")
            raise

    @staticmethod
    def _process_openai_stream(
        stream: Iterator[Dict[str, Any]],
        response_format: Optional[Union[Type[T], Type[Iterable[T]]]] = None,
    ) -> Iterator[Dict[str, Any]]:
        """
        Process OpenAI stream for chat completion.

        Args:
            stream: The response stream from the OpenAI API.
            response_format: The optional Pydantic model or iterable model for validating the response.

        Yields:
            dict: Each processed and validated chunk from the chat completion response.
        """
        content_accumulator = ""
        json_extraction_active = False
        json_brace_level = 0
        json_string_buffer = ""
        tool_calls = {}

        for chunk in stream:
            processed_chunk = StreamHandler._process_openai_chunk(chunk)
            chunk_type = processed_chunk["type"]
            chunk_data = processed_chunk["data"]

            if chunk_type == "content":
                content_accumulator += chunk_data
                yield processed_chunk
            elif chunk_type in ["tool_calls", "function_call"]:
                for tool_chunk in chunk_data:
                    tool_call_index = tool_chunk["index"]
                    tool_call_id = tool_chunk["id"]
                    tool_call_function = tool_chunk["function"]
                    tool_call_arguments = tool_call_function["arguments"]

                    if tool_call_id is not None:
                        tool_calls.setdefault(
                            tool_call_index,
                            {
                                "id": tool_call_id,
                                "type": tool_chunk["type"],
                                "function": {
                                    "name": tool_call_function["name"],
                                    "arguments": tool_call_arguments,
                                },
                            },
                        )

                    # Add tool call arguments to current tool calls
                    tool_calls[tool_call_index]["function"][
                        "arguments"
                    ] += tool_call_arguments

                    # Process Iterable model if provided
                    if (
                        response_format
                        and isinstance(response_format, Iterable) is True
                    ):
                        trimmed_character = tool_call_arguments.strip()
                        # Check beginning of List
                        if trimmed_character == "[" and json_extraction_active is False:
                            json_extraction_active = True
                        # Check beginning of a JSON object
                        elif (
                            trimmed_character == "{" and json_extraction_active is True
                        ):
                            json_brace_level += 1
                            json_string_buffer += trimmed_character
                        # Check the end of a JSON object
                        elif (
                            "}" in trimmed_character and json_extraction_active is True
                        ):
                            json_brace_level -= 1
                            json_string_buffer += trimmed_character.rstrip(",")
                            if json_brace_level == 0:
                                yield from StreamHandler._validate_json_object(
                                    response_format, json_string_buffer
                                )
                                # Reset buffers and counts
                                json_string_buffer = ""
                        elif json_extraction_active is True:
                            json_string_buffer += tool_call_arguments

        if content_accumulator:
            yield {"type": "final_content", "data": content_accumulator}

        if tool_calls:
            yield from StreamHandler._get_final_tool_calls(tool_calls, response_format)

    @staticmethod
    def _process_openai_chunk(chunk: ChatCompletionChunk) -> Dict[str, Any]:
        """
        Process OpenAI chat completion chunk.

        Args:
            chunk: The chunk from the OpenAI API.

        Returns:
            dict: Processed chunk.
        """
        try:
            chunk_dict = chunk.model_dump()

            if chunk_dict.get("choices") and len(chunk_dict["choices"]) > 0:
                choice: Dict = chunk_dict["choices"][0]
                delta: Dict = choice.get("delta", {})

                # Process content
                if delta.get("content") is not None:
                    return {"type": "content", "data": delta["content"], "chunk": chunk}

                # Process tool calls
                if delta.get("tool_calls"):
                    return {
                        "type": "tool_calls",
                        "data": delta["tool_calls"],
                        "chunk": chunk,
                    }

                # Process function calls
                if delta.get("function_call"):
                    return {
                        "type": "function_call",
                        "data": delta["function_call"],
                        "chunk": chunk,
                    }

                # Process finish reason
                if choice.get("finish_reason"):
                    return {
                        "type": "finish",
                        "data": choice["finish_reason"],
                        "chunk": chunk,
                    }

            return {}
        except Exception as e:
            logger.error(f"Error handling OpenAI chat completion chunk: {e}")
            raise

    @staticmethod
    def _process_dapr_stream(
        stream: Iterator[Dict[str, Any]],
        response_format: Optional[Union[Type[T], Type[Iterable[T]]]] = None,
    ) -> Iterator[Dict[str, Any]]:
        """
        Process Dapr stream for chat completion.

        Args:
            stream: The response stream from the Dapr API.
            response_format: The optional Pydantic model or iterable model for validating the response.

        Yields:
            dict: Each processed and validated chunk from the chat completion response.
        """
        content_accumulator = ""
        final_usage = None

        for chunk in stream:
            processed_chunk = StreamHandler._process_dapr_chunk(chunk)
            chunk_type = processed_chunk["type"]
            chunk_data = processed_chunk["data"]

            if chunk_type == "content":
                content_accumulator += chunk_data
                yield processed_chunk
            elif chunk_type == "usage":
                final_usage = chunk_data
            elif chunk_type == "context_id":
                yield processed_chunk

        # Yield final content and usage if available
        if content_accumulator:
            yield {"type": "final_content", "data": content_accumulator}

        if final_usage:
            yield {"type": "final_usage", "data": final_usage}

    @staticmethod
    def _process_dapr_chunk(chunk: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process Dapr chat completion chunk.

        Args:
            chunk: The chunk from the Dapr API.

        Returns:
            dict: Processed chunk.
        """
        try:
            # Handle content chunks
            if chunk.get("choices") and len(chunk["choices"]) > 0:
                choice = chunk["choices"][0]
                delta = choice.get("delta", {})
                
                if delta.get("content") is not None:
                    return {"type": "content", "data": delta["content"], "chunk": chunk}

            # Handle usage information
            if chunk.get("usage"):
                return {"type": "usage", "data": chunk["usage"], "chunk": chunk}

            # Handle context ID
            if chunk.get("context_id"):
                return {"type": "context_id", "data": chunk["context_id"], "chunk": chunk}

            return {"type": "unknown", "data": chunk, "chunk": chunk}
        except Exception as e:
            logger.error(f"Error handling Dapr chat completion chunk: {e}")
            raise

    @staticmethod
    def _validate_json_object(
        response_format: Optional[Union[Type[T], Type[Iterable[T]]]],
        json_string_buffer: str,
    ):
        try:
            model_class = get_args(response_format)[0]
            # Return current tool call
            structured_output = StructureHandler.validate_response(
                json_string_buffer, model_class
            )
            if isinstance(structured_output, model_class):
                logger.info("Structured output was successfully validated.")
                yield {"type": "structured_output", "data": structured_output}
        except ValidationError as validation_error:
            logger.error(
                f"Validation error: {validation_error} with JSON: {json_string_buffer}"
            )

    @staticmethod
    def _get_final_tool_calls(
        tool_calls: Dict[int, Any],
        response_format: Optional[Union[Type[T], Type[Iterable[T]]]],
    ) -> Iterator[Dict[str, Any]]:
        """
        Yield final tool calls after processing.

        Args:
            tool_calls: The dictionary of accumulated tool calls.
            response_format: The response model for validation.

        Yields:
            dict: Each processed and validated tool call.
        """
        for tool in tool_calls.values():
            if response_format and isinstance(response_format, Iterable) is False:
                structured_output = StructureHandler.validate_response(
                    tool["function"]["arguments"], response_format
                )
                if isinstance(structured_output, response_format):
                    logger.info("Structured output was successfully validated.")
                    yield {"type": "structured_output", "data": structured_output}
            else:
                tool_call = ToolCall(**tool)
                yield {"type": "final_tool_call", "data": tool_call}

================
File: llm/utils/structure.py
================
import json
import logging
from collections.abc import Iterable
from typing import (
    Annotated,
    Any,
    Dict,
    List,
    Literal,
    Optional,
    Type,
    TypeVar,
    Union,
    get_args,
    get_origin,
)

from pydantic import BaseModel, Field, TypeAdapter, ValidationError, create_model

from dapr_agents.tool.utils.function_calling import to_function_call_definition
from dapr_agents.types import OAIJSONSchema, OAIResponseFormatSchema, StructureError

logger = logging.getLogger(__name__)

T = TypeVar("T", bound=BaseModel)


class StructureHandler:
    @staticmethod
    def is_json_string(input_string: str) -> bool:
        """
        Check if the given input is a valid JSON string.

        Args:
            input_string (str): The string to check.

        Returns:
            bool: True if the input is a valid JSON string, False otherwise.
        """
        try:
            json.loads(input_string)
            return True
        except json.JSONDecodeError:
            return False

    @staticmethod
    def normalize_iterable_format(tp: Any) -> Any:
        origin = get_origin(tp)
        args = get_args(tp)

        if origin in (list, List, tuple, Iterable) and args:
            item_type = args[0]
            if isinstance(item_type, type) and issubclass(item_type, BaseModel):
                logger.debug(
                    "Detected iterable of BaseModel. Wrapping in generated Pydantic model."
                )
                return StructureHandler.create_iterable_model(item_type)

        return tp

    @staticmethod
    def generate_request(
        response_format: Union[Type[T], Dict[str, Any], Iterable[Type[T]]],
        llm_provider: str,
        structured_mode: Literal["json", "function_call"] = "json",
        **params,
    ) -> Dict[str, Any]:
        """
        Generates a structured request that conforms to a specified API format using the given Pydantic model.
        This function prepares a request configuration that includes the model as a tool specification and
        sets the necessary parameters for the API call.

        Args:
            response_format (Union[Type[T], Dict[str, Any], Iterable[Type[T]]]): Defines the response structure.
                - If `structured_mode="json"`: Can be a Pydantic model (converted to JSON schema) or a JSON schema dictionary.
                - If `structured_mode="function_call"`: Must be a Pydantic model or an iterable of models.
                - If an iterable of models is provided in either mode, it is treated as a list schema.
            llm_provider (str): The LLM provider (e.g., "openai", "claude").
            structured_mode (Literal["json", "function_call"]): Determines the response structure.
                - "json": Generates and enforces a strict JSON schema.
                - "function_call": Converts a Pydantic model to a function-call definition.
            **params: Additional request parameters.

        Returns:
            Dict[str, Any]: Updated request parameters, including tools or response format.

        Raises:
            ValueError: If an unsupported `structured_mode` is provided.
            TypeError: If `response_format` is invalid for the selected mode.
        """
        logger.debug(f"Structured response mode: {structured_mode}")

        response_format = StructureHandler.normalize_iterable_format(response_format)

        # Handle iterable models in both modes
        if structured_mode == "function_call":
            model_cls = StructureHandler.resolve_response_model(response_format)
            if not model_cls:
                raise TypeError(
                    "function_call mode requires a single, unambiguous Pydantic model."
                )

            name = model_cls.__name__
            description = model_cls.__doc__ or ""
            model_tool_format = to_function_call_definition(
                name, description, model_cls, llm_provider
            )

            params["tools"] = [model_tool_format]
            params["tool_choice"] = {
                "type": "function",
                "function": {"name": model_tool_format["function"]["name"]},
            }
            return params

        elif structured_mode == "json":
            try:
                logger.debug(
                    f"generate_request called with type={type(response_format)}, mode={structured_mode}, provider={llm_provider}"
                )
                # If it's a dict, assume it's already a JSON schema; otherwise, try to create from model
                if isinstance(response_format, dict):
                    raw_schema = response_format
                    name = response_format.get("name", "custom_schema")
                    description = response_format.get("description")
                elif isinstance(response_format, type) and issubclass(
                    response_format, BaseModel
                ):
                    raw_schema = response_format.model_json_schema()
                    name = response_format.__name__
                    description = response_format.__doc__
                else:
                    raise TypeError("json mode requires a dict or a Pydantic model.")

                # Enforce strict JSON schema (process $refs, $defs, etc.)
                logger.debug(f"Raw Schema: {raw_schema}")
                strict_schema = StructureHandler.enforce_strict_json_schema(raw_schema)

                # Construct the JSON schema object using OAIJSONSchema
                json_schema_obj = OAIJSONSchema(
                    name=name,
                    description=description,
                    schema_=strict_schema,
                    strict=True,
                )

                # Wrap it in the top-level response format object
                response_format_obj = OAIResponseFormatSchema(
                    json_schema=json_schema_obj
                )

                logger.debug(
                    f"Generated JSON schema: {response_format_obj.model_dump()}"
                )

                # Use model_dump() to serialize the response format into a dictionary
                params["response_format"] = response_format_obj.model_dump(
                    by_alias=True
                )

            except ValidationError as e:
                logger.error(f"Validation error in JSON schema: {e}")
                raise ValueError(f"Invalid response_format provided: {e}")

            return params

        else:
            raise ValueError(
                f"Unsupported structured_mode: {structured_mode}. Must be 'json' or 'function_call'."
            )

    @staticmethod
    def create_iterable_model(
        model: Type[BaseModel],
        model_name: Optional[str] = None,
        model_description: Optional[str] = None,
    ) -> Type[BaseModel]:
        """
        Constructs an iterable Pydantic model for a given Pydantic model.

        Args:
            model (Type[BaseModel]): The original Pydantic model to capture a list of objects of the original model type.
            model_name (Optional[str]): The name of the new iterable model. Defaults to None.
            model_description (Optional[str]): The description of the new iterable model. Defaults to None.

        Returns:
            Type[BaseModel]: A new Pydantic model class representing a list of the original Pydantic model.
        """
        model_name = model.__name__ if model_name is None else model_name
        iterable_model_name = f"Iterable{model_name}"

        objects_field = (
            List[model],
            Field(..., description=f"A list of `{model_name}` objects"),
        )

        iterable_model = create_model(
            iterable_model_name, objects=objects_field, __base__=(BaseModel,)
        )

        iterable_model.__doc__ = (
            f"A Pydantic model to capture `{iterable_model_name}` objects"
            if model_description is None
            else model_description
        )

        return iterable_model

    @staticmethod
    def extract_structured_response(
        response: Any,
        llm_provider: str,
        structured_mode: Literal["json", "function_call"] = "json",
    ) -> Union[str, Dict[str, Any]]:
        """
        Extracts the structured JSON string or content from the response.

        Args:
            response (Any): The API response data to extract.
            llm_provider (str): The LLM provider (e.g., 'openai').
            structured_mode (Literal["json", "function_call"]): The structured response mode.

        Returns:
            Union[str, Dict[str, Any]]: The extracted structured response.

        Raises:
            StructureError: If the structured response is not found or extraction fails.
        """
        try:
            logger.debug(f"Processing structured response for mode: {structured_mode}")
            if llm_provider in ("openai", "nvidia"):
                # Extract the `choices` list from the response
                choices = getattr(response, "choices", None)
                if not choices or not isinstance(choices, list):
                    raise StructureError("Response does not contain valid 'choices'.")

                # Extract the message object
                message = getattr(choices[0], "message", None)
                if not message:
                    raise StructureError("Response message is missing.")

                if structured_mode == "function_call":
                    tool_calls = getattr(message, "tool_calls", None)
                    if tool_calls:
                        function = getattr(tool_calls[0], "function", None)
                        if function and hasattr(function, "arguments"):
                            extracted_response = function.arguments
                            logger.debug(
                                f"Extracted function-call response: {extracted_response}"
                            )
                            return extracted_response
                    raise StructureError("No tool_calls found for function_call mode.")

                elif structured_mode == "json":
                    content = getattr(message, "content", None)
                    refusal = getattr(message, "refusal", None)

                    if refusal:
                        logger.warning(
                            f"Model refused to fulfill the request: {refusal}"
                        )
                        raise StructureError(f"Request refused by the model: {refusal}")

                    if not content:
                        raise StructureError("No content found for JSON mode.")

                    logger.debug(f"Extracted JSON content: {content}")
                    return content

                else:
                    raise ValueError(
                        f"Unsupported structured_mode: {structured_mode}. Must be 'json' or 'function_call'."
                    )
            else:
                raise StructureError(f"Unsupported LLM provider: {llm_provider}")
        except Exception as e:
            logger.error(f"Error while extracting structured response: {e}")
            raise StructureError(f"Extraction failed: {e}")

    @staticmethod
    def validate_response(response: Union[str, dict], model: Type[T]) -> T:
        """
        Validates a JSON string or a dictionary using a specified Pydantic model.

        This method checks whether the response is a JSON string or a dictionary.
        If the response is a JSON string, it validates it using the `model_validate_json` method.
        If the response is a dictionary, it validates it using the `model_validate` method.

        Args:
            response (Union[str, dict]): The JSON string or dictionary to validate.
            model (Type[T]): The Pydantic model that defines the expected structure of the response.

        Returns:
            T: An instance of the Pydantic model populated with the validated data.

        Raises:
            StructureError: If the validation fails.
        """
        try:
            if isinstance(response, str) and StructureHandler.is_json_string(response):
                return model.model_validate_json(response)
            elif isinstance(response, dict):
                # If it's a dictionary, use model_validate
                return model.model_validate(response)
            else:
                raise ValueError("Response must be a JSON string or a dictionary.")
        except ValidationError as e:
            logger.error(f"Validation error while parsing structured response: {e}")
            raise StructureError(f"Validation failed for structured response: {e}")

    @staticmethod
    def expand_local_refs(part: Dict[str, Any], root: Dict[str, Any]) -> Dict[str, Any]:
        """
        Recursively expand all local $refs in the schema, including nested references.

        Args:
            part (Dict[str, Any]): The schema part to process.
            root (Dict[str, Any]): The root schema for resolving $refs.

        Returns:
            Dict[str, Any]: The schema part with all $refs expanded.
        """
        ref = part.pop("$ref", None)
        if ref:
            logger.debug(f"Found $ref: {ref}")
            if not ref.startswith("#/$defs/"):
                raise ValueError(f"Unexpected $ref format: {ref}")

            ref_name = ref.split("/")[-1]
            defs_section = root.get("$defs", {})
            if ref_name not in defs_section:
                raise ValueError(f"Reference '{ref_name}' not found in $defs.")

            # Merge the referenced schema with the current part, resolving nested $refs
            merged = {
                **defs_section[ref_name],
                **{k: v for k, v in part.items() if k != "$ref"},
            }
            return StructureHandler.expand_local_refs(merged, root)

        # Process objects and their properties
        if part.get("type") == "object" and "properties" in part:
            for key, value in part["properties"].items():
                part["properties"][key] = StructureHandler.expand_local_refs(
                    value, root
                )

        # Process arrays and their items
        if part.get("type") == "array" and "items" in part:
            part["items"] = StructureHandler.expand_local_refs(part["items"], root)

        # Process anyOf and allOf schemas
        for key in ("anyOf", "allOf"):
            if key in part and isinstance(part[key], list):
                part[key] = [
                    StructureHandler.expand_local_refs(subschema, root)
                    for subschema in part[key]
                ]

        return part

    @staticmethod
    def enforce_strict_json_schema(schema: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enforces strict JSON schema constraints while making it OpenAI-compatible.

        - Expands all local $refs by resolving them from the $defs section.
        - Ensures "additionalProperties": false for object schemas.
        - Removes default values and replaces optional fields with `anyOf: [{"type": T}, {"type": "null"}]`.
        - Converts optional arrays to `anyOf: [{"type": "array", "items": T}, {"type": "null"}]`.
        - Ensures all `array` schemas define `items`.
        - Converts optional integers and floats to `anyOf: [{"type": T}, {"type": "null"}]`.
        - Prevents the use of `anyOf` at the root level of an array.

        Args:
            schema (Dict[str, Any]): The JSON schema dictionary to process.

        Returns:
            Dict[str, Any]: The updated schema with strict constraints applied.
        """
        # Expand all $refs (resolves and removes them)
        schema = StructureHandler.expand_local_refs(schema, schema)

        # Ensure "additionalProperties": false for all objects
        if schema.get("type") == "object":
            schema.setdefault("additionalProperties", False)

            required_fields = set(schema.get("required", []))

            for key, value in schema.get("properties", {}).items():
                schema["properties"][key] = StructureHandler.enforce_strict_json_schema(
                    value
                )

                # Remove default values (not allowed by OpenAI)
                schema["properties"][key].pop("default", None)

                # Convert optional fields (string, number, integer) to `anyOf`
                if key not in required_fields:
                    field_type = schema["properties"][key].get("type")

                    if field_type and not isinstance(
                        field_type, list
                    ):  # Ensure it's not already `anyOf`
                        if field_type in ["string", "integer", "number"]:
                            schema["properties"][key]["anyOf"] = [
                                {"type": field_type},
                                {"type": "null"},
                            ]
                            schema["properties"][key].pop(
                                "type", None
                            )  # Remove direct "type" field

                    # Ensure field is included in "required" (even if it allows null)
                    required_fields.add(key)

                # Handle optional arrays inside object properties
                if schema["properties"][key].get("anyOf") and isinstance(
                    schema["properties"][key]["anyOf"], list
                ):
                    for subschema in schema["properties"][key]["anyOf"]:
                        if subschema.get("type") == "array":
                            schema["properties"][key] = {
                                "anyOf": [
                                    {
                                        "type": "array",
                                        "items": subschema.get("items", {}),
                                    },
                                    {"type": "null"},
                                ]
                            }

            # Ensure all required fields are explicitly listed
            schema["required"] = list(required_fields)

        # Process arrays and enforce strictness
        if schema.get("type") == "array":
            # Ensure `items` is always present in arrays
            if "items" not in schema:
                raise ValueError(f"Array schema missing 'items': {schema}")

            schema["items"] = StructureHandler.enforce_strict_json_schema(
                schema["items"]
            )

            # Convert optional arrays from `anyOf` to `anyOf: [{"type": "array", "items": T}, {"type": "null"}]`
            if "anyOf" in schema and isinstance(schema["anyOf"], list):
                if any(
                    subschema.get("type") == "array" for subschema in schema["anyOf"]
                ):
                    schema["anyOf"] = [
                        {"type": "array", "items": schema["items"]},
                        {"type": "null"},
                    ]
                    schema.pop("type", None)  # Remove direct "type" field
                    schema.pop(
                        "minItems", None
                    )  # Remove `minItems`, not needed with null

        # Process $defs and remove after expansion
        if "$defs" in schema:
            for def_name, def_schema in schema["$defs"].items():
                schema["$defs"][def_name] = StructureHandler.enforce_strict_json_schema(
                    def_schema
                )
            schema.pop("$defs", None)

        # Process anyOf and allOf schemas recursively
        for key in ("anyOf", "allOf"):
            if key in schema and isinstance(schema[key], list):
                schema[key] = [
                    StructureHandler.enforce_strict_json_schema(subschema)
                    for subschema in schema[key]
                ]

        return schema

    @staticmethod
    def unwrap_annotated_type(tp: Any) -> Any:
        origin = get_origin(tp)
        if origin is Annotated:
            args = get_args(tp)
            return StructureHandler.unwrap_annotated_type(args[0])
        if hasattr(tp, "__supertype__"):  # for NewType
            return StructureHandler.unwrap_annotated_type(tp.__supertype__)
        return tp

    @staticmethod
    def resolve_all_pydantic_models(tp: Any) -> List[Type[BaseModel]]:
        models = []

        tp = StructureHandler.unwrap_annotated_type(tp)
        origin = get_origin(tp)
        args = get_args(tp)

        if isinstance(tp, type):
            try:
                if issubclass(tp, BaseModel):
                    return [tp]
            except TypeError:
                pass

        if origin in (list, List, tuple, Iterable) and args:
            inner = args[0]
            if isinstance(inner, type):
                try:
                    if issubclass(inner, BaseModel):
                        return [inner]
                except TypeError:
                    pass
            else:
                logger.debug(
                    f"[resolve] Skipping non-class inner: {inner} ({type(inner)})"
                )

        if origin is Union:
            for arg in args:
                if isinstance(arg, type):
                    try:
                        if issubclass(arg, BaseModel):
                            models.append(arg)
                    except TypeError:
                        continue

        return list(dict.fromkeys(models))

    @staticmethod
    def resolve_response_model(tp: Any) -> Optional[Type[BaseModel]]:
        """
        Resolves a single Pydantic model from a type annotation if available.

        This method attempts to extract exactly one BaseModel from the given type. It is used
        to determine if structured output formatting (e.g., JSON schema or function call) should be applied.

        - If the annotation is a BaseModel or a container of one (e.g., List[BaseModel]), it returns the model.
        - If no Pydantic models are found (e.g., str, int), it returns None without logging.
        - If multiple Pydantic models are found (e.g., Union[ModelA, ModelB]), it logs a warning and returns None.

        Args:
            tp (Any): The return type annotation to analyze.

        Returns:
            Optional[Type[BaseModel]]: The resolved model class, or None if not applicable or ambiguous.
        """
        tp = StructureHandler.unwrap_annotated_type(tp)
        models = StructureHandler.resolve_all_pydantic_models(tp)

        if len(models) == 1:
            return models[0]
        elif len(models) == 0:
            return None  # No model = primitive or unsupported type → silently skip
        else:
            logger.warning(
                f"Ambiguous model resolution: found multiple models in {tp}. Returning None."
            )
            return None

    @staticmethod
    def validate_against_signature(result: Any, expected_type: Any) -> Any:
        """
        Validates a result against an expected return annotation type.

        Supports:
        - Single BaseModel
        - List[BaseModel]
        - Union[BaseModel, ...]
        - Primitives (int, str, bool, etc.)
        - Dict[str, Any], List[Dict[str, Any]]

        Returns:
            Any: The validated and possibly transformed result (e.g., model_dump()).

        Raises:
            TypeError: If validation fails or types mismatch.
        """
        expected_type = StructureHandler.unwrap_annotated_type(expected_type)

        origin = get_origin(expected_type)
        args = get_args(expected_type)

        # Handle one or more BaseModels
        models = StructureHandler.resolve_all_pydantic_models(expected_type)
        for model_cls in models:
            try:
                if isinstance(result, list):
                    return [
                        StructureHandler.validate_response(item, model_cls).model_dump()
                        for item in result
                    ]
                else:
                    validated = StructureHandler.validate_response(result, model_cls)
                    return validated.model_dump()
            except ValidationError:
                continue

        # Handle Union[str, dict, etc.]
        if origin is Union:
            for variant in args:
                if isinstance(variant, type) and isinstance(result, variant):
                    return result

        # Handle Dict[str, Any]
        if origin is dict and isinstance(result, dict):
            return result

        # Handle List[Dict[str, Any]]
        if origin is list and args and args[0] is dict and isinstance(result, list):
            return result

        # Fallback for primitives via TypeAdapter
        try:
            logger.debug(f"Falling back to TypeAdapter for type: {expected_type}")
            adapter = TypeAdapter(expected_type)
            return adapter.validate_python(result)
        except ValidationError as e:
            raise TypeError(f"Validation failed for type {expected_type}: {e}")

================
File: llm/__init__.py
================
from .base import LLMClientBase
from .chat import ChatClientBase
from .openai.client import OpenAIClient, AzureOpenAIClient
from .openai.chat import OpenAIChatClient
from .openai.audio import OpenAIAudioClient
from .openai.embeddings import OpenAIEmbeddingClient
from .huggingface.client import HFHubInferenceClientBase
from .huggingface.chat import HFHubChatClient
from .nvidia.client import NVIDIAClientBase
from .nvidia.chat import NVIDIAChatClient
from .nvidia.embeddings import NVIDIAEmbeddingClient
from .elevenlabs import ElevenLabsSpeechClient
from .dapr import DaprChatClient

================
File: llm/base.py
================
from pydantic import BaseModel, PrivateAttr
from abc import ABC, abstractmethod
from typing import Any


class LLMClientBase(BaseModel, ABC):
    """
    Abstract base class for LLM models.
    """

    # Private attributes for provider and api
    _provider: str = PrivateAttr()
    _api: str = PrivateAttr()

    # Private attributes for config and client
    _config: Any = PrivateAttr()
    _client: Any = PrivateAttr()

    @property
    def provider(self) -> str:
        return self._provider

    @property
    def api(self) -> str:
        return self._api

    @property
    def config(self) -> Any:
        return self._config

    @property
    def client(self) -> Any:
        return self._client

    @abstractmethod
    def get_client(self) -> Any:
        """Abstract method to get the client for the LLM model."""
        pass

    @abstractmethod
    def get_config(self) -> Any:
        """Abstract method to get the configuration for the LLM model."""
        pass

    def refresh_client(self) -> None:
        """
        Public method to refresh the client by regenerating the config and client.
        """
        # Refresh config and client using the current state
        self._config = self.get_config()
        self._client = self.get_client()

================
File: llm/chat.py
================
from typing import Union, Dict, Any, Optional, Iterable, List, Iterator, Type
from dapr_agents.prompt.base import PromptTemplateBase
from dapr_agents.prompt.prompty import Prompty
from pydantic import BaseModel, Field
from abc import ABC, abstractmethod
from pathlib import Path


class ChatClientBase(BaseModel, ABC):
    """
    Base class for chat-specific functionality.
    Handles Prompty integration and provides abstract methods for chat client configuration.
    """

    prompty: Optional[Prompty] = Field(
        default=None, description="Instance of the Prompty object (optional)."
    )
    prompt_template: Optional[PromptTemplateBase] = Field(
        default=None, description="Prompt template for rendering (optional)."
    )

    @classmethod
    @abstractmethod
    def from_prompty(
        cls,
        prompty_source: Union[str, Path],
        timeout: Union[int, float, Dict[str, Any]] = 1500,
    ) -> "ChatClientBase":
        """
        Abstract method to load a Prompty source and configure the chat client.

        Args:
            prompty_source (Union[str, Path]): Source of the Prompty, either a file path or inline Prompty content.
            timeout (Union[int, float, Dict[str, Any]]): Timeout for requests.

        Returns:
            ChatClientBase: Configured chat client instance.
        """
        pass

    @abstractmethod
    def generate(
        self,
        messages: Union[
            str, Dict[str, Any], BaseModel, Iterable[Union[Dict[str, Any], BaseModel]]
        ] = None,
        input_data: Optional[Dict[str, Any]] = None,
        model: Optional[str] = None,
        tools: Optional[List[Union[Dict[str, Any]]]] = None,
        response_format: Optional[Type[BaseModel]] = None,
        structured_mode: Optional[str] = None,
        **kwargs,
    ) -> Union[Iterator[Dict[str, Any]], Dict[str, Any]]:
        """
        Abstract method to generate chat completions.

        Args:
            messages (Optional): Either pre-set messages or None if using input_data.
            input_data (Optional[Dict[str, Any]]): Input variables for prompt templates.
            model (Optional[str]): Specific model to use for the request, overriding the default.
            tools (Optional[List[Union[Dict[str, Any]]]]): List of tools for the request.
            response_format (Optional[Type[BaseModel]]): Optional Pydantic model for structured response parsing.
            structured_mode (Optional[str]): Mode for structured output.
            **kwargs: Additional parameters for the chat completion API.

        Returns:
            Union[Iterator[Dict[str, Any]], Dict[str, Any]]: The chat completion response(s).
        """
        pass

================
File: memory/__init__.py
================
from .base import MemoryBase
from .liststore import ConversationListMemory
from .vectorstore import ConversationVectorMemory
from .daprstatestore import ConversationDaprStateMemory

================
File: memory/base.py
================
from dapr_agents.types import BaseMessage
from pydantic import BaseModel, ConfigDict
from abc import ABC, abstractmethod
from typing import List


class MemoryBase(BaseModel, ABC):
    """
    Abstract base class for managing message memory. This class defines a standard interface for memory operations,
    allowing for different implementations of message storage mechanisms in subclasses.
    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @abstractmethod
    def add_message(self, message: BaseMessage):
        """
        Adds a single message to the memory storage.

        Args:
            message (BaseMessage): The message object to be added.

        Note:
            This method must be implemented by subclasses.
        """
        pass

    @abstractmethod
    def add_messages(self, messages: List[BaseMessage]):
        """
        Adds a list of messages to the memory storage.

        Args:
            messages (List[BaseMessage]): A list of message objects to be added.

        Note:
            This method must be implemented by subclasses.
        """
        pass

    @abstractmethod
    def add_interaction(
        self, user_message: BaseMessage, assistant_message: BaseMessage
    ):
        """
        Adds a user-assistant interaction to the memory storage.

        Args:
            user_message (BaseMessage): The user message.
            assistant_message (BaseMessage): The assistant message.
        """
        pass

    @abstractmethod
    def get_messages(self) -> List[BaseMessage]:
        """
        Retrieves all messages from the memory storage.

        Returns:
            List[BaseMessage]: A list of all stored messages.

        Note:
            This method must be implemented by subclasses.
        """
        pass

    @abstractmethod
    def reset_memory(self):
        """
        Clears all messages from the memory storage.

        Note:
            This method must be implemented by subclasses.
        """
        pass

================
File: memory/daprstatestore.py
================
from dapr_agents.storage.daprstores.statestore import DaprStateStore
from dapr_agents.types import BaseMessage
from dapr_agents.memory import MemoryBase
from typing import List, Union, Optional, Dict, Any
from pydantic import Field, model_validator
from datetime import datetime
import json
import uuid
import logging

logger = logging.getLogger(__name__)


def generate_numeric_session_id() -> int:
    """
    Generates a random numeric session ID by extracting digits from a UUID.

    Returns:
        int: A numeric session ID.
    """
    return int("".join(filter(str.isdigit, str(uuid.uuid4()))))


class ConversationDaprStateMemory(MemoryBase):
    """
    Manages conversation memory stored in a Dapr state store. Each message in the conversation is saved
    individually with a unique key and includes a session ID and timestamp for querying and retrieval.
    """

    store_name: str = Field(
        default="statestore", description="The name of the Dapr state store."
    )
    session_id: Optional[Union[str, int]] = Field(
        default=None, description="Unique identifier for the conversation session."
    )

    # Private attribute to hold the initialized DaprStateStore
    dapr_store: Optional[DaprStateStore] = Field(
        default=None, init=False, description="Dapr State Store."
    )

    @model_validator(mode="before")
    def set_session_id(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sets a numeric session ID if none is provided.

        Args:
            values (Dict[str, Any]): The dictionary of attribute values before initialization.

        Returns:
            Dict[str, Any]: Updated values including the generated session ID if not provided.
        """
        if not values.get("session_id"):
            values["session_id"] = generate_numeric_session_id()
        return values

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes the Dapr state store after validation
        """
        self.dapr_store = DaprStateStore(store_name=self.store_name)
        logger.info(
            f"ConversationDaprStateMemory initialized with session ID: {self.session_id}"
        )

        # Complete post-initialization
        super().model_post_init(__context)

    def _get_message_key(self, message_id: str) -> str:
        """
        Generates a unique key for each message using session_id and message_id.

        Args:
            message_id (str): A unique identifier for the message.

        Returns:
            str: A composite key for storing individual messages.
        """
        return f"{self.session_id}:{message_id}"

    def add_message(self, message: Union[Dict, BaseMessage]):
        """
        Adds a single message to the memory and saves it to the Dapr state store.

        Args:
            message (Union[Dict, BaseMessage]): The message to add to the memory.
        """

        if isinstance(message, BaseMessage):
            message = message.model_dump()

        message_id = str(uuid.uuid4())
        message_key = self._get_message_key(message_id)
        message.update(
            {
                "sessionId": self.session_id,
                "createdAt": datetime.now().isoformat() + "Z",
            }
        )

        existing = self.get_messages()
        existing.append(message)

        logger.debug(
            f"Adding message with key {message_key} to session {self.session_id}"
        )
        self.dapr_store.save_state(
            self.session_id, json.dumps(existing), {"contentType": "application/json"}
        )

    def add_messages(self, messages: List[Union[Dict, BaseMessage]]):
        """
        Adds multiple messages to the memory and saves each one individually to the Dapr state store.

        Args:
            messages (List[Union[Dict, BaseMessage]]): A list of messages to add to the memory.
        """
        logger.info(f"Adding {len(messages)} messages to session {self.session_id}")
        for message in messages:
            if isinstance(message, BaseMessage):
                message = message.model_dump()
            self.add_message(message)

    def add_interaction(
        self, user_message: BaseMessage, assistant_message: BaseMessage
    ):
        """
        Adds a user-assistant interaction to the memory storage and saves it to the state store.

        Args:
            user_message (BaseMessage): The user message.
            assistant_message (BaseMessage): The assistant message.
        """
        self.add_messages([user_message, assistant_message])

    def _decode_message(self, message_data: Union[bytes, str]) -> dict:
        """
        Decodes the message data if it's in bytes, otherwise parses it as a JSON string.

        Args:
            message_data (Union[bytes, str]): The message data to decode.

        Returns:
            dict: The decoded message as a dictionary.
        """
        if isinstance(message_data, bytes):
            message_data = message_data.decode("utf-8")
        return json.loads(message_data)

    def get_messages(self, limit: int = 100) -> List[Dict[str, str]]:
        """
        Retrieves messages stored in the state store for the current session_id, with an optional limit.

        Args:
            limit (int): The maximum number of messages to retrieve. Defaults to 100.

        Returns:
            List[Dict[str, str]]: A list containing the 'content' and 'role' fields of the messages.
        """
        response = self.query_messages(session_id=self.session_id)
        if response and response.data:
            raw_messages = json.loads(response.data)
            if raw_messages:
                messages = [
                    {"content": msg.get("content"), "role": msg.get("role")}
                    for msg in raw_messages
                ]

                logger.info(
                    f"Retrieved {len(messages)} messages for session {self.session_id}"
                )
                return messages

        return []

    def query_messages(self, session_id: str) -> List[Dict[str, str]]:
        """
        Queries messages from the state store based on a pre-constructed query string.

        Args:
            query (Optional[str]): A JSON-formatted query string to be executed.

        Returns:
            List[Dict[str, str]]: A list containing the 'content' and 'role' fields of the messages.
        """
        logger.debug(f"Executing query for session {self.session_id}")
        states_metadata = {"contentType": "application/json"}
        response = self.dapr_store.get_state(session_id, state_metadata=states_metadata)
        return response

    def reset_memory(self):
        """
        Clears all messages stored in the memory and resets the state store for the current session.
        """
        self.dapr_store.delete_state(self.session_id)
        logger.info(f"Memory reset for session {self.session_id} completed.")

================
File: memory/liststore.py
================
from dapr_agents.memory import MemoryBase
from dapr_agents.types import BaseMessage
from pydantic import Field
from typing import List, Dict, Union


class ConversationListMemory(MemoryBase):
    """
    Memory storage for conversation messages using a list-based approach. This class provides a simple way to store,
    retrieve, and manage messages during a conversation session.
    """

    messages: List[BaseMessage] = Field(
        default_factory=list,
        description="List of messages stored in conversation memory.",
    )

    def add_message(self, message: Union[Dict, BaseMessage]):
        """
        Adds a single message to the end of the memory list.

        Args:
            message (Union[Dict, BaseMessage]): The message to add to the memory.
        """
        self.messages.append(self._convert_to_dict(message))

    def add_messages(self, messages: List[Union[Dict, BaseMessage]]):
        """
        Adds multiple messages to the memory by appending each message from the provided list to the end of the memory list.

        Args:
            messages (List[Union[Dict, BaseMessage]]): A list of messages to add to the memory.
        """
        self.messages.extend(self._convert_to_dict(msg) for msg in messages)

    def add_interaction(
        self, user_message: BaseMessage, assistant_message: BaseMessage
    ):
        """
        Adds a user-assistant interaction to the memory storage.

        Args:
            user_message (BaseMessage): The user message.
            assistant_message (BaseMessage): The assistant message.
        """
        self.add_messages([user_message, assistant_message])

    def get_messages(self) -> List[BaseMessage]:
        """
        Retrieves a copy of all messages stored in the memory.

        Returns:
            List[BaseMessage]: A list containing copies of all stored messages.
        """
        return self.messages.copy()

    def reset_memory(self):
        """Clears all messages stored in the memory, resetting the memory to an empty state."""
        self.messages.clear()

    @staticmethod
    def _convert_to_dict(message: Union[Dict, BaseMessage]) -> Dict:
        """
        Converts a BaseMessage to a dictionary if necessary.

        Args:
            message (Union[Dict, BaseMessage]): The message to potentially convert.

        Returns:
            Dict: The message as a dictionary.
        """
        return message.model_dump() if isinstance(message, BaseMessage) else message

================
File: memory/vectorstore.py
================
from dapr_agents.storage.vectorstores import VectorStoreBase
from dapr_agents.types import MessageContent, UserMessage, AssistantMessage
from dapr_agents.memory import MemoryBase
from datetime import datetime, timezone
from pydantic import Field
from typing import List, Optional
import uuid
import logging

logger = logging.getLogger(__name__)


class ConversationVectorMemory(MemoryBase):
    """
    Memory storage using a vector store, managing data storage and retrieval in a vector store for conversation sessions.
    """

    vector_store: VectorStoreBase = Field(
        ..., description="The vector store instance used for message storage."
    )

    def add_message(self, message: MessageContent):
        """
        Adds a single message to the vector store.

        Args:
            message (MessageContent): The message to add to the vector store.
        """
        metadata = {
            "role": message.role,
            f"{message.role}_message": message.content,
            "message_id": str(uuid.uuid4()),
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }
        self.vector_store.add(documents=[message.content], metadatas=[metadata])

    def add_messages(self, messages: List[MessageContent]):
        """
        Adds multiple messages to the vector store.

        Args:
            messages (List[MessageContent]): A list of messages to add to the vector store.
        """
        contents = [msg.content for msg in messages]
        metadatas = [
            {
                "role": msg.role,
                f"{msg.role}_message": msg.content,
                "message_id": str(uuid.uuid4()),
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }
            for msg in messages
        ]
        self.vector_store.add(contents, metadatas)

    def add_interaction(
        self, user_message: UserMessage, assistant_message: AssistantMessage
    ):
        """
        Adds a user-assistant interaction to the vector store as a single document.

        Args:
            user_message (UserMessage): The user message.
            assistant_message (AssistantMessage): The assistant message.
        """
        conversation_id = str(uuid.uuid4())
        conversation_text = (
            f"User: {user_message.content}\nAssistant: {assistant_message.content}"
        )
        conversation_embeddings = self.vector_store.embed_documents([conversation_text])
        metadata = {
            "user_message": user_message.content,
            "assistant_message": assistant_message.content,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }
        self.vector_store.add(
            documents=[conversation_text],
            embeddings=conversation_embeddings,
            metadatas=[metadata],
            ids=[conversation_id],
        )

    def get_messages(
        self,
        query_embeddings: Optional[List[List[float]]] = None,
        k: int = 4,
        distance_metric: str = "cosine",
    ) -> List[MessageContent]:
        """
        Retrieves messages from the vector store. If a query is provided, it performs a similarity search.

        Args:
            query_embeddings (Optional[List[List[float]]]): The query embeddings for similarity search.
            k (int): The number of similar results to retrieve.
            distance_metric (str): The distance metric to use ("l2", "ip", "cosine").

        Returns:
            List[MessageContent]: A list of all stored or similar messages.
        """
        if query_embeddings:
            logger.info("Getting conversations related to user's query...")
            return self.get_similar_conversation(
                query_embeddings=query_embeddings, k=k, distance_metric=distance_metric
            )

        logger.info("Getting all conversations.")
        items = self.vector_store.get(include=["documents", "metadatas"])
        messages = []
        for item in items:
            metadata = item["metadata"]
            if (
                metadata
                and "user_message" in metadata
                and "assistant_message" in metadata
            ):
                messages.append(UserMessage(metadata["user_message"]))
                messages.append(AssistantMessage(metadata["assistant_message"]))
        return messages

    def reset_memory(self):
        """Clears all messages from the vector store."""
        self.vector_store.reset()

    def get_similar_conversation(
        self,
        query_embeddings: Optional[List[List[float]]] = None,
        k: int = 4,
        distance_metric: str = "cosine",
    ) -> List[MessageContent]:
        """
        Performs a similarity search in the vector store and retrieves the conversation pairs.

        Args:
            query_embeddings (Optional[List[List[float]]]): The query embeddings.
            k (int): The number of results to return.
            distance_metric (str): The distance metric to use ("l2", "ip", "cosine").

        Returns:
            List[MessageContent]: A list of user and assistant messages in chronological order.
        """
        distance_thresholds = {"l2": 1.0, "ip": 0.5, "cosine": 0.75}
        distance_threshold = distance_thresholds.get(distance_metric, 0.75)
        results = self.vector_store.search_similar(
            query_embeddings=query_embeddings, k=k
        )
        messages = []

        if not results or not results["ids"][0]:
            return (
                messages  # Return an empty list if no similar conversations are found
            )

        for idx, distance in enumerate(results["distances"][0]):
            if distance <= distance_threshold:
                metadata = results["metadatas"][0][idx]
                if metadata:
                    timestamp = metadata.get("timestamp")
                    if "user_message" in metadata and "assistant_message" in metadata:
                        user_message = UserMessage(metadata["user_message"])
                        assistant_message = AssistantMessage(
                            metadata["assistant_message"]
                        )
                        messages.append((user_message, assistant_message, timestamp))
                    elif "user_message" in metadata:
                        user_message = UserMessage(metadata["user_message"])
                        messages.append((user_message, None, timestamp))
                    elif "assistant_message" in metadata:
                        assistant_message = AssistantMessage(
                            metadata["assistant_message"]
                        )
                        messages.append((None, assistant_message, timestamp))

        messages.sort(key=lambda x: x[2])
        sorted_messages = [msg for pair in messages for msg in pair[:2] if msg]

        return sorted_messages

================
File: prompt/utils/chat.py
================
from dapr_agents.types.message import (
    BaseMessage,
    SystemMessage,
    UserMessage,
    AssistantMessage,
    ToolMessage,
)
from dapr_agents.prompt.utils.jinja import (
    render_jinja_template,
    extract_jinja_variables,
)
from dapr_agents.prompt.utils.fstring import (
    render_fstring_template,
    extract_fstring_variables,
)
from typing import Any, Dict, List, Tuple, Union, Optional
import re
import logging

logger = logging.getLogger(__name__)

DEFAULT_FORMATTER_MAPPING = {
    "f-string": render_fstring_template,
    "jinja2": render_jinja_template,
}

DEFAULT_VARIABLE_EXTRACTOR_MAPPING = {
    "f-string": extract_fstring_variables,
    "jinja2": extract_jinja_variables,
}


class ChatPromptHelper:
    """
    Utility class for handling various operations on chat prompt messages, such as
    formatting, normalizing, and extracting variables.

    Attributes:
        _ROLE_MAP (Dict[str, Type[BaseMessage]]): A mapping of role names to message classes.
    """

    _ROLE_MAP = {
        "system": SystemMessage,
        "user": UserMessage,
        "assistant": AssistantMessage,
        "tool": ToolMessage,
    }

    @classmethod
    def normalize_chat_messages(cls, variable_value: Any) -> List[BaseMessage]:
        """
        Normalize the variable value into a list of BaseMessages, handling strings, dictionaries, and lists.

        Args:
            variable_value (Any): The value associated with a placeholder variable to normalize.

        Returns:
            List[BaseMessage]: A list of normalized BaseMessage instances.

        Raises:
            ValueError: If an unsupported type is encountered within the list or variable.
        """
        normalized_messages = []

        def validate_and_create_message(
            role: str, content: str, message_data: dict
        ) -> BaseMessage:
            if role not in cls._ROLE_MAP:
                raise ValueError(
                    f"Unrecognized role '{role}' in message: {message_data}"
                )
            return cls.create_message(role, content, message_data)

        if isinstance(variable_value, str):
            normalized_messages.append(cls.create_message("user", variable_value, {}))
        elif isinstance(variable_value, list):
            for item in variable_value:
                if isinstance(item, str):
                    normalized_messages.append(cls.create_message("user", item, {}))
                elif isinstance(item, BaseMessage):
                    normalized_messages.append(item)
                elif isinstance(item, dict):
                    role = item.get("role", "user")
                    content = item.get("content", "")
                    normalized_messages.append(
                        validate_and_create_message(role, content, item)
                    )
                else:
                    raise ValueError(
                        f"Unsupported type in list for variable: {type(item)}"
                    )
        elif isinstance(variable_value, dict):
            role = variable_value.get("role", "user")
            content = variable_value.get("content", "")
            normalized_messages.append(
                validate_and_create_message(role, content, variable_value)
            )
        else:
            raise ValueError(f"Unsupported type for variable: {type(variable_value)}")

        return normalized_messages

    @classmethod
    def format_message(
        cls,
        message: Union[Tuple[str, str], Dict[str, Any], BaseMessage],
        template_format: str,
        **kwargs: Any,
    ) -> BaseMessage:
        """
        Format a single message by replacing template variables based on the specified format.

        Args:
            message (Union[Tuple[str, str], Dict[str, Any], BaseMessage]): The message to format.
            template_format (str): The format for rendering ('f-string' or 'jinja2').
            **kwargs: Variables used to populate placeholders within the message.

        Returns:
            BaseMessage: The message with variables replaced as per the template format.
        """
        role, content = cls.extract_role_and_content(message)
        content = cls.format_content(content, template_format=template_format, **kwargs)
        return cls.create_message(role, content, message)

    @staticmethod
    def format_content(content: str, template_format: str, **kwargs: Any) -> str:
        """
        Apply template formatting to the content string using the specified format.

        Args:
            content (str): The content string to format.
            template_format (str): Template format ('f-string' or 'jinja2').
            **kwargs: Variables for populating placeholders within the content.

        Returns:
            str: The formatted content.
        """
        formatter = DEFAULT_FORMATTER_MAPPING.get(template_format)
        if not formatter:
            raise ValueError(f"Unsupported template format: {template_format}")
        return formatter(content, **kwargs)

    @classmethod
    def extract_role_and_content(
        cls, message: Union[Tuple[str, str], Dict[str, Any], BaseMessage]
    ) -> Tuple[str, str]:
        """
        Extract role and content from a message.

        Args:
            message (Union[Tuple[str, str], Dict[str, Any], BaseMessage]): A message object in the form of a tuple, dictionary, or BaseMessage.

        Returns:
            Tuple[str, str]: Extracted role and content.

        Raises:
            ValueError: If the message is not in a supported format.
        """
        if isinstance(message, tuple) and len(message) == 2:
            return message[0], message[1]
        elif isinstance(message, dict):
            return message.get("role"), message.get("content", "")
        elif isinstance(message, BaseMessage):
            return message.role, message.content
        else:
            raise ValueError(
                "Message must be a tuple (role, content), a dict with 'role' and 'content', or a BaseMessage instance."
            )

    @classmethod
    def create_message(
        cls, role: str, content: str, message_data: Dict[str, Any]
    ) -> BaseMessage:
        """
        Create a BaseMessage instance based on role.

        Args:
            role (str): Role of the message (system, user, assistant, tool).
            content (str): Message content.
            message_data (Dict[str, Any]): Additional data (e.g., tool_call_id for tool messages).

        Returns:
            BaseMessage: Formatted message instance.

        Raises:
            ValueError: If the role is not recognized.
        """
        if role not in cls._ROLE_MAP:
            raise ValueError(f"Invalid message role: {role}")

        message_class = cls._ROLE_MAP[role]
        if role == "tool":
            return message_class(
                content=content, tool_call_id=message_data.get("tool_call_id")
            )
        return message_class(content=content)

    @classmethod
    def get_message_class(cls, role: str) -> Union[BaseMessage, None]:
        """Get the message class for a given role."""
        role = role.lower()
        return cls._ROLE_MAP.get(role, None)

    @staticmethod
    def parse_role_content(content: str) -> Tuple[List[str], Optional[str]]:
        """
        Parse the formatted content into role-based chunks and any remaining plain text.
        - `role_chunks`: List of chunks containing roles and their messages.
        - `plain_text`: Text that does not match any role pattern.

        Returns:
            Tuple[List[str], Optional[str]]: Role-based chunks and any remaining plain text.
        """
        # Regex to split on role definitions, capturing role-based content.
        role_pattern = (
            r"(?i)^\s*#?\s*("
            + "|".join(ChatPromptHelper._ROLE_MAP.keys())
            + r")\s*:\s*\n"
        )

        # First, check for role-based patterns. If none, return the entire content as plain_text.
        if not re.search(role_pattern, content, flags=re.MULTILINE):
            return [], content.strip()

        # Split the content on the role pattern.
        chunks = re.split(role_pattern, content, flags=re.MULTILINE)

        # Extract plain text that is not part of role-based content, if any.
        plain_text = None
        if (
            chunks
            and chunks[0].strip()
            and chunks[0].lower() not in ChatPromptHelper._ROLE_MAP
        ):
            plain_text = chunks.pop(0).strip()

        # Filter out empty or whitespace-only chunks from role-based content.
        role_chunks = [chunk.strip() for chunk in chunks if chunk.strip()]

        return role_chunks, plain_text

    @staticmethod
    def to_message(role: str, content: str) -> BaseMessage:
        """
        Parse a single chunk of content into a message object.
        """
        role = role.strip().lower()
        content = content.strip()

        logger.debug(f"Parsing role: '{role}', content: '{content[:30]}...'")

        # Map role to a message class
        message_class = ChatPromptHelper.get_message_class(role)
        if not message_class:
            raise ValueError(f"Invalid message role: '{role}'")

        if not content:
            raise ValueError(f"Content missing for role: '{role}'")

        return message_class(content=content)

    @staticmethod
    def parse_as_messages(
        content: str,
    ) -> Tuple[Optional[List[BaseMessage]], Optional[str]]:
        """
        Parse the content into a list of role-based messages and any unstructured plain text.

        Returns:
            Tuple[List[BaseMessage], Optional[str]]: Parsed messages if role-based chunks are found,
            and any remaining plain text if detected.
        """
        role_chunks, plain_text = ChatPromptHelper.parse_role_content(content)

        # If there are no role-based chunks, return the plain_text directly.
        if not role_chunks:
            logger.debug("No role-based content found; returning plain text.")
            return [], plain_text

        # Parse each role-based chunk to extract messages.
        messages = []
        role = None

        for chunk in role_chunks:
            if chunk.lower() in ChatPromptHelper._ROLE_MAP.keys():
                role = chunk  # Assign role if chunk matches a defined role.
            elif (
                role
            ):  # If a role is set, treat this chunk as the content for that role.
                messages.append(ChatPromptHelper.to_message(role, chunk))
                role = None
            else:
                raise ValueError(f"Unexpected content without a role: {chunk}")

        return messages, plain_text

================
File: prompt/utils/fstring.py
================
from typing import Any, List


def render_fstring_template(template: str, **kwargs: Any) -> str:
    """
    Render an f-string style template by formatting it with the provided variables.

    Args:
        template (str): The f-string style template.
        **kwargs: Variables to be used for formatting the template.

    Returns:
        str: The rendered template string with variables replaced.
    """
    return template.format(**kwargs)


def extract_fstring_variables(template: str) -> List[str]:
    """
    Extract variables from an f-string style template.

    Args:
        template (str): The f-string style template.

    Returns:
        List[str]: A list of variable names found in the template.
    """
    return [
        var.strip("{}")
        for var in template.split()
        if var.startswith("{") and var.endswith("}")
    ]

================
File: prompt/utils/jinja.py
================
from jinja2 import Environment, Template
from jinja2.meta import find_undeclared_variables
from typing import List, Any


def render_jinja_template(template: str, **kwargs: Any) -> str:
    """
    Render a Jinja2 template using the provided variables.

    Args:
        template (str): The Jinja2 template string.
        **kwargs: Variables to be used in rendering the template.

    Returns:
        str: The rendered template string.
    """
    return Template(template).render(**kwargs)


def extract_jinja_variables(template: str) -> List[str]:
    """
    Extract undeclared variables from a Jinja2 template. These variables represent placeholders
    that need to be filled in during rendering.

    Args:
        template (str): The Jinja2 template string.

    Returns:
        List[str]: A list of undeclared variable names in the template.
    """
    environment = Environment()
    parsed_content = environment.parse(template)

    # Extract all undeclared variables (placeholders)
    undeclared_variables = find_undeclared_variables(parsed_content)

    return list(undeclared_variables)

================
File: prompt/utils/prompty.py
================
from dapr_agents.types.message import (
    UserMessage,
    AssistantMessage,
    SystemMessage,
    ToolMessage,
    BaseMessage,
)
from dapr_agents.prompt.utils.fstring import extract_fstring_variables
from dapr_agents.prompt.utils.jinja import extract_jinja_variables
from typing import Dict, Any, Tuple, Optional, Union, List, Literal
from jinja2 import Template, TemplateError
from pathlib import Path
import yaml
import re
import os
import json
import logging

logger = logging.getLogger(__name__)


class RoleMap:
    """
    A utility class to map roles to message types.
    This ensures consistency in how roles like 'system', 'user', etc., are handled.
    """

    _ROLE_MAP = {
        "system": SystemMessage,
        "user": UserMessage,
        "assistant": AssistantMessage,
        "tool": ToolMessage,
    }

    @classmethod
    def get_message_class(cls, role: str) -> Union[BaseMessage, None]:
        """Get the message class for a given role."""
        role = role.lower()
        return cls._ROLE_MAP.get(role, None)

    @classmethod
    def add_custom_role(cls, role: str, message_class: BaseMessage):
        """Add custom roles dynamically."""
        cls._ROLE_MAP[role.lower()] = message_class


class PromptyHelper:
    """
    Utility class for handling operations related to Prompty files,
    including parsing frontmatter, file loading, environment variable resolution,
    and preparing inputs for Prompty templates.
    """

    @staticmethod
    def parse_prompty_content(
        prompty_source: Union[Path, str],
    ) -> Tuple[Dict[str, Any], str]:
        """
        Extract YAML frontmatter and markdown content from a Prompty source, which can be a file path or raw content.

        Args:
            prompty_source (Union[Path, str]): A file path to the Prompty file or the inline content as a string.

        Returns:
            Tuple[Dict[str, Any], str]: A tuple with parsed YAML metadata and markdown content.

        Raises:
            ValueError: If the frontmatter format is invalid.
        """
        # Check if `prompty_source` is a file path and read from it
        if isinstance(prompty_source, Path) and prompty_source.exists():
            with open(prompty_source, "r", encoding="utf-8") as f:
                content = f.read()
        else:
            content = prompty_source  # Treat as inline content

        pattern = r"-{3,}\n(.*?)\n-{3,}\n(.*)"
        match = re.search(pattern, content, re.DOTALL)

        if match:
            yaml_frontmatter = match.group(1)
            markdown_content = match.group(2)
            yaml_metadata = yaml.safe_load(yaml_frontmatter)
            return yaml_metadata, markdown_content
        else:
            raise ValueError(
                "Invalid Prompty content format: could not extract frontmatter."
            )

    @staticmethod
    def resolve_env(
        value: Any, parent: Optional[Path] = None, env_error: bool = True
    ) -> Any:
        """
        Resolve environment variables or file references from the input value.
        If the value is not a string (e.g., a list or dict), return it as-is.
        """
        # If the value is not a string, just return it (no need to resolve env variables)
        if not isinstance(value, str):
            return value

        # Handle string values (resolve environment variables or file references)
        if value.startswith("${") and value.endswith("}"):
            variable = value[2:-1].split(":")

            # Handle environment variables ${env:VAR_NAME} or legacy ${VAR_NAME}
            if variable[0] == "env" and len(variable) > 1:
                result = PromptyHelper.process_env(
                    variable[1], env_error, variable[2] if len(variable) > 2 else None
                )

            # Handle file references ${file:path/to/file}
            elif variable[0] == "file" and len(variable) > 1:
                if parent:
                    result = PromptyHelper.process_file(variable[1], parent)
                else:
                    raise ValueError(f"File parent path not provided for {variable[1]}")

            # Fallback for legacy environment variables
            else:
                result = PromptyHelper.process_env(variable[0], env_error=False)
                if not result and len(variable) > 1:
                    result = variable[1]  # Use default value
                elif not result and env_error:
                    raise ValueError(f"Variable {variable[0]} not found in environment")

            return result

        # If it's not an environment variable reference, just return the value
        return value

    @staticmethod
    def process_env(
        var_name: str, env_error: bool = True, default_value: Optional[str] = None
    ) -> Any:
        value = os.getenv(var_name, default_value)
        if value is None and env_error:
            raise ValueError(f"Environment variable '{var_name}' not found.")
        return value

    @staticmethod
    def process_file(path: str, parent: Path) -> Any:
        """
        Process file resolution and return its contents as JSON.
        """
        full_path = parent / Path(path).resolve()
        if not full_path.exists():
            raise FileNotFoundError(f"File {full_path} not found.")

        with open(full_path, "r") as file:
            items = json.load(file)
            if isinstance(items, list):
                return [PromptyHelper.normalize(value, parent) for value in items]
            elif isinstance(items, dict):
                return {
                    key: PromptyHelper.normalize(value, parent)
                    for key, value in items.items()
                }
            return items

    @staticmethod
    def normalize(
        attribute: Any, parent: Path = Path().resolve(), env_error=True
    ) -> Any:
        """
        Normalize the attribute by resolving environment variables, file references, and default values.

        Args:
            attribute (Any): The attribute to normalize.
            parent (Path): The base path for file references, defaulting to the current working directory.
            env_error (bool): Whether to raise an error if an environment variable is missing.

        Returns:
            Any: The normalized attribute value.
        """
        if isinstance(attribute, str):
            attribute: str = attribute.strip()
            if attribute.startswith("${") and attribute.endswith("}"):
                variable = attribute[2:-1].split(":")
                if variable[0] == "env" and len(variable) > 1:
                    return PromptyHelper.process_env(
                        variable[1],
                        env_error,
                        variable[2] if len(variable) > 2 else None,
                    )
                elif variable[0] == "file" and len(variable) > 1:
                    return PromptyHelper.process_file(variable[1], parent)
                else:
                    v = PromptyHelper.process_env(variable[0], False)
                    if len(v) == 0:
                        if len(variable) > 1:
                            return variable[1]
                        elif env_error:
                            raise ValueError(
                                f"Variable {variable[0]} not found in environment"
                            )
                    return v
            elif (
                attribute.startswith("file:")
                and Path(parent / attribute.split(":")[1]).exists()
            ):
                return PromptyHelper.process_file(attribute.split(":")[1], parent)
            return attribute
        elif isinstance(attribute, list):
            return [PromptyHelper.normalize(value, parent) for value in attribute]
        elif isinstance(attribute, dict):
            return {
                key: PromptyHelper.normalize(value, parent)
                for key, value in attribute.items()
            }
        return attribute

    @staticmethod
    def prepare_inputs(
        inputs: Dict[str, Any],
        prompty_inputs: Dict[str, Any],
        prompty_sample: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Prepare the inputs by merging them with the default template values and resolving environment variables.
        """
        prepared_inputs = {}

        # Mapping from string types to Python types
        type_mapping = {
            "string": str,
            "number": (int, float),
            "array": list,
            "object": dict,
            "boolean": bool,
        }

        # Merge provided inputs with sample data in the template, if any.
        merged_inputs = {**prompty_sample, **inputs}

        # First, check for any user-provided inputs not defined in the template
        for key in inputs:
            if key not in prompty_inputs:
                # Issue a warning for undefined inputs
                logger.warning(
                    f"Input '{key}' is not defined in the template. It will be included as-is."
                )
                # Include the user-provided input as-is
                prepared_inputs[key] = inputs[key]

        # Process the defined template inputs
        for key, settings in prompty_inputs.items():
            # Check if the key exists in the provided inputs or merged inputs (user + sample)
            if key in merged_inputs:
                expected_type = settings.get("type", None)
                if isinstance(expected_type, str):
                    expected_type = type_mapping.get(expected_type.lower(), None)

                # Optionally validate types if defined in `settings`
                if expected_type and not isinstance(merged_inputs[key], expected_type):
                    raise TypeError(
                        f"Input '{key}' must be of type {expected_type.__name__}, but got {type(merged_inputs[key]).__name__}"
                    )
                prepared_inputs[key] = PromptyHelper.resolve_env(merged_inputs[key])

            # Check if there is a default value in the settings
            elif "default" in settings:
                prepared_inputs[key] = settings["default"]

            # Raise an error if the input is required and not provided
            elif settings.get("required", False):
                raise ValueError(f"Input '{key}' is required but not provided.")

            # If it's optional and no default or input is provided, assign None
            else:
                prepared_inputs[key] = None

        # process any additional keys that exist in merged_inputs but not in prompty_inputs
        for key in merged_inputs:
            if key not in prepared_inputs:
                # Log a warning if the input is not defined in the template
                logger.warning(
                    f"Input '{key}' is not defined in the template but will be included as-is."
                )
                prepared_inputs[key] = merged_inputs[key]

        return prepared_inputs

    @staticmethod
    def render_content(content: str, inputs: Dict[str, Any]) -> str:
        """
        Render the Prompty content using Jinja2 with the provided inputs.
        """
        try:
            template = Template(content)
            rendered_content = template.render(inputs)
        except TemplateError as e:
            raise ValueError(f"Template rendering error: {e}")

        return rendered_content

    @staticmethod
    def to_prompt(
        content: str,
        inputs: Dict[str, Any] = None,
        api_type: Literal["chat", "completion"] = "chat",
    ) -> Union[str, List[BaseMessage]]:
        """
        Parse the content of the Prompty template using the provided inputs.
        Return either a formatted string (for completion APIs) or a list of messages (for chat APIs).
        """
        # Render the template content using Jinja2
        if not inputs:
            rendered_content = content
        else:
            rendered_content = PromptyHelper.render_content(content, inputs)

        if api_type == "chat":
            # For chat models, split the content into role-based messages
            return PromptyHelper.parse_as_messages(rendered_content)
        else:
            # For completion models, return the entire rendered content as a string
            return rendered_content

    @staticmethod
    def parse_as_messages(content: str) -> List[BaseMessage]:
        """
        Parse the rendered content into a list of messages based on roles.
        If no roles are detected, return an empty list.

        Args:
            content (str): The content string to parse.

        Returns:
            List[BaseMessage]: List of messages with appropriate roles or an empty list if no roles are found.
        """
        chunks = PromptyHelper.parse_role_content(content)

        # Immediately return an empty list if no role-based chunks were found
        if not chunks:
            logger.info("No role-based content found; returning an empty list.")
            return []

        # Parse the content based on detected roles
        messages = []
        role = None

        for chunk in chunks:
            if chunk.lower() in RoleMap._ROLE_MAP.keys():
                role = chunk  # Assign role if chunk matches a defined role
            elif (
                role
            ):  # If a role is set, treat this chunk as the content for that role
                messages.append(PromptyHelper.to_message(role, chunk))
                role = None
            else:
                raise ValueError(f"Unexpected content without a role: {chunk}")

        return messages

    @staticmethod
    def parse_role_content(content: str) -> List[str]:
        """
        Parse the formatted content into chunks based on role delimiters.
        Only returns chunks if role-based content is detected.
        """
        # Regex to split on role definitions, ensuring we capture role-based content
        role_pattern = (
            r"(?i)^\s*#?\s*(" + "|".join(RoleMap._ROLE_MAP.keys()) + r")\s*:\s*\n"
        )

        # Check if any role-based pattern exists in the content
        if not re.search(role_pattern, content, flags=re.MULTILINE):
            # Return an empty list if no role-based pattern is found
            return []

        # Split content by role pattern if a role is detected
        chunks = re.split(role_pattern, content, flags=re.MULTILINE)

        # Filter out any empty or whitespace-only chunks
        return [chunk.strip() for chunk in chunks if chunk.strip()]

    @staticmethod
    def to_message(role: str, content: str) -> BaseMessage:
        """
        Parse a single chunk of content into a message object.
        """
        role = role.strip().lower()
        content = content.strip()

        logger.debug(f"Parsing role: '{role}', content: '{content[:30]}...'")

        # Map role to a message class
        message_class = RoleMap.get_message_class(role)
        if not message_class:
            raise ValueError(f"Invalid message role: '{role}'")

        if not content:
            raise ValueError(f"Content missing for role: '{role}'")

        return message_class(content=content)

    @staticmethod
    def extract_placeholders_from_content(
        content: str, template_format: Literal["f-string", "jinja2"] = "jinja2"
    ) -> Union[List[str], Tuple[List[str], List[str]]]:
        """
        Extract undeclared variables from the content based on the specified template format.

        Args:
            content (str): The content of the Prompty template from which to extract placeholders.
            template_format (Literal["f-string", "jinja2"]): The format of the template, either "f-string" or "jinja2". Default is "jinja2".

        Returns:
            Union[List[str], Tuple[List[str], List[str]]]:
                - For "f-string" format: A list of variable names.
                - For "jinja2" format: A list of undeclared variables.
        """
        if template_format == "jinja2":
            return extract_jinja_variables(content)
        elif template_format == "f-string":
            return extract_fstring_variables(content)
        else:
            raise ValueError(f"Unsupported template format: {template_format}")

================
File: prompt/utils/string.py
================
from dapr_agents.prompt.utils.jinja import (
    render_jinja_template,
    extract_jinja_variables,
)
from dapr_agents.prompt.utils.fstring import (
    render_fstring_template,
    extract_fstring_variables,
)
from typing import Any, Dict

DEFAULT_FORMATTER_MAPPING = {
    "f-string": render_fstring_template,
    "jinja2": render_jinja_template,
}

DEFAULT_VARIABLE_EXTRACTOR_MAPPING = {
    "f-string": extract_fstring_variables,
    "jinja2": extract_jinja_variables,
}


class StringPromptHelper:
    """
    Utility class for handling string-based operations, such as template formatting,
    extracting variables, and normalizing input data.
    """

    @staticmethod
    def format_content(content: str, template_format: str, **kwargs: Any) -> str:
        """
        Apply template formatting to the content string using the specified format.

        Args:
            content (str): The content string to format.
            template_format (str): Template format ('f-string' or 'jinja2').
            **kwargs: Variables for populating placeholders within the content.

        Returns:
            str: The formatted content.
        """
        formatter = DEFAULT_FORMATTER_MAPPING.get(template_format)
        if not formatter:
            raise ValueError(f"Unsupported template format: {template_format}")
        return formatter(content, **kwargs)

    @staticmethod
    def extract_variables(template: str, template_format: str) -> Dict[str, Any]:
        """
        Extract variables from the template content based on the template format.

        Args:
            template (str): The template content string.
            template_format (str): Template format ('f-string' or 'jinja2').

        Returns:
            Dict[str, Any]: A dictionary of extracted variables.
        """
        extractor = DEFAULT_VARIABLE_EXTRACTOR_MAPPING.get(template_format)
        if not extractor:
            raise ValueError(f"Unsupported template format: {template_format}")
        return extractor(template)

================
File: prompt/__init__.py
================
from .base import PromptTemplateBase
from .chat import ChatPromptTemplate
from .string import StringPromptTemplate
from .prompty import Prompty
from .utils.prompty import PromptyHelper

================
File: prompt/base.py
================
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Union, Callable
from pydantic import BaseModel, Field, ConfigDict
import logging

logger = logging.getLogger(__name__)


class PromptTemplateBase(ABC, BaseModel):
    """
    Abstract base class for creating prompt templates. This class provides common attributes and methods for handling
    input variables and placeholders.
    """

    input_variables: List[str] = Field(
        ...,
        description="A list of undeclared input variables expected in the prompt template.",
    )
    pre_filled_variables: Dict[str, Any] = Field(
        default_factory=dict,
        description="A dictionary of variables used to pre-fill the template.",
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @abstractmethod
    def format_prompt(self, **kwargs: Any) -> Union[str, List[Any]]:
        """
        Abstract method for formatting the prompt. Must be implemented by subclasses.

        Args:
            **kwargs: Keyword arguments to be used for formatting the prompt.

        Returns:
            Union[str, List[Any]]: The formatted prompt.
        """
        pass

    def pre_fill_variables(
        self, **kwargs: Union[str, Callable[[], str]]
    ) -> "PromptTemplateBase":
        """
        Create a new instance of the prompt template with some input variables already pre-filled.

        Args:
            **kwargs: Variables to pre-fill the template. Can be strings or callables that return strings.

        Returns:
            PromptTemplateBase: A new instance of the prompt template with the pre-filled variables set.
        """
        # Directly access the attributes without calling model_dump()
        input_variables = list(set(self.input_variables) - set(kwargs.keys()))
        pre_filled_variables = {**self.pre_filled_variables, **kwargs}

        # Directly construct a new instance to retain MessagePlaceHolder objects
        return self.__class__(
            input_variables=input_variables,
            pre_filled_variables=pre_filled_variables,
            messages=self.messages,  # Preserve MessagePlaceHolder without transforming it
            template_format=self.template_format,
        )

    def prepare_variables_for_formatting(self, **kwargs: Any) -> Dict[str, Any]:
        """
        Merge user-provided variables with pre-filled variables, resolving any callable pre-fills.

        Args:
            **kwargs: User-provided variables.

        Returns:
            Dict[str, Any]: The merged variables.
        """
        pre_filled_kwargs = {
            k: v() if callable(v) else v for k, v in self.pre_filled_variables.items()
        }
        merged_kwargs = {**pre_filled_kwargs, **kwargs}
        logger.debug(f"Merged variables for formatting: {merged_kwargs}")
        return merged_kwargs

================
File: prompt/chat.py
================
from dapr_agents.prompt.utils.jinja import (
    render_jinja_template,
    extract_jinja_variables,
)
from dapr_agents.prompt.utils.fstring import (
    render_fstring_template,
    extract_fstring_variables,
)
from dapr_agents.prompt.utils.chat import ChatPromptHelper
from dapr_agents.prompt.base import PromptTemplateBase
from dapr_agents.types.message import (
    BaseMessage,
    SystemMessage,
    UserMessage,
    AssistantMessage,
    ToolMessage,
    MessagePlaceHolder,
)
from typing import Any, Dict, List, Tuple, Union, Literal, Optional
from pydantic import Field
import logging

logger = logging.getLogger(__name__)

DEFAULT_FORMATTER_MAPPING = {
    "f-string": render_fstring_template,
    "jinja2": render_jinja_template,
}

DEFAULT_VARIABLE_EXTRACTOR_MAPPING = {
    "f-string": extract_fstring_variables,
    "jinja2": extract_jinja_variables,
}


class ChatPromptTemplate(PromptTemplateBase):
    """
    A template class designed to handle chat-based prompts. This class can format a sequence of chat messages
    and merge chat history with provided variables or placeholders.

    Attributes:
        messages (List[Union[Tuple[str, str], Dict[str, Any], BaseMessage, MessagePlaceHolder]]): A list of messages that make up the prompt template.
        template_format (Literal["f-string", "jinja2"]): The format used for rendering the template.
    """

    messages: List[
        Union[Tuple[str, str], Dict[str, Any], BaseMessage, MessagePlaceHolder]
    ] = Field(default_factory=list)
    template_format: Literal["f-string", "jinja2"] = "f-string"

    _ROLE_MAP = {
        "system": SystemMessage,
        "user": UserMessage,
        "assistant": AssistantMessage,
        "tool": ToolMessage,
    }

    def format_prompt(
        self, template_format: Optional[str] = None, **kwargs: Any
    ) -> List[Dict[str, Any]]:
        """
        Format the prompt by processing placeholders, rendering the template with variables,
        and then incorporating both plain text and role-based messages, if applicable.

        Returns:
            List[Dict[str, Any]]: The list of formatted messages as dictionaries.
        """
        template_format = template_format or self.template_format
        all_variables = self.prepare_variables_for_formatting(**kwargs)

        # Check for undeclared or missing variables
        extra_variables = [
            var
            for var in all_variables
            if var not in self.input_variables and var not in self.pre_filled_variables
        ]
        if extra_variables:
            raise ValueError(f"Undeclared variables were passed: {extra_variables}")

        missing_variables = [
            var for var in self.input_variables if var not in all_variables
        ]
        if missing_variables:
            logger.info(f"Some input variables were not provided: {missing_variables}")

        rendered_messages = []

        for item in self.messages:
            # Process MessagePlaceHolder with dynamic messages from all_variables
            if isinstance(item, MessagePlaceHolder):
                variable_name = item.variable_name
                if variable_name in all_variables:
                    normalized_messages = ChatPromptHelper.normalize_chat_messages(
                        all_variables[variable_name]
                    )
                    rendered_messages.extend(
                        [msg.model_dump() for msg in normalized_messages]
                    )
                else:
                    logger.info(
                        f"MessagePlaceHolder variable '{variable_name}' was not provided."
                    )

            # Process BaseMessage, Tuple, and Dict with parse_as_messages
            else:
                role, content = ChatPromptHelper.extract_role_and_content(item)
                formatted_content = ChatPromptHelper.format_content(
                    content, template_format, **all_variables
                )
                parsed_messages, plain_text = ChatPromptHelper.parse_as_messages(
                    formatted_content
                )

                # Add the plain text only if parsed messages are also returned
                if plain_text and parsed_messages:
                    rendered_messages.append(
                        ChatPromptHelper.create_message(
                            role, plain_text, {}
                        ).model_dump()
                    )

                # Add parsed role-based messages if they exist
                if parsed_messages:
                    rendered_messages.extend(
                        [msg.model_dump() for msg in parsed_messages]
                    )
                else:
                    # If only plain text is present (no parsed messages), add it as a single message
                    rendered_messages.append(
                        ChatPromptHelper.create_message(
                            role, plain_text or formatted_content, {}
                        ).model_dump()
                    )

        return rendered_messages

    @classmethod
    def from_messages(
        cls,
        messages: List[
            Union[Tuple[str, str], Dict[str, Any], BaseMessage, MessagePlaceHolder]
        ],
        template_format: str = "f-string",
    ) -> "ChatPromptTemplate":
        """
        Create a ChatPromptTemplate from a list of messages, including placeholders.

        Args:
            messages (List[Union[Tuple[str, str], Dict[str, Any], BaseMessage, MessagePlaceHolder]]):
                The list of messages that define the template.
            template_format (str): The format of the template, either "f-string" or "jinja2". Default is "f-string".

        Returns:
            ChatPromptTemplate: A new instance of the template with extracted input variables.
        """
        input_vars = set()

        for msg in messages:
            content = None  # Initialize content to None

            # Handle MessagePlaceHolder by adding its variable directly to input_vars
            if isinstance(msg, MessagePlaceHolder):
                input_vars.add(msg.variable_name)

            elif isinstance(msg, tuple) and len(msg) == 2:
                content = msg[1]

            elif isinstance(msg, dict) and "content" in msg:
                content = msg["content"]

            elif isinstance(msg, BaseMessage):
                content = msg.content

            if isinstance(content, str):
                input_vars.update(
                    DEFAULT_VARIABLE_EXTRACTOR_MAPPING[template_format](content)
                )

        return cls(
            input_variables=list(input_vars),
            messages=messages,
            template_format=template_format,
        )

================
File: prompt/prompty.py
================
from dapr_agents.types.llm import (
    PromptyModelConfig,
    OpenAIModelConfig,
    AzureOpenAIModelConfig,
    PromptyDefinition,
)
from dapr_agents.prompt.base import PromptTemplateBase
from dapr_agents.prompt.chat import ChatPromptTemplate
from dapr_agents.prompt.string import StringPromptTemplate
from dapr_agents.prompt.utils.prompty import PromptyHelper
from typing import Dict, Any, Union, Optional, Literal, List, Tuple
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class Prompty(PromptyDefinition):
    """
    A class to handle loading and formatting of Prompty templates for language models workflows.
    """

    def extract_input_variables(
        self, template_format: Literal["f-string", "jinja2"] = "jinja2"
    ) -> Tuple[List[str], List[str]]:
        """
        Extract all input variables from the Prompty instance, including placeholders from content,
        predefined inputs, and sample inputs. This method returns both regular input variables and
        more complex placeholders that may require additional processing.

        Args:
            template_format (Literal["f-string", "jinja2"]): Template format for content parsing. Default is 'jinja2'.

        Returns:
            Tuple[List[str], List[str]]:
                - A list of regular input variables.
                - A list of placeholders that may require extra processing (e.g., loops or attributes).
        """
        # Extract undeclared variables and placeholders from the content
        undeclared_variables = PromptyHelper.extract_placeholders_from_content(
            self.content, template_format
        )

        # Gather predefined inputs and sample inputs, default to empty dict if they are None
        predefined_inputs = list(self.inputs.keys()) if self.inputs else []
        sample_inputs = list(self.sample.keys()) if self.sample else []

        # Combine undeclared variables (regular inputs), filtered predefined inputs, and sample inputs
        regular_variables = list(
            set(undeclared_variables + predefined_inputs + sample_inputs)
        )

        return regular_variables

    def to_prompt_template(
        self, template_format: Literal["f-string", "jinja2"] = "jinja2"
    ) -> PromptTemplateBase:
        """
        Convert this Prompty instance into a PromptTemplateBase instance by pre-processing the content
        into a list of messages (for chat) or a string (for completion).
        No inputs are provided at this stage, so placeholders and dynamic parts remain in the template.

        Args:
            template_format (Literal["f-string", "jinja2"]): Template format for content parsing. Default is 'jinja2'.

        Returns:
            PromptTemplateBase: An instance of ChatPromptTemplate or StringPromptTemplate.
        """
        # Ensure that content is present in the Prompty instance
        if not self.content:
            raise ValueError(
                "Prompty instance is missing 'content'. Cannot convert to prompt template."
            )

        # Extract input variables and placeholders using the updated method
        regular_variables = self.extract_input_variables(template_format)

        # Pre-process the content into a list of messages for chat-based prompts
        if self.model.api == "chat":
            # Process the content into messages using PromptyHelper
            messages = PromptyHelper.to_prompt(self.content, api_type="chat")
            return ChatPromptTemplate(
                input_variables=regular_variables,
                messages=messages,
                template_format=template_format,
            )
        else:
            return StringPromptTemplate(
                input_variables=regular_variables,
                template=self.content,
                template_format=template_format,
            )

    @classmethod
    def load(
        cls,
        prompty_source: Union[str, Path],
        model: Optional[
            Union[OpenAIModelConfig, AzureOpenAIModelConfig, Dict[str, Any]]
        ] = None,
    ) -> "Prompty":
        """
        Load a Prompty template from a file or inline content and configure the Prompty object.

        Args:
            prompty_source (Union[str, Path]): Path to the Prompty file or inline content as a string.
            model (Optional[Union[OpenAIModelConfig, AzureOpenAIModelConfig, Dict[str, Any]]]): Optional model configuration to override.

        Returns:
            Prompty: A validated Prompty object.
        """
        # Convert prompty_source to Path if it seems like a file path
        if isinstance(prompty_source, str) and prompty_source.endswith(".prompty"):
            path_object = Path(prompty_source).resolve()
        elif isinstance(prompty_source, Path):
            path_object = prompty_source.resolve()
        else:
            path_object = None

        # Determine if we're dealing with a file path or inline content
        if path_object and path_object.exists():
            # Use file path, resolving any path-based references
            metadata, content = PromptyHelper.parse_prompty_content(path_object)
            metadata = PromptyHelper.normalize(metadata, parent=path_object.parent)
        else:
            # Treat prompty_source as inline content
            metadata, content = PromptyHelper.parse_prompty_content(prompty_source)
            metadata = PromptyHelper.normalize(metadata, parent=Path().resolve())

        # Override the model if provided
        if model:
            if isinstance(model, dict):
                metadata["model"] = PromptyModelConfig(**model)
            else:
                metadata["model"] = model.model_dump()

        # Validate and construct the Prompty object
        metadata["content"] = content
        return cls.model_validate(metadata)

================
File: prompt/string.py
================
from dapr_agents.prompt.base import PromptTemplateBase
from dapr_agents.prompt.utils.string import StringPromptHelper
from typing import Any, Union, Literal


class StringPromptTemplate(PromptTemplateBase):
    """
    A template class designed to handle string-based prompts. This class can format a string template
    by replacing variables and merging additional context or keywords.

    Attributes:
        template (str): The template string that defines the prompt structure.
        template_format (Literal["f-string", "jinja2"]): The format used for rendering the template, either f-string or Jinja2.
    """

    template: str
    template_format: Literal["f-string", "jinja2"] = "f-string"

    def format_prompt(self, **kwargs: Any) -> str:
        """
        Format the prompt by replacing variables in the template, using any provided keyword arguments.
        Validates the template structure before formatting.

        Args:
            **kwargs: Additional keyword arguments to replace variables in the template.

        Returns:
            str: The formatted prompt with all variables replaced.

        Raises:
            ValueError: If required variables are missing or extra undeclared variables are passed.
        """
        # Extract the required input variables from the template
        input_variables = StringPromptHelper.extract_variables(
            self.template, self.template_format
        )

        # Check for missing variables
        missing_vars = [var for var in input_variables if var not in kwargs]
        if missing_vars:
            raise ValueError(f"Missing required variables in template: {missing_vars}")

        # Check for extra variables that were not expected
        extra_variables = [var for var in kwargs if var not in input_variables]
        if extra_variables:
            raise ValueError(f"Undeclared variables were passed: {extra_variables}")

        # Prepare the variables for formatting
        kwargs = self.prepare_variables_for_formatting(**kwargs)

        # Use the helper to format the content
        return StringPromptHelper.format_content(
            self.template, self.template_format, **kwargs
        )

    @classmethod
    def from_template(
        cls, template: str, template_format: str = "f-string", **kwargs: Any
    ) -> "StringPromptTemplate":
        """
        Create a StringPromptTemplate from a template string.

        Args:
            template (str): The template string that defines the structure of the prompt.
            template_format (str): The format of the template, either "f-string" or "jinja2". Default is "f-string".
            **kwargs: Additional keyword arguments for the constructor.

        Returns:
            StringPromptTemplate: A new instance of the template with extracted input variables.
        """
        input_variables = StringPromptHelper.extract_variables(
            template, template_format
        )
        return cls(
            template=template,
            template_format=template_format,
            input_variables=input_variables,
            **kwargs,
        )

    def __add__(
        self, other: Union[str, "StringPromptTemplate"]
    ) -> "StringPromptTemplate":
        """
        Override the + operator to allow for combining prompt templates.

        Args:
            other: Another prompt template or string to be combined with the current one.

        Returns:
            StringPromptTemplate: A new instance of the combined templates with merged variables.
        """
        if isinstance(other, StringPromptTemplate):
            # Ensure both templates use the same format
            if self.template_format != other.template_format:
                raise ValueError(
                    "Adding prompt templates only supported for the same template format."
                )

            # Combine input variables
            input_variables = list(
                set(self.input_variables) | set(other.input_variables)
            )

            # Combine template strings
            template = self.template + other.template

            # Combine pre-filled variables
            pre_filled_variables = {
                **self.pre_filled_variables,
                **other.pre_filled_variables,
            }

            # Create and return a new combined StringPromptTemplate
            return StringPromptTemplate(
                template=template,
                template_format=self.template_format,
                input_variables=input_variables,
                pre_filled_variables=pre_filled_variables,
            )
        elif isinstance(other, str):
            # If other is a string, convert it to a StringPromptTemplate and combine
            return self + StringPromptTemplate.from_template(
                other, template_format=self.template_format
            )
        else:
            # Raise error for unsupported types
            raise NotImplementedError(f"Unsupported operand type for +: {type(other)}")

================
File: service/fastapi/__init__.py
================
from .base import FastAPIServerBase
from .dapr import DaprFastAPIServer

================
File: service/fastapi/base.py
================
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from fastapi import FastAPI
from pydantic import Field, ConfigDict
from typing import List, Optional, Any
from dapr_agents.service import APIServerBase
import uvicorn
import asyncio
import signal
import logging

logger = logging.getLogger(__name__)


class FastAPIServerBase(APIServerBase):
    """
    Abstract base class for FastAPI-based API server services.
    Provides core FastAPI functionality, with support for CORS, lifecycle management, and graceful shutdown.
    """

    description: Optional[str] = Field(
        None, description="Description of the API service."
    )
    cors_origins: Optional[List[str]] = Field(
        default_factory=lambda: ["*"], description="Allowed CORS origins."
    )
    cors_credentials: bool = Field(
        True, description="Whether to allow credentials in CORS requests."
    )
    cors_methods: Optional[List[str]] = Field(
        default_factory=lambda: ["*"], description="Allowed HTTP methods for CORS."
    )
    cors_headers: Optional[List[str]] = Field(
        default_factory=lambda: ["*"], description="Allowed HTTP headers for CORS."
    )

    # Fields initialized in model_post_init
    app: Optional[FastAPI] = Field(
        default=None, init=False, description="The FastAPI application instance."
    )
    server: Optional[Any] = Field(
        default=None,
        init=False,
        description="Server handle for running the FastAPI app.",
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def model_post_init(self, __context: Any) -> None:
        """
        Post-initialization to configure core FastAPI app and CORS settings.
        """

        # Initialize FastAPI app with title and description
        self.app = FastAPI(
            title=f"{self.service_name} API Server",
            description=self.description or self.service_name,
            lifespan=self.lifespan,
        )

        # Configure CORS settings
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=self.cors_origins,
            allow_credentials=self.cors_credentials,
            allow_methods=self.cors_methods,
            allow_headers=self.cors_headers,
        )

        logger.info(
            f"{self.service_name} FastAPI server initialized on port {self.service_port} with CORS settings."
        )

        # Call the base post-initialization
        super().model_post_init(__context)

    @asynccontextmanager
    async def lifespan(self, app: FastAPI):
        """
        Default lifespan function to manage startup and shutdown processes.
        Can be overridden by subclasses to add setup and teardown tasks such as handling agent metadata.
        """
        try:
            yield
        finally:
            await self.stop()

    async def start(self, log_level=None):
        """
        Start the FastAPI app server using the existing event loop with a specified logging level,
        and ensure that shutdown is handled gracefully with SIGINT and SIGTERM signals.
        """
        if log_level is None:
            log_level = logging.getLevelName(logger.getEffectiveLevel()).lower()

        # Set port to 0 if we want a random port
        requested_port = self.service_port or 0

        config = uvicorn.Config(
            self.app,
            host=self.service_host,
            port=requested_port,
            log_level=log_level,
        )
        self.server: uvicorn.Server = uvicorn.Server(config)

        # Add signal handlers
        loop = asyncio.get_event_loop()
        for s in (signal.SIGINT, signal.SIGTERM):
            loop.add_signal_handler(s, lambda: asyncio.create_task(self.stop()))

        # Start in background so we can inspect the actual port
        server_task = asyncio.create_task(self.server.serve())

        # Wait for startup to complete
        while not self.server.started:
            await asyncio.sleep(0.1)

        # Extract the real port from the bound socket
        if self.server.servers:
            sock = list(self.server.servers)[0].sockets[0]
            actual_port = sock.getsockname()[1]
            self.service_port = actual_port
        else:
            logger.warning(f"{self.service_name} could not determine bound port")

        await server_task

    async def stop(self):
        """
        Stop the FastAPI server gracefully.
        """
        if self.server:
            logger.info(
                f"Stopping {self.service_name} server on port {self.service_port}."
            )
            self.server.should_exit = True

================
File: service/fastapi/dapr.py
================
from dapr_agents.service.fastapi.base import FastAPIServerBase
from dapr.ext.fastapi import DaprApp
from pydantic import Field
from typing import Optional, Any
import logging

logger = logging.getLogger(__name__)


class DaprFastAPIServer(FastAPIServerBase):
    """
    A Dapr-enabled service class extending FastAPIServerBase with Dapr-specific functionality.
    """

    # Initialized in model_post_init
    dapr_app: Optional[DaprApp] = Field(
        default=None, init=False, description="DaprApp for pub/sub integration."
    )

    def model_post_init(self, __context: Any) -> None:
        """
        Post-initialization to configure the FastAPI app and Dapr-specific settings.
        """
        # Initialize inherited FastAPI app setup
        super().model_post_init(__context)

        # Initialize DaprApp for pub/sub
        self.dapr_app = DaprApp(self.app)

        logger.info(f"{self.service_name} DaprFastAPIServer initialized.")

================
File: service/__init__.py
================
from .base import APIServerBase
from .fastapi import DaprFastAPIServer

================
File: service/base.py
================
from abc import ABC, abstractmethod
from pydantic import BaseModel, Field
from typing import Optional


class APIServerBase(BaseModel, ABC):
    """
    Abstract base class for API server services.
    Supports both FastAPI and Flask implementations.
    """

    service_name: str = Field(..., description="The name of the API service.")
    service_port: Optional[int] = Field(
        default=None,
        description="Port to run the API server on. If None, use a random available port.",
    )
    service_host: str = Field("0.0.0.0", description="Host address for the API server.")

    @abstractmethod
    async def start(self, log_level=None):
        """
        Abstract method to start the API server.
        Must be implemented by subclasses.
        """
        pass

    @abstractmethod
    async def stop(self):
        """
        Abstract method to stop the API server.
        Must be implemented by subclasses.
        """
        pass

================
File: storage/daprstores/__init__.py
================
from .base import DaprStoreBase
from .statestore import DaprStateStore

================
File: storage/daprstores/base.py
================
from dapr.clients import DaprClient
from pydantic import BaseModel, Field, ConfigDict
from typing import Optional, Any


class DaprStoreBase(BaseModel):
    """
    Pydantic-based Dapr store base model with configuration options for store name, address, host, and port.
    """

    store_name: str = Field(..., description="The name of the Dapr store.")
    client: Optional[DaprClient] = Field(
        default=None, init=False, description="Dapr client for store operations."
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def model_post_init(self, __context: Any) -> None:
        """
        Post-initialization to set Dapr settings based on provided or environment values for host and port.
        """

        # Complete post-initialization
        super().model_post_init(__context)

================
File: storage/daprstores/secretstore.py
================
from dapr_agents.storage.daprstores.base import DaprStoreBase
from typing import Dict, Optional


class DaprSecretStore(DaprStoreBase):
    def get_secret(
        self, key: str, secret_metadata: Optional[Dict[str, str]] = {}
    ) -> Optional[Dict[str, str]]:
        """
        Retrieves a secret from the secret store using the provided key.

        Args:
            key (str): The key for the secret.
            secret_metadata (Dict[str, str], optional): Metadata for the secret request.

        Returns:
            Optional[Dict[str, str]]: The secret stored in the secret store, or None if not found.
        """
        response = self.client.get_secret(
            store_name=self.store_name, key=key, secret_metadata=secret_metadata
        )
        return response.secret

    def get_bulk_secret(
        self, secret_metadata: Optional[Dict[str, str]] = {}
    ) -> Dict[str, Dict[str, str]]:
        """
        Retrieves all granted secrets from the secret store.

        Args:
            secret_metadata (Dict[str, str], optional): Metadata for the secret request.

        Returns:
            Dict[str, Dict[str, str]]: A dictionary of secrets.
        """
        response = self.client.get_bulk_secret(
            store_name=self.store_name, secret_metadata=secret_metadata
        )
        return response.secrets

================
File: storage/daprstores/statestore.py
================
from dapr.clients.grpc._response import (
    BulkStatesResponse,
    BulkStateItem,
    StateResponse,
    QueryResponse,
)
from dapr.clients import DaprClient
from dapr.clients.grpc._state import StateItem
from dapr_agents.storage.daprstores.base import DaprStoreBase
from typing import Optional, Union, Dict, List, Tuple


class DaprStateStore(DaprStoreBase):
    def get_state(
        self,
        key: str,
        state_metadata: Optional[Dict[str, str]] = dict(),
    ) -> StateResponse:
        """
        Retrieves a value from the state store using the provided key.

        Args:
            key (str): The key for the state store item.
            state_metadata (Dict[str, str], optional): Dapr metadata for state request

        Returns:
            StateResponse: gRPC metadata returned from callee and value obtained from the state store
        """
        with DaprClient() as client:
            response: StateResponse = client.get_state(
                store_name=self.store_name, key=key, state_metadata=state_metadata
            )
            return response

    def try_get_state(
        self, key: str, state_metadata: Optional[Dict[str, str]] = dict()
    ) -> Tuple[bool, Optional[dict]]:
        """
        Attempts to retrieve a value from the state store using the provided key.

        Args:
            key (str): The key for the state store item.
            state_metadata (Dict[str, str], optional): Dapr metadata for state request.

        Returns:
            Tuple[bool, Optional[dict]]: A tuple where the first element is a boolean indicating whether the state exists,
                                        and the second element is the retrieved state data or None if not found.
        """
        with DaprClient() as client:
            response: StateResponse = client.get_state(
                store_name=self.store_name, key=key, state_metadata=state_metadata
            )
            if response and response.data:
                return True, response.json()
            return False, None

    def get_bulk_state(
        self,
        keys: List[str],
        parallelism: int = 1,
        states_metadata: Optional[Dict[str, str]] = None,
    ) -> List[BulkStateItem]:
        """
        Retrieves multiple values from the state store in bulk using a list of keys.

        Args:
            keys (List[str]): The keys to retrieve in bulk.
            parallelism (int, optional): Number of keys to retrieve in parallel.
            states_metadata (Dict[str, str], optional): Metadata for state request.

        Returns:
            List[BulkStateItem]: A list of BulkStateItem objects representing the retrieved state.
        """
        states_metadata = states_metadata or {}

        with DaprClient() as client:
            response: BulkStatesResponse = client.get_bulk_state(
                store_name=self.store_name,
                keys=keys,
                parallelism=parallelism,
                states_metadata=states_metadata,
            )

            if response and response.items:
                return response.items
            return []

    def save_state(
        self,
        key: str,
        value: Union[str, bytes],
        state_metadata: Optional[Dict[str, str]] = dict(),
    ):
        """
        Saves a key-value pair in the state store.

        Args:
            key (str): The key to save.
            value (Union[str, bytes]): The value to save.
            state_metadata (Dict[str, str], optional): Dapr metadata for state request
        """
        with DaprClient() as client:
            client.save_state(
                store_name=self.store_name,
                key=key,
                value=value,
                state_metadata=state_metadata,
            )

    def save_bulk_state(
        self, states: List[StateItem], metadata: Optional[Dict[str, str]] = None
    ) -> None:
        """
        Saves multiple key-value pairs to the state store in bulk.

        Args:
            states (List[StateItem]): The list of key-value pairs to save.
            metadata (Dict[str, str], optional): Metadata for the save request.
        """
        with DaprClient() as client:
            client.save_bulk_state(
                store_name=self.store_name, states=states, metadata=metadata
            )

    def delete_state(self, key: str):
        """
        Deletes a key-value pair from the state store.

        Args:
            key (str): The key to delete.
        """
        with DaprClient() as client:
            client.delete_state(store_name=self.store_name, key=key)

    def query_state(
        self, query: str, states_metadata: Optional[Dict[str, str]] = None
    ) -> QueryResponse:
        """
        Queries the state store with a specific query.

        Args:
            query (str): The query to be executed (in JSON format).
            states_metadata (Dict[str, str], optional): Custom metadata for the state request.

        Returns:
            QueryResponse: Contains query results and metadata.
        """
        with DaprClient() as client:
            client.query_state(
                store_name=self.store_name, query=query, states_metadata=states_metadata
            )

================
File: storage/graphstores/neo4j/__init__.py
================
from .base import Neo4jGraphStore
from .client import Neo4jClient

================
File: storage/graphstores/neo4j/base.py
================
from dapr_agents.storage.graphstores import GraphStoreBase
from dapr_agents.storage.graphstores.neo4j.client import Neo4jClient
from dapr_agents.storage.graphstores.neo4j.utils import value_sanitize, get_current_time
from dapr_agents.types import Node, Relationship
from pydantic import BaseModel, ValidationError, Field
from typing import Any, Dict, Optional, List, Literal
from collections import defaultdict
import logging

logger = logging.getLogger(__name__)


class Neo4jGraphStore(GraphStoreBase):
    """
    Neo4j-based graph store implementation using Pydantic.
    """

    uri: str = Field(..., description="The URI of the Neo4j database.")
    user: str = Field(..., description="The username for authentication.")
    password: str = Field(..., description="The password for authentication.")
    database: str = Field(default="neo4j", description="The Neo4j database to use.")
    sanitize: bool = Field(default=True, description="Whether to sanitize the results.")
    timeout: int = Field(default=30, description="Query timeout in seconds.")
    graph_schema: Dict[str, Any] = Field(
        default_factory=dict, description="Schema of the graph structure."
    )

    # Client initialized in model_post_init, not during regular initialization
    client: Optional[Neo4jClient] = Field(
        default=None,
        init=False,
        description="Client for interacting with the Neo4j database.",
    )

    def model_post_init(self, __context: Any) -> None:
        """
        Post-initialization to set up the Neo4j client after model instantiation.
        """
        self.client = Neo4jClient(
            uri=self.uri, user=self.user, password=self.password, database=self.database
        )
        logger.info(f"Neo4jGraphStore initialized with database {self.database}")

        # Complete post-initialization
        super().model_post_init(__context)

    def batch_execute(
        self, query: str, data: List[Dict[str, Any]], batch_size: int = 1000
    ) -> None:
        """
        Execute a Cypher query in batches.

        Args:
            query (str): The Cypher query to execute.
            data (List[Dict[str, Any]]): The data to pass to the query.
            batch_size (int): The size of each batch. Defaults to 1000.

        Raises:
            ValueError: If there is an issue with the query execution.
        """
        from neo4j.exceptions import Neo4jError

        total_batches = (len(data) + batch_size - 1) // batch_size
        for i in range(0, len(data), batch_size):
            batch = data[i : i + batch_size]
            try:
                with self.client.driver.session(
                    database=self.client.database
                ) as session:
                    # Pass the correct parameter name
                    session.run(query, {"data": batch})
                    logger.info(
                        "Processed batch %d/%d", i // batch_size + 1, total_batches
                    )
            except Neo4jError as e:
                logger.error("Batch execution failed: %s", str(e))
                raise ValueError(f"Batch execution failed: {str(e)}")

    def add_node(self, node: Node) -> None:
        """
        Add a single node to the Neo4j database.

        Args:
            node (Node): The node to add.

        Raises:
            ValueError: If there is an issue with the query execution.
        """
        # Encapsulate single node in a list and call `add_nodes`
        self.add_nodes([node])

    def add_nodes(self, nodes: List[Node], batch_size: int = 1000) -> None:
        """
        Add multiple nodes to the Neo4j database in batches, supporting different labels.
        Handles cases where vector support is not available.

        Args:
            nodes (List[Node]): A list of nodes to add.
            batch_size (int): The size of each batch. Defaults to 1000.

        Raises:
            ValueError: If there is an issue with the query execution.
        """

        # Group nodes by their labels
        nodes_by_label = defaultdict(list)
        for node in nodes:
            nodes_by_label[node.label].append(node)

        for label, grouped_nodes in nodes_by_label.items():
            query = f"""
            UNWIND $data AS node
            MERGE (n:`{label}` {{id: node.id}})
            ON CREATE SET n.createdAt = node.current_time
            SET n.updatedAt = node.current_time, n += apoc.map.clean(node.properties, [], [])
            WITH n, node.additional_labels AS additional_labels, node.embedding AS embedding
            CALL apoc.create.addLabels(n, additional_labels)
            YIELD node AS labeled_node
            WITH labeled_node AS n, embedding
            CALL apoc.do.when(
                embedding IS NOT NULL,
                'CALL db.create.setNodeVectorProperty(n, "embedding", $embedding) YIELD node RETURN node',
                'RETURN n',
                {{n: n, embedding: embedding}}
            ) YIELD value AS final_node
            RETURN final_node
            """

            # Prepare data for batch processing
            current_time = get_current_time()
            data = [
                {**n.model_dump(), "current_time": current_time} for n in grouped_nodes
            ]

            # Execute in batches for the current label
            try:
                self.batch_execute(query, data, batch_size)
                logger.info(f"Nodes with label `{label}` added successfully.")
            except ValueError as e:
                logger.error(f"Failed to add nodes with label `{label}`: {str(e)}")
                raise

    def add_relationship(self, relationship: Relationship) -> None:
        """
        Create a single relationship between two nodes in the Neo4j database.

        Args:
            relationship (Relationship): The relationship to create.

        Raises:
            ValueError: If there is an issue with the query execution.
        """
        # Encapsulate the single relationship in a list and delegate
        self.add_relationships([relationship])

    def add_relationships(
        self, relationships: List[Relationship], batch_size: int = 1000
    ) -> None:
        """
        Create multiple relationships between nodes in the Neo4j database in batches.

        Args:
            relationships (List[Relationship]): A list of relationships to create.
            batch_size (int): The size of each batch. Defaults to 1000.

        Raises:
            ValueError: If there is an issue with the query execution.
        """
        # Group relationships by their types
        relationships_by_type = defaultdict(list)
        for relationship in relationships:
            relationships_by_type[relationship.type].append(relationship)

        # Process each relationship type separately
        for rel_type, rel_group in relationships_by_type.items():
            query = f"""
            UNWIND $data AS rel
            MATCH (a {{id: rel.source_node_id}}), (b {{id: rel.target_node_id}})
            MERGE (a)-[r:`{rel_type}`]->(b)
            ON CREATE SET r.createdAt = rel.current_time
            SET r.updatedAt = rel.current_time, r += rel.properties
            RETURN r
            """

            # Prepare data for batch processing
            current_time = get_current_time()
            data = [
                {**rel.model_dump(), "current_time": current_time} for rel in rel_group
            ]

            # Execute in batches for the current relationship type
            try:
                self.batch_execute(query, data, batch_size)
                logger.info(f"Relationships of type `{rel_type}` added successfully.")
            except ValueError as e:
                logger.error(
                    f"Failed to add relationships of type `{rel_type}`: {str(e)}"
                )
                raise

    def query(
        self,
        query: str,
        params: Optional[Dict[str, Any]] = None,
        sanitize: Optional[bool] = None,
        pagination_limit: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        Execute a Cypher query against the Neo4j database and optionally sanitize or paginate the results.

        Args:
            query (str): The Cypher query to execute.
            params (Dict[str, Any], optional): Parameters for the query. Defaults to None.
            sanitize (bool, optional): Whether to sanitize the results. Defaults to class-level setting.
            pagination_limit (int, optional): Limit the number of results for pagination. Defaults to None.

        Returns:
            List[Dict[str, Any]]: A list of dictionaries representing the query results.

        Raises:
            ValueError: If there is a syntax error in the Cypher query.
            Neo4jError: If any other Neo4j-related error occurs.
        """
        from neo4j import Query
        from neo4j.exceptions import Neo4jError, CypherSyntaxError
        import time

        params = params or {}
        sanitize = sanitize if sanitize is not None else self.sanitize
        start_time = time.time()

        try:
            with self.client.driver.session(database=self.client.database) as session:
                # Add pagination support if a limit is provided
                if pagination_limit:
                    query = f"{query} LIMIT {pagination_limit}"

                result = session.run(
                    Query(text=query, timeout=self.timeout), parameters=params
                )
                json_data = [record.data() for record in result]

                # Optional sanitization of results
                if sanitize:
                    json_data = [value_sanitize(el) for el in json_data]

                execution_time = time.time() - start_time
                logger.info(
                    "Query executed successfully: %s | Time: %.2f seconds | Results: %d",
                    query,
                    execution_time,
                    len(json_data),
                )
                return json_data

        except CypherSyntaxError as e:
            logger.error("Syntax error in Cypher query: %s | Query: %s", str(e), query)
            raise ValueError(f"Syntax error in Cypher query: {str(e)}")
        except Neo4jError as e:
            logger.error("Neo4j error: %s | Query: %s", str(e), query)
            raise ValueError(f"Neo4j error: {str(e)}")

    def reset(self):
        """
        Reset the Neo4j database by deleting all nodes and relationships.

        Raises:
            ValueError: If there is an issue with the query execution.
        """
        from neo4j.exceptions import Neo4jError

        try:
            with self.client.driver.session() as session:
                session.run("CALL apoc.schema.assert({}, {})")
                session.run(
                    "CALL apoc.periodic.iterate('MATCH (n) RETURN n', 'DETACH DELETE n', {batchSize:1000, iterateList:true})"
                )
                logger.info("Database reset successfully")
        except Neo4jError as e:
            logger.error("Failed to reset database: %s", str(e))
            raise ValueError(f"Failed to reset database: {str(e)}")

    def refresh_schema(self) -> None:
        """
        Refresh the database schema, including node properties, relationship properties, constraints, and indexes.

        Raises:
            ValueError: If there is an issue with the query execution.
        """
        from neo4j.exceptions import Neo4jError

        try:
            # Define queries as constants for reusability
            NODE_PROPERTIES_QUERY = """
            CALL apoc.meta.data()
            YIELD label, property, type
            WHERE type <> 'RELATIONSHIP'
            RETURN label, collect({property: property, type: type}) AS properties
            """
            RELATIONSHIP_PROPERTIES_QUERY = """
            CALL apoc.meta.data()
            YIELD label, property, type
            WHERE type = 'RELATIONSHIP'
            RETURN label, collect({property: property, type: type}) AS properties
            """
            INDEXES_QUERY = """
            CALL apoc.schema.nodes()
            YIELD label, properties, type, size, valuesSelectivity
            WHERE type = 'RANGE'
            RETURN *, size * valuesSelectivity AS distinctValues
            """

            # Execute queries
            logger.debug("Refreshing node properties...")
            node_properties = self.query(NODE_PROPERTIES_QUERY)

            logger.debug("Refreshing relationship properties...")
            relationship_properties = self.query(RELATIONSHIP_PROPERTIES_QUERY)

            logger.debug("Refreshing constraints...")
            constraints = self.query("SHOW CONSTRAINTS")

            logger.debug("Refreshing indexes...")
            indexes = self.query(INDEXES_QUERY)

            # Transform query results into schema dictionary
            self.graph_schema = {
                "node_props": {
                    record.get("label"): record.get("properties", [])
                    for record in (node_properties or [])
                },
                "rel_props": {
                    record.get("label"): record.get("properties", [])
                    for record in (relationship_properties or [])
                },
                "constraints": constraints or [],
                "indexes": indexes or [],
            }

            logger.info("Schema refreshed successfully")

        except Neo4jError as e:
            logger.error("Failed to refresh schema: %s", str(e))
            raise ValueError(f"Failed to refresh schema: {str(e)}")

        except Exception as e:
            logger.error("Unexpected error while refreshing schema: %s", str(e))
            raise ValueError(f"Unexpected error while refreshing schema: {str(e)}")

    def get_schema(self, refresh: bool = False) -> Dict[str, Any]:
        """
        Get the schema of the Neo4jGraph store.

        Args:
            refresh (bool): Whether to refresh the schema before returning it. Defaults to False.

        Returns:
            Dict[str, Any]: The schema of the Neo4jGraph store.
        """
        if not self.graph_schema or refresh:
            self.refresh_schema()
        return self.graph_schema

    def validate_schema(self, expected_schema: BaseModel) -> bool:
        """
        Validate the current graph schema against an expected Pydantic schema model.

        Args:
            expected_schema (Type[BaseModel]): The Pydantic schema model to validate against.

        Returns:
            bool: True if schema matches, False otherwise.
        """
        # Retrieve the current schema
        current_schema = self.get_schema()

        try:
            # Attempt to initialize the expected schema with the current schema
            validated_schema = expected_schema(**current_schema)
            logger.info("Schema validation passed: %s", validated_schema)
            return True
        except ValidationError as e:
            # Handle and log validation errors
            logger.error("Schema validation failed due to validation errors:")
            for error in e.errors():
                logger.error(f"Field: {error['loc']}, Error: {error['msg']}")
            return False

    def create_vector_index(
        self,
        label: str,
        property: str,
        dimensions: int,
        similarity_function: Literal["cosine", "dot", "euclidean"] = "cosine",
    ) -> None:
        """
        Creates a vector index for a specified label and property in the Neo4j database.

        Args:
            label (str): The label of the nodes to index (non-empty).
            property (str): The property of the nodes to index (non-empty).
            dimensions (int): The number of dimensions of the vector.
            similarity_function (Literal): The similarity function to use ('cosine', 'dot', 'euclidean').

        Raises:
            ValueError: If there is an issue with the query execution or invalid arguments.
        """
        from neo4j.exceptions import Neo4jError

        # Ensure label and property are non-empty strings
        if not all([label, property]):
            raise ValueError("Both `label` and `property` must be non-empty strings.")

        # Ensure dimensions is valid
        if not isinstance(dimensions, int) or dimensions <= 0:
            raise ValueError("`dimensions` must be a positive integer.")

        # Construct index name and query
        index_name = f"{label.lower()}_{property}_vector_index"
        query = f"""
        CREATE VECTOR INDEX {index_name} IF NOT EXISTS
        FOR (n:{label})
        ON (n.{property})
        OPTIONS {{
            indexConfig: {{
                'vector.dimensions': {dimensions},
                'vector.similarity_function': '{similarity_function}'
            }}
        }}
        """

        try:
            with self.client.driver.session(database=self.database) as session:
                session.run(query)
                logger.info(
                    "Vector index `%s` for label `%s` on property `%s` created successfully.",
                    index_name,
                    label,
                    property,
                )

            # Optionally update graph schema
            if "indexes" in self.graph_schema:
                self.graph_schema["indexes"].append(
                    {
                        "label": label,
                        "property": property,
                        "dimensions": dimensions,
                        "similarity_function": similarity_function,
                    }
                )

        except Neo4jError as e:
            logger.error("Failed to create vector index: %s", str(e))
            raise ValueError(f"Failed to create vector index: {str(e)}")
        except Exception as e:
            logger.error("Unexpected error during vector index creation: %s", str(e))
            raise ValueError(f"Unexpected error: {str(e)}")

================
File: storage/graphstores/neo4j/client.py
================
from pydantic import BaseModel, Field
from typing import Optional, Any
import os
import logging

logger = logging.getLogger(__name__)


class Neo4jClient(BaseModel):
    """
    Client for interacting with a Neo4j database.
    Handles connection initialization, closing, and basic testing of connectivity.
    """

    uri: str = Field(
        default=None,
        description="The URI of the Neo4j database. Defaults to the 'NEO4J_URI' environment variable.",
    )
    user: str = Field(
        default=None,
        description="The username for Neo4j authentication. Defaults to the 'NEO4J_USERNAME' environment variable.",
    )
    password: str = Field(
        default=None,
        description="The password for Neo4j authentication. Defaults to the 'NEO4J_PASSWORD' environment variable.",
    )
    database: str = Field(
        default="neo4j", description="The default database to use. Defaults to 'neo4j'."
    )
    driver: Optional[Any] = Field(
        default=None,
        init=False,
        description="The Neo4j driver instance for database operations. Initialized in 'model_post_init'.",
    )

    def model_post_init(self, __context: Any) -> None:
        """
        Post-initialization logic to handle dynamic imports and environment variable defaults.
        """
        try:
            from neo4j import GraphDatabase
        except ImportError as e:
            raise ImportError(
                "The 'neo4j' package is required but not installed. Install it with 'pip install neo4j'."
            ) from e

        # Handle environment variable defaults
        self.uri = self.uri or os.getenv("NEO4J_URI")
        self.user = self.user or os.getenv("NEO4J_USERNAME")
        self.password = self.password or os.getenv("NEO4J_PASSWORD")

        if not all([self.uri, self.user, self.password]):
            raise ValueError(
                "Missing required connection parameters (uri, user, password). Set them as environment variables or pass explicitly."
            )

        # Initialize the Neo4j driver
        try:
            self.driver = GraphDatabase.driver(
                self.uri, auth=(self.user, self.password)
            )
            logger.info("Successfully created the driver for URI: %s", self.uri)
        except Exception as e:
            logger.error("Failed to create the driver: %s", str(e))
            raise ValueError(f"Failed to initialize the Neo4j driver: {str(e)}")

        # Complete post-initialization
        super().model_post_init(__context)

    def close(self) -> None:
        """
        Closes the Neo4j driver connection.
        """
        if self.driver is not None:
            self.driver.close()
            logger.info("Neo4j driver connection closed")

    def test_connection(self) -> bool:
        """
        Tests the connection to the Neo4j database.

        Returns:
            bool: True if the connection is successful, False otherwise.

        Raises:
            ValueError: If there is an error testing the connection.
        """
        try:
            with self.driver.session() as session:
                result = session.run(
                    "CALL dbms.components() YIELD name, versions, edition"
                )
                record = result.single()
                if record:
                    logger.info(
                        "Connected to %s version %s (%s edition)",
                        record["name"],
                        record["versions"][0],
                        record["edition"],
                    )
                    return True
                else:
                    logger.warning("No record found during the connection test")
                    return False
        except Exception as e:
            logger.error("Error testing connection: %s", str(e))
            raise ValueError(f"Error testing connection: {str(e)}")

================
File: storage/graphstores/neo4j/utils.py
================
from typing import Any
import datetime
import logging

LIST_LIMIT = 100  # Maximum number of elements in a list to be processed

logger = logging.getLogger(__name__)


def value_sanitize(data: Any) -> Any:
    """
    Sanitizes the input data (dictionary or list) for use in a language model or database context.
    This function filters out large lists, simplifies nested structures, and ensures Neo4j-specific
    data types are handled efficiently.

    Args:
        data (Any): The data to sanitize, which can be a dictionary, list, or other types.

    Returns:
        Any: The sanitized data. Returns `None` for lists exceeding the size limit or unsupported types.
    """
    if isinstance(data, dict):
        # Sanitize each key-value pair in the dictionary.
        sanitized_dict = {}
        for key, value in data.items():
            # Preserve essential metadata keys starting with "_" (e.g., Neo4j system keys).
            if key.startswith("_"):
                sanitized_dict[key] = value
                continue

            # Recursively sanitize the value.
            sanitized_value = value_sanitize(value)
            if sanitized_value is not None:
                sanitized_dict[key] = sanitized_value

        return sanitized_dict

    elif isinstance(data, list):
        # Truncate or sample large lists to avoid exceeding size limits.
        if len(data) > LIST_LIMIT:
            return data[
                :LIST_LIMIT
            ]  # Return the first `LIST_LIMIT` elements instead of discarding the list.

        # Recursively sanitize each element in the list.
        sanitized_list = [
            sanitized_item
            for item in data
            if (sanitized_item := value_sanitize(item)) is not None
        ]
        return sanitized_list

    elif isinstance(data, tuple):
        # Sanitize tuples (e.g., Neo4j relationships)
        return tuple(value_sanitize(item) for item in data)

    elif isinstance(data, datetime.datetime):
        # Convert datetime objects to ISO 8601 string for consistency.
        return data.isoformat()

    elif isinstance(data, (int, float, bool, str)):
        # Primitive types are returned as-is.
        return data

    else:
        logger.warning(
            f"Unsupported data type encountered: {type(data)}. Value: {repr(data)}"
        )
        return None  # Exclude the data entirely.


def get_current_time():
    """Get current time in UTC for creation and modification of nodes and relationships"""
    return (
        datetime.datetime.now(datetime.timezone.utc).isoformat().replace("+00:00", "Z")
    )

================
File: storage/graphstores/__init__.py
================
from .base import GraphStoreBase
from .neo4j import Neo4jGraphStore

================
File: storage/graphstores/base.py
================
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
from pydantic import BaseModel, Field, ConfigDict
import logging

logger = logging.getLogger(__name__)


class GraphStoreBase(BaseModel, ABC):
    """
    Base interface for a graph store.
    """

    client: Any = Field(..., description="The client to interact with the graph store.")

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @abstractmethod
    def add_node(self, label: str, properties: Dict[str, Any]) -> None:
        """Add a node to the graph store.

        Args:
            label (str): The label of the node.
            properties (Dict[str, Any]): The properties of the node.
        """
        pass

    @abstractmethod
    def add_relationship(
        self,
        start_node_props: Dict[str, Any],
        end_node_props: Dict[str, Any],
        relationship_type: str,
        relationship_props: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Add a relationship to the graph store.

        Args:
            start_node_props (Dict[str, Any]): The properties of the start node.
            end_node_props (Dict[str, Any]): The properties of the end node.
            relationship_type (str): The type of the relationship.
            relationship_props (Optional[Dict[str, Any]]): The properties of the relationship.
        """
        pass

    @abstractmethod
    def query(self, query: str, params: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """Execute a query against the graph store.

        Args:
            query (str): The query to execute.
            params (Dict[str, Any], optional): The parameters for the query.

        Returns:
            List[Dict[str, Any]]: The query results.
        """
        pass

    @abstractmethod
    def reset(self):
        """Reset the graph store."""
        pass

================
File: storage/vectorstores/__init__.py
================
from .base import VectorStoreBase
from .chroma import ChromaVectorStore
from .postgres import PostgresVectorStore

================
File: storage/vectorstores/base.py
================
from typing import List, Dict, Optional, Iterable, Any, Union
from dapr_agents.document.embedder.base import EmbedderBase
from pydantic import BaseModel, Field
from dapr_agents.types.document import Document
from abc import ABC, abstractmethod
import uuid
import logging

logger = logging.getLogger(__name__)


class VectorStoreBase(BaseModel, ABC):
    """Base interface for a vector store."""

    client: Any = Field(
        default=None,
        init=False,
        description="The client to interact with the vector store.",
    )
    embedding_function: EmbedderBase = Field(
        default=None,
        init=False,
        description="Embedding function to use to embed documents.",
    )

    @abstractmethod
    def add(
        self,
        documents: Iterable[str],
        embeddings: Optional[List[List[float]]] = None,
        metadatas: Optional[List[dict]] = None,
        **kwargs: Any,
    ) -> List[int]:
        """Add documents to the vector store.

        Args:
            documents (Iterable[str]): Strings to add to the vector store.
            embeddings (Optional[List[List[float]]]): The embeddings of the documents to add to vector store.
            metadatas Optional[List[dict]]: List of metadatas associated with the texts.
            kwargs: vector store specific parameters

        Returns:
            List of ids from adding the texts into the vector store.
        """
        pass

    @abstractmethod
    def delete(self, ids: List[int]) -> Optional[bool]:
        """Delete by vector ID or other criteria.

        Args:
            ids: List of ids to delete.
            kwargs: Other keyword arguments that subclasses might use.

        Returns:
            Optional[bool]: True if deletion is successful,
            False otherwise, None if not implemented.
        """
        pass

    @abstractmethod
    def get(self, ids: Optional[List[str]] = None) -> List[Dict]:
        """
        Retrieves items from vector store by IDs. If no IDs are provided, retrieves all items.

        Args:
            ids (Optional[List[str]]): The IDs of the items to retrieve. If None, retrieves all items.

        Returns:
            List[Dict]: A list of dictionaries containing the metadata and documents of the retrieved items.
        """
        pass

    @abstractmethod
    def reset(self):
        """
        Resets the vector store.
        """
        pass

    @abstractmethod
    def search_similar(
        self,
        query_texts: Optional[Union[List[str], str]] = None,
        k: int = 4,
        **kwargs: Any,
    ) -> List[Dict]:
        """Search for similar documents and Return metadata of documents most similar to query.

        Args:
            query_texts: Text to look up documents similar to.
            k: Number of Documents to return. Defaults to 4.

        Returns:
            List of metadata of Documents most similar to the query.
        """
        pass

    def add_documents(self, documents: List[Document]):
        """
        Adds `Document` objects to the Chroma collection, extracting text and metadata.

        Args:
            documents (List[Document]): List of `Document` objects to add.
        """
        texts = [doc.text for doc in documents]
        metadatas = (
            [doc.metadata for doc in documents] if documents[0].metadata else None
        )
        ids = [str(uuid.uuid4()) for _ in documents]
        self.add(documents=texts, embeddings=None, metadatas=metadatas, ids=ids)

================
File: storage/vectorstores/chroma.py
================
from dapr_agents.storage.vectorstores import VectorStoreBase
from dapr_agents.document.embedder import SentenceTransformerEmbedder
from dapr_agents.document.embedder.base import EmbedderBase
from typing import List, Dict, Optional, Iterable, Union, Any
from pydantic import Field, ConfigDict
import uuid
import logging

logger = logging.getLogger(__name__)


class ChromaVectorStore(VectorStoreBase):
    """
    Chroma-based vector store implementation with flexible persistence and server mode.
    Supports storing, querying, and filtering documents with embeddings generated on-the-fly.
    """

    name: str = Field(
        default="dapr_agents", description="The name of the Chroma collection."
    )
    api_key: Optional[str] = Field(
        None, description="API key for the embedding service."
    )
    embedding_function: Optional[EmbedderBase] = Field(
        default_factory=SentenceTransformerEmbedder,
        description="Embedding function for embedding generation.",
    )
    persistent: bool = Field(False, description="Whether to enable persistent storage.")
    path: Optional[str] = Field(None, description="Path for persistent storage.")
    client_server_mode: bool = Field(
        False, description="Whether to enable client-server mode."
    )
    host: str = Field(
        "localhost", description="Host for the Chroma server in client-server mode."
    )
    port: int = Field(
        8000, description="Port for the Chroma server in client-server mode."
    )
    settings: Optional[Any] = Field(
        None, description="Optional Chroma settings object."
    )

    client: Optional[Any] = Field(
        default=None, init=False, description="Chroma client instance."
    )
    collection: Optional[Any] = Field(
        default=None, init=False, description="Chroma collection for document storage."
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def model_post_init(self, __context: Any) -> None:
        """
        Post-initialization setup for ChromaVectorStore, configuring the embedding manager, client, and collection.
        """
        try:
            from chromadb import Client
            from chromadb.config import Settings as ChromaSettings
        except ImportError:
            raise ImportError(
                "The `chromadb` library is required to use this store. "
                "Install it using `pip install chromadb`."
            )

        if not self.settings:
            # Start with base settings
            settings_kwargs = {"allow_reset": True, "anonymized_telemetry": False}

            # Add specific settings based on the configuration
            if self.client_server_mode:
                settings_kwargs.update(
                    {
                        "chroma_server_host": self.host,
                        "chroma_server_http_port": self.port,
                        "chroma_api_impl": "chromadb.api.fastapi.FastAPI",
                    }
                )
            elif self.persistent:
                settings_kwargs.update(
                    {
                        "persist_directory": self.path or "db",
                        "is_persistent": True,
                    }
                )

            # Initialize settings
            self.settings = ChromaSettings(**settings_kwargs)

        # Initialize Chroma client and collection
        self.client: Client = Client(settings=self.settings)
        self.collection = self.client.get_or_create_collection(
            name=self.name,
            embedding_function=self.embedding_function,
            metadata={"hnsw:space": "cosine"},
        )

        logger.info(f"ChromaVectorStore initialized with collection: {self.name}")
        super().model_post_init(__context)

    def add(
        self,
        documents: Iterable[str],
        embeddings: Optional[List[List[float]]] = None,
        metadatas: Optional[List[dict]] = None,
        ids: Optional[List[str]] = None,
    ) -> List[str]:
        """
        Adds documents and their corresponding metadata to the Chroma collection.

        Args:
            documents (Iterable[str]): The documents to add to the collection.
            embeddings (Optional[List[List[float]]]): The embeddings of the documents to add to the collection.
                If None, the configured embedding function will automatically generate embeddings.
            metadatas (Optional[List[dict]]): The metadata associated with each text.
            ids (Optional[List[str]]): The IDs for each text. If not provided, random UUIDs are generated.
        """
        try:
            if ids is None:
                ids = [str(uuid.uuid4()) for _ in documents]

            self.collection.add(
                documents=documents, embeddings=embeddings, metadatas=metadatas, ids=ids
            )
            return ids
        except Exception as e:
            logger.error(f"Failed to add documents: {e}")
            raise

    def delete(self, ids: Optional[List[int]] = None) -> Optional[bool]:
        """
        Deletes items from the Chroma collection by IDs.

        Args:
            ids (Optional[List[int]]): The IDs of the items to delete.

        Returns:
            Optional[bool]: True if deletion is successful, False otherwise.
        """
        if ids:
            string_ids = [str(i) for i in ids]
            self.collection.delete(ids=string_ids)
            return True
        return False

    def get(
        self,
        ids: Optional[List[str]] = None,
        include: Optional[List[str]] = ["documents", "metadatas"],
    ) -> List[Dict]:
        """
        Retrieves items from the Chroma collection by IDs. If no IDs are provided, retrieves all items.

        Args:
            ids (Optional[List[str]]): The IDs of the items to retrieve. If None, retrieves all items.
            include (Optional[List[str]]): List of fields to include in the response.

        Returns:
            List[Dict]: A list of dictionaries containing the metadata and documents of the retrieved items.
        """
        if ids is None:
            items = self.collection.get(include=include)
        else:
            items = self.collection.get(ids=ids, include=include)

        return [
            {"id": item_id, "metadata": item_meta, "document": item_doc}
            for item_id, item_meta, item_doc in zip(
                items["ids"], items["metadatas"], items["documents"]
            )
        ]

    def reset(self):
        """Resets the Chroma database."""
        self.client.reset()

    def update(
        self,
        ids: List[str],
        metadatas: Optional[List[dict]] = None,
        documents: Optional[List[str]] = None,
    ):
        """
        Updates items in the Chroma collection.

        Args:
            ids (List[str]): The IDs of the items to update.
            metadatas (Optional[List[dict]]): The new metadata for the items.
            documents (Optional[List[str]]): The new documents for the items.
        """
        self.collection.update(ids=ids, metadatas=metadatas, documents=documents)

    def count(self) -> int:
        """
        Counts the number of items in the Chroma collection.

        Returns:
            int: The number of items in the collection.
        """
        return self.collection.count()

    def search_similar(
        self,
        query_texts: Optional[Union[List[str], str]] = None,
        query_embeddings: Optional[List[List[float]]] = None,
        k: int = 4,
    ) -> List[Dict]:
        """
        Performs a similarity search in the Chroma collection using either query texts or query embeddings.

        Args:
            query_texts (Optional[Union[List[str], str]]): The query texts.
            query_embeddings (Optional[List[List[float]]]): The query embeddings.
            k (int): The number of results to return.

        Returns:
            List[Dict]: A list of dictionaries containing the metadata of the most similar documents.
        """
        try:
            if query_texts:
                results = self.collection.query(query_texts=query_texts, n_results=k)
            elif query_embeddings:
                results = self.collection.query(
                    query_embeddings=query_embeddings, n_results=k
                )
            else:
                raise ValueError(
                    "Either query_texts or query_embeddings must be provided."
                )
            return results
        except Exception as e:
            logger.error(f"An error occurred during similarity search: {e}")
            return []

    def query_with_filters(
        self,
        query_texts: Optional[List[str]] = None,
        query_embeddings: Optional[List[List[float]]] = None,
        k: int = 4,
        where: Optional[Dict] = None,
    ) -> List[Dict]:
        """
        Queries the Chroma collection with additional filters using either query texts or query embeddings.

        Args:
            query_texts (Optional[List[str]]): The query texts.
            query_embeddings (Optional[List[List[float]]]): The query embeddings.
            k (int): The number of results to return.
            where (Optional[Dict]): Criteria to filter items.

        Returns:
            List[Dict]: A list of dictionaries containing the metadata of the most similar documents.
        """
        try:
            if query_texts is not None:
                results = self.collection.query(
                    query_texts=query_texts,
                    n_results=k,
                    where=where,
                    include=["distances", "documents", "metadatas"],
                )
            elif query_embeddings is not None:
                results = self.collection.query(
                    query_embeddings=query_embeddings,
                    n_results=k,
                    where=where,
                    include=["distances", "documents", "metadatas"],
                )
            else:
                raise ValueError(
                    "Either query_texts or query_embeddings must be provided."
                )
            return results
        except Exception as e:
            logger.error(f"An error occurred during filtered query search: {e}")
            return []

================
File: storage/vectorstores/postgres.py
================
from dapr_agents.storage.vectorstores import VectorStoreBase
from dapr_agents.document.embedder import SentenceTransformerEmbedder
from dapr_agents.document.embedder.base import EmbedderBase
from typing import List, Dict, Optional, Iterable, Any, Literal, Union
from pydantic import Field, ConfigDict
import uuid
import logging

logger = logging.getLogger(__name__)


class PostgresVectorStore(VectorStoreBase):
    """
    A PostgreSQL-based vector store implementation leveraging pgvector for similarity search.
    Supports automatic embedding generation, metadata filtering, and flexible indexing.
    """

    connection_string: str = Field(..., description="PostgreSQL connection string.")
    table_name: str = Field(
        "vector_store", description="The table name for storing vectors."
    )
    embedding_dim: Optional[int] = Field(
        None, description="Fixed dimensionality of embedding vectors (optional)."
    )
    embedding_function: Optional[EmbedderBase] = Field(
        default_factory=SentenceTransformerEmbedder,
        description="Embedding function for embedding generation.",
    )
    pool_config: Dict = Field(
        default_factory=lambda: {"min_size": 1, "max_size": 10, "timeout": 30},
        description="Connection pool settings.",
    )
    index_type: Literal["hnsw", "ivfflat", "flat"] = Field(
        "ivfflat", description="The type of index to use for vector search."
    )
    index_params: Dict = Field(
        default_factory=lambda: {"lists": 100, "m": 16, "ef_construction": 64},
        description="Parameters for the vector index.",
    )

    pool: Optional[Any] = Field(
        default=None, init=False, description="Connection pool for PostgreSQL."
    )
    tracked_dimensions: set = Field(
        default_factory=set,
        init=False,
        description="Set of tracked vector dimensions for partial indexing.",
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes the PostgreSQL connection pool, ensures table and initial index setup.
        """
        super().model_post_init(__context)

        try:
            from psycopg_pool import ConnectionPool
            from pgvector.psycopg import register_vector
        except ImportError as e:
            raise ImportError(
                "The psycopg, psycopg-pool, and pgvector libraries are required to use this store. "
                "Install them using pip install 'psycopg[binary,pool]' pgvector"
            ) from e

        try:
            self.pool: ConnectionPool = ConnectionPool(
                self.connection_string, **self.pool_config
            )
            with self.pool.connection() as conn:
                # Enable the pgvector extension if not already enabled
                with conn.cursor() as cursor:
                    cursor.execute("CREATE EXTENSION IF NOT EXISTS vector;")
                    logger.info("pgvector extension ensured in the database.")

                # Register vector type with psycopg
                register_vector(conn)

                # Create the table
                with conn.cursor() as cursor:
                    embedding_column = (
                        f"VECTOR({self.embedding_dim})"
                        if self.embedding_dim
                        else "VECTOR"
                    )
                    cursor.execute(
                        f"""
                        CREATE TABLE IF NOT EXISTS {self.table_name} (
                            id UUID PRIMARY KEY,
                            document TEXT,
                            metadata JSONB,
                            embedding {embedding_column}
                        );
                    """
                    )
                    logger.info(f"Table '{self.table_name}' ensured.")

                # Create global index for fixed-dimension embeddings
                if self.embedding_dim:
                    with conn.cursor() as cursor:
                        cursor.execute(
                            f"""
                            CREATE INDEX IF NOT EXISTS {self.table_name}_embedding_idx
                            ON {self.table_name} USING {self.index_type} (embedding vector_cosine_ops)
                            WITH ({", ".join(f"{k}={v}" for k, v in self.index_params.items())});
                            """
                        )
                        logger.info(
                            "Global index created for fixed-dimension embeddings."
                        )
                else:
                    logger.info(
                        "No fixed dimension specified; relying on dynamic partial indexing."
                    )
        except Exception as e:
            logger.error(f"Failed to initialize PostgresVectorStore: {e}")
            raise

    def _ensure_partial_index(self, dimension: int):
        """
        Creates a partial index for embeddings with the specified dimension if it doesn't already exist.
        """
        if dimension in self.tracked_dimensions:
            return

        try:
            with self.pool.connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(
                        f"""
                        CREATE INDEX IF NOT EXISTS {self.table_name}_embedding_{dimension}_idx
                        ON {self.table_name} USING hnsw ((embedding::vector({dimension})) vector_cosine_ops)
                        WHERE vector_dims(embedding) = {dimension};
                        """
                    )
                    logger.info(
                        f"Partial index created for embedding dimension {dimension}."
                    )
            self.tracked_dimensions.add(dimension)
        except Exception as e:
            logger.error(
                f"Failed to create partial index for dimension {dimension}: {e}"
            )

    def add(
        self,
        documents: Iterable[str],
        embeddings: Optional[List[List[float]]] = None,
        metadatas: Optional[List[Dict]] = None,
        ids: Optional[List[str]] = None,
        upsert: bool = False,
    ) -> List[str]:
        """
        Adds or upserts documents into the vector store.

        Args:
            documents (Iterable[str]): The documents to add or upsert.
            embeddings (Optional[List[List[float]]]): Precomputed embeddings.
            metadatas (Optional[List[Dict]]): Metadata for each document.
            ids (Optional[List[str]]): Unique IDs for the documents.
            upsert (bool): Whether to update existing records on conflict. Defaults to False.

        Returns:
            List[str]: List of IDs for the added or upserted documents.
        """

        try:
            from psycopg.types.json import Jsonb
        except ImportError as e:
            raise ImportError("Required library 'psycopg' is missing.") from e

        try:
            if embeddings is None:
                embeddings = self.embedding_function(list(documents))
                logger.info(
                    "Generated embeddings using the provided embedding function."
                )

            if ids is None:
                ids = [str(uuid.uuid4()) for _ in documents]

            on_conflict_action = (
                """
                DO UPDATE SET
                    document = EXCLUDED.document,
                    metadata = EXCLUDED.metadata,
                    embedding = EXCLUDED.embedding
                """
                if upsert
                else "DO NOTHING"
            )

            with self.pool.connection() as conn:
                with conn.cursor() as cursor:
                    for i, doc in enumerate(documents):
                        dimension = len(embeddings[i])
                        if not self.embedding_dim:
                            self._ensure_partial_index(dimension)

                        metadata = Jsonb(metadatas[i]) if metadatas else Jsonb({})
                        query = f"""
                        INSERT INTO {self.table_name} (id, document, metadata, embedding)
                        VALUES (%s, %s, %s, %s)
                        ON CONFLICT (id) {on_conflict_action};
                        """
                        cursor.execute(query, (ids[i], doc, metadata, embeddings[i]))
            logger.info(
                f"{'Upserted' if upsert else 'Added'} {len(documents)} documents."
            )
            return ids
        except Exception as e:
            logger.error(f"Failed to {'upsert' if upsert else 'add'} documents: {e}")
            raise

    def update(
        self,
        ids: List[str],
        embeddings: Optional[List[List[float]]] = None,
        metadatas: Optional[List[Dict]] = None,
        documents: Optional[List[str]] = None,
    ) -> None:
        """
        Updates existing documents in the vector store.

        Args:
            ids (List[str]): A list of document IDs to update.
            embeddings (Optional[List[List[float]]]): The new embedding vectors for the documents.
            metadatas (Optional[List[Dict]]): The new metadata for the documents.
            documents (Optional[List[str]]): The new text for the documents.

        Returns:
            None
        """
        try:
            from psycopg.types.json import Jsonb
        except ImportError as e:
            raise ImportError("Required library 'psycopg' is missing.") from e

        try:
            if not any([embeddings, metadatas, documents]):
                raise ValueError(
                    "At least one of embeddings, metadatas, or documents must be provided for update."
                )

            with self.pool.connection() as conn:
                with conn.cursor() as cursor:
                    for i, doc_id in enumerate(ids):
                        update_fields = []
                        params = []

                        if embeddings and embeddings[i]:
                            dimension = len(embeddings[i])
                            if not self.embedding_dim:
                                self._ensure_partial_index(dimension)
                            update_fields.append("embedding = %s")
                            params.append(embeddings[i])

                        if documents and documents[i]:
                            update_fields.append("document = %s")
                            params.append(documents[i])

                        if metadatas and metadatas[i]:
                            update_fields.append("metadata = %s")
                            params.append(Jsonb(metadatas[i]))

                        if not update_fields:
                            continue

                        params.append(doc_id)  # Add the ID for the WHERE clause

                        query = f"""
                        UPDATE {self.table_name}
                        SET {", ".join(update_fields)}
                        WHERE id = %s;
                        """
                        cursor.execute(query, params)

            logger.info(f"Updated {len(ids)} documents in {self.table_name}.")
        except Exception as e:
            logger.error(f"Failed to update documents: {e}")
            raise

    def delete(self, ids: List[str]) -> bool:
        """
        Deletes documents from the vector store by their IDs.

        Args:
            ids (List[str]): List of document IDs to delete.

        Returns:
            bool: True if the deletion was successful, False otherwise.
        """
        try:
            with self.pool.connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(
                        f"DELETE FROM {self.table_name} WHERE id = ANY(%s)", (ids,)
                    )
            logger.info(f"Deleted {len(ids)} documents.")
            return True
        except Exception as e:
            logger.error(f"Failed to delete documents: {e}")
            return False

    def get(
        self, ids: Optional[List[str]] = None, with_embedding: bool = False
    ) -> List[Dict]:
        """
        Retrieves items from the PostgreSQL vector store by IDs. If no IDs are provided, retrieves all items.

        Args:
            ids (Optional[List[str]]): The IDs of the items to retrieve. If None, retrieves all items.
            with_embedding (bool): Whether to include embeddings in the retrieved data.

        Returns:
            List[Dict]: A list of dictionaries containing the metadata, documents, and optionally embeddings of the retrieved items.
        """
        try:
            with self.pool.connection() as conn:
                with conn.cursor() as cursor:
                    if ids:
                        query = "SELECT id, document, metadata"
                        if with_embedding:
                            query += ", embedding"
                        query += f" FROM {self.table_name} WHERE id = ANY(%s)"
                        cursor.execute(query, (ids,))
                    else:
                        query = "SELECT id, document, metadata"
                        if with_embedding:
                            query += ", embedding"
                        query += f" FROM {self.table_name}"
                        cursor.execute(query)

                    # Get column names for row-to-dict conversion
                    colnames = [desc[0] for desc in cursor.description]
                    rows = cursor.fetchall()

            return [dict(zip(colnames, row)) for row in rows]
        except Exception as e:
            logger.error(f"Failed to retrieve documents: {e}")
            raise

    def search_similar(
        self,
        query_texts: Optional[Union[str, List[str]]] = None,
        query_embeddings: Optional[Union[List[float], List[List[float]]]] = None,
        k: int = 4,
        distance_metric: str = "cosine",
        metadata_filter: Optional[Dict] = None,
    ) -> List[Dict]:
        """
        Perform a similarity search in the vector store with optional metadata filtering.

        Args:
            query_texts (Optional[Union[str, List[str]]]): Text queries to embed and search.
                If provided, embeddings are generated using the configured embedding function.
            query_embeddings (Optional[Union[List[float], List[List[float]]]]): Precomputed embeddings
                for similarity search. Provide either `query_texts` or `query_embeddings`, not both.
            k (int): Number of top results to return. Defaults to 4.
            distance_metric (str): Distance metric to use for similarity computation.
                Supported values are:
                    - "cosine": Cosine similarity.
                    - "l2": Euclidean distance.
                    - "inner_product": Inner product similarity.
                Defaults to "cosine".
            metadata_filter (Optional[Dict]): Metadata conditions for filtering results.
                Keys are metadata fields, and values are the required values for those fields.

        Returns:
            List[Dict]: A list of dictionaries, each representing a search result. Each dictionary contains:
                - "id" (str): The ID of the matched item.
                - "document" (str): The document content of the matched item.
                - "metadata" (Dict): Metadata associated with the matched item.
                - "similarity" (float): The similarity score of the matched item.

        Raises:
            ValueError: If neither `query_texts` nor `query_embeddings` is provided, or if both are provided.
            Exception: For any issues during the database query execution.
        """
        try:
            # Validate inputs
            if not query_texts and not query_embeddings:
                raise ValueError(
                    "Either `query_texts` or `query_embeddings` must be provided."
                )
            if query_texts and query_embeddings:
                raise ValueError(
                    "Provide either `query_texts` or `query_embeddings`, not both."
                )

            # Generate embeddings if query_texts is provided
            if query_texts:
                if isinstance(query_texts, str):
                    query_texts = [query_texts]
                query_embeddings = self.embedding_function(query_texts)

            # Handle single embedding case
            if isinstance(query_embeddings[0], (int, float)):
                query_embeddings = [query_embeddings]

            # Map distance metrics to PostgreSQL operators
            operator_map = {
                "cosine": "<=>",  # Cosine similarity
                "l2": "<->",  # Euclidean distance
                "inner_product": "<#>",  # Inner product
            }
            operator = operator_map.get(distance_metric, "<=>")

            results = []
            with self.pool.connection() as conn:
                # Use a compatible row factory or default cursor
                with conn.cursor() as cursor:
                    for embedding in query_embeddings:
                        # Convert the embedding to a PostgreSQL-compatible vector format
                        embedding_vector = f"ARRAY{embedding}::vector"

                        # Base query
                        query = f"""
                        SELECT id, document, metadata, 1 - (embedding {operator} {embedding_vector}) AS similarity
                        FROM {self.table_name}
                        """

                        # Add metadata filtering conditions if provided
                        params = []
                        if metadata_filter:
                            filter_conditions = " AND ".join(
                                ["metadata ->> %s = %s" for _ in metadata_filter]
                            )
                            query += f" WHERE {filter_conditions}"
                            params.extend(
                                sum(([k, v] for k, v in metadata_filter.items()), [])
                            )

                        # Add ordering and limit
                        query += " ORDER BY similarity DESC LIMIT %s"
                        params.append(k)

                        # Execute the query
                        cursor.execute(query, tuple(params))
                        for row in cursor.fetchall():
                            results.append(row)

            # Format results for consistency
            formatted_results = [
                {
                    "id": row[0],
                    "document": row[1],
                    "metadata": row[2],
                    "similarity": row[3],
                }
                for row in results
            ]

            return formatted_results
        except Exception as e:
            logger.error(f"Failed to perform similarity search: {e}")
            raise

    def reset(self):
        """
        Resets the vector store by truncating the table, deleting all stored data.

        Returns:
            None
        """
        try:
            with self.pool.connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(f"TRUNCATE TABLE {self.table_name}")
            logger.info(f"Table '{self.table_name}' reset.")
        except Exception as e:
            logger.error(f"Failed to reset table: {e}")
            raise

    def count(self) -> int:
        """
        Counts the number of documents in the vector store.

        Returns:
            int: The total number of documents in the store.
        """
        try:
            with self.pool.connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(f"SELECT COUNT(*) FROM {self.table_name}")
                    count = cursor.fetchone()[0]
            return count
        except Exception as e:
            logger.error(f"Failed to count documents in the store: {e}")
            raise

================
File: storage/__init__.py
================
from .graphstores import GraphStoreBase, Neo4jGraphStore
from .vectorstores import VectorStoreBase, ChromaVectorStore, PostgresVectorStore

================
File: tool/http/__init__.py
================
from .client import DaprHTTPClient

================
File: tool/http/client.py
================
import os
from typing import Dict, Optional, Any, Union
from distutils.util import strtobool
import logging
import requests

from pydantic import BaseModel, Field, PrivateAttr
from dapr_agents.types import ToolError
from dapr_agents import tool
from urllib.parse import urlparse
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry import trace
from opentelemetry._logs import set_logger_provider


logger = logging.getLogger(__name__)


class DaprHTTPClient(BaseModel):
    """
    Client for sending HTTP requests to Dapr endpoints.
    """

    dapr_app_id: Optional[str] = Field(
        default="", description="Optional name of the Dapr App ID to invoke."
    )

    dapr_http_endpoint: Optional[str] = Field(
        default="",
        description="Optional name of the HTTPEndpoint to call for invocation",
    )

    http_endpoint: Optional[str] = Field(
        default="", description="Optional FQDN URL to request to."
    )

    path: Optional[str] = Field(
        default="", description="Optional name of the path to invoke."
    )

    headers: Optional[Dict[str, str]] = Field(
        default={},
        description="Default headers to include in all requests.",
    )

    # Private attributes not exposed in model schema
    _base_url: str = PrivateAttr(default="http://localhost:3500/v1.0/invoke")

    def model_post_init(self, __context: Any) -> None:
        """Initialize the client after the model is created."""

        try:
            otel_enabled: bool = bool(
                strtobool(os.getenv("DAPR_AGENTS_OTEL_ENABLED", "True"))
            )
        except ValueError:
            otel_enabled = False

        if otel_enabled:
            from dapr_agents.agent.telemetry.otel import DaprAgentsOTel  # type: ignore[import-not-found]

            otel_client = DaprAgentsOTel(
                service_name=os.getenv("OTEL_SERVICE_NAME", "dapr-http-client"),
                otlp_endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", ""),
            )
            tracer = otel_client.create_and_instrument_tracer_provider()
            trace.set_tracer_provider(tracer)

            otel_logger = otel_client.create_and_instrument_logging_provider(
                logger=logger,
            )
            set_logger_provider(otel_logger)

            RequestsInstrumentor().instrument()

        logger.debug("Initializing DaprHTTPClient client")

        super().model_post_init(__context)

    def do_http_request(
        self,
        payload: dict[str, str],
        endpoint: str = "",
        path: str = "",
        verb: str = "GET",
    ) -> Union[tuple[int, str] | ToolError]:
        """
        Send a POST request to the specified endpoint with the given input.

        Args:
            endpoint_url (str): The host of the URI to send the request to.
            payload (dict[str, str]): The payload to include in the request.
            path (str): The path of the URI to invoke including any query strings appended.
            verb (str): The HTTP verb. Either GET or POST.
        Returns:
            A tuple with the http status code and respose or a ToolError.
        """

        try:
            url = self._validate_endpoint_type(
                endpoint=endpoint, path=self.path if path == "" else path
            )
        except ToolError as e:
            logger.error(f"Error validating endpoint: {e}")
            raise e

        logger.debug(
            f"[HTTP] Sending POST request to '{url}' with input '{payload}' and headers '{self.headers}"
        )

        match verb.upper():
            case "GET":
                response = requests.get(url=str(url), headers=self.headers)
            case "POST":
                response = requests.post(
                    url=str(url), headers=self.headers, json=payload
                )
            case _:
                raise ValueError(
                    f"Value for 'verb' not in expected format ['GET'|'POST']: {verb}"
                )

        logger.debug(
            f"Request returned status code '{response.status_code}' and '{response.text}'"
        )

        if not response.ok:
            raise ToolError(
                f"Error occured sending the request. Received '{response.status_code}' - '{response.text}'"
            )

        return (response.status_code, response.text)

    def _validate_endpoint_type(
        self, endpoint: str, path: Optional[str | None]
    ) -> Union[str | ToolError]:
        if path == "":
            raise ToolError("No path provided. Please provide a valid path.")

        if isinstance(path, str) and path.startswith("/"):
            # Remove leading slash
            path = path[1:]

        try:
            if self.dapr_app_id != "":
                # Prefered option
                if isinstance(self.dapr_app_id, str) and self.dapr_app_id.endswith("/"):
                    # Remove trailing slash
                    self.dapr_app_id = self.dapr_app_id[:-1]
                url = f"{self._base_url}/{self.dapr_app_id}/method/{self.path if path == '' else path}"
            elif self.dapr_http_endpoint != "":
                # Dapr HTTPEndpoint
                if isinstance(
                    self.dapr_http_endpoint, str
                ) and self.dapr_http_endpoint.endswith("/"):
                    # Remove trailing slash
                    self.dapr_http_endpoint = self.dapr_http_endpoint[:-1]
                url = f"{self._base_url}/{self.dapr_http_endpoint}/method/{self.path if path == '' else path}"
            elif self.http_endpoint != "":
                # FQDN URL
                if isinstance(self.http_endpoint, str) and self.http_endpoint.endswith(
                    "/"
                ):
                    # Remove trailing slash
                    self.http_endpoint = self.http_endpoint[:-1]
                url = f"{self._base_url}/{self.http_endpoint}/method/{self.path if path == '' else path}"
            elif endpoint != "":
                # Fallback to default
                if isinstance(endpoint, str) and endpoint.endswith("/"):
                    # Remove trailing slash
                    endpoint = endpoint[:-1]
                url = f"{self._base_url}/{endpoint}/method/{self.path if path == '' else path}"
            else:
                raise ToolError(
                    "No endpoint provided. Please provide a valid dapr-app-id, HTTPEndpoint or endpoint."
                )
        except Exception as e:
            logger.error(f"Error validating endpoint: {e}")
            raise ToolError(
                "Error occured while validating the endpoint. Please check the provided values."
            )

        if not self._validate_url(url):
            raise ToolError(f"'{url}' is not a valid URL.")

        return url

    def _validate_url(self, url) -> bool:
        """
        Valides URL for HTTP requests
        """
        logger.debug(f"[HTTP] Url to be validated: {url}")
        try:
            parsed_url = urlparse(url=url)
            return all([parsed_url.scheme, parsed_url.netloc])
        except AttributeError:
            return False

================
File: tool/mcp/__init__.py
================
from .client import MCPClient, create_sync_mcp_client
from .transport import connect_stdio, connect_sse
from .schema import create_pydantic_model_from_schema
from .prompt import convert_prompt_message

================
File: tool/mcp/client.py
================
from contextlib import AsyncExitStack, asynccontextmanager
from typing import Dict, List, Optional, Set, Any, Type, AsyncGenerator
from types import TracebackType
import asyncio
import logging

from pydantic import BaseModel, Field, PrivateAttr
from mcp import ClientSession
from mcp.types import Tool as MCPTool, Prompt

from dapr_agents.tool import AgentTool
from dapr_agents.types import ToolError


logger = logging.getLogger(__name__)


class MCPClient(BaseModel):
    """
    Client for connecting to MCP servers and integrating their tools with the Dapr agent framework.

    This client manages connections to one or more MCP servers, retrieves their tools,
    and converts them to native AgentTool objects that can be used in the agent framework.

    Attributes:
        allowed_tools: Optional set of tool names to include (when None, all tools are included)
        server_timeout: Timeout in seconds for server connections
        sse_read_timeout: Read timeout for SSE connections in seconds
    """

    allowed_tools: Optional[Set[str]] = Field(
        default=None,
        description="Optional set of tool names to include (when None, all tools are included)",
    )
    server_timeout: float = Field(
        default=5.0, description="Timeout in seconds for server connections"
    )
    sse_read_timeout: float = Field(
        default=300.0, description="Read timeout for SSE connections in seconds"
    )

    # Private attributes not exposed in model schema
    _exit_stack: AsyncExitStack = PrivateAttr(default_factory=AsyncExitStack)
    _sessions: Dict[str, ClientSession] = PrivateAttr(default_factory=dict)
    _server_tools: Dict[str, List[AgentTool]] = PrivateAttr(default_factory=dict)
    _server_prompts: Dict[str, Dict[str, Prompt]] = PrivateAttr(default_factory=dict)
    _task_locals: Dict[str, Any] = PrivateAttr(default_factory=dict)
    _server_configs: Dict[str, Dict[str, Any]] = PrivateAttr(default_factory=dict)

    def model_post_init(self, __context: Any) -> None:
        """Initialize the client after the model is created."""
        logger.debug("Initializing MCP client")
        super().model_post_init(__context)

    @asynccontextmanager
    async def create_session(
        self, server_name: str
    ) -> AsyncGenerator[ClientSession, None]:
        """
        Create an ephemeral session for the given server and yield it.
        Used during tool execution to avoid reuse issues.

        Args:
            server_name: The server to create a session for.

        Yields:
            A short-lived, initialized MCP session.
        """
        logger.debug(f"[MCP] Creating ephemeral session for server '{server_name}'")
        session = await self._create_ephemeral_session(server_name)
        try:
            yield session
        finally:
            # Session cleanup is managed by AsyncExitStack (via transport module)
            pass

    async def connect_stdio(
        self,
        server_name: str,
        command: str,
        args: List[str],
        env: Optional[Dict[str, str]] = None,
    ) -> None:
        """
        Connect to an MCP server using stdio transport and store the connection
        metadata for future dynamic reconnection if needed.

        Args:
            server_name (str): Unique identifier for this server connection.
            command (str): Executable to run.
            args (List[str]): Command-line arguments.
            env (Optional[Dict[str, str]]): Environment variables for the process.

        Raises:
            RuntimeError: If a server with the same name is already connected.
            Exception: If connection setup or initialization fails.
        """
        logger.info(
            f"Connecting to MCP server '{server_name}' via stdio: {command} {args}"
        )

        if server_name in self._sessions:
            raise RuntimeError(f"Server '{server_name}' is already connected")

        try:
            self._task_locals[server_name] = asyncio.current_task()

            from dapr_agents.tool.mcp.transport import (
                connect_stdio as transport_connect_stdio,
            )

            session = await transport_connect_stdio(
                command=command, args=args, env=env, stack=self._exit_stack
            )

            await session.initialize()
            self._sessions[server_name] = session

            # Store how to reconnect this server later
            self._server_configs[server_name] = {
                "type": "stdio",
                "params": {"command": command, "args": args, "env": env},
            }

            logger.debug(
                f"Initialized session for server '{server_name}', loading tools and prompts"
            )
            await self._load_tools_from_session(server_name, session)
            await self._load_prompts_from_session(server_name, session)
            logger.info(f"Successfully connected to MCP server '{server_name}'")

        except Exception as e:
            logger.error(f"Failed to connect to MCP server '{server_name}': {str(e)}")
            self._sessions.pop(server_name, None)
            self._task_locals.pop(server_name, None)
            self._server_configs.pop(server_name, None)
            raise

    async def connect_sse(
        self, server_name: str, url: str, headers: Optional[Dict[str, str]] = None
    ) -> None:
        """
        Connect to an MCP server using Server-Sent Events (SSE) transport and store
        the connection metadata for future dynamic reconnection if needed.

        Args:
            server_name (str): Unique identifier for this server connection.
            url (str): The SSE endpoint URL.
            headers (Optional[Dict[str, str]]): HTTP headers to include with the request.

        Raises:
            RuntimeError: If a server with the same name is already connected.
            Exception: If connection setup or initialization fails.
        """
        logger.info(f"Connecting to MCP server '{server_name}' via SSE: {url}")

        if server_name in self._sessions:
            raise RuntimeError(f"Server '{server_name}' is already connected")

        try:
            self._task_locals[server_name] = asyncio.current_task()

            from dapr_agents.tool.mcp.transport import (
                connect_sse as transport_connect_sse,
            )

            session = await transport_connect_sse(
                url=url,
                headers=headers,
                timeout=self.server_timeout,
                read_timeout=self.sse_read_timeout,
                stack=self._exit_stack,
            )

            await session.initialize()
            self._sessions[server_name] = session

            # Store how to reconnect this server later
            self._server_configs[server_name] = {
                "type": "sse",
                "params": {
                    "url": url,
                    "headers": headers,
                    "timeout": self.server_timeout,
                    "read_timeout": self.sse_read_timeout,
                },
            }

            logger.debug(
                f"Initialized session for server '{server_name}', loading tools and prompts"
            )
            await self._load_tools_from_session(server_name, session)
            await self._load_prompts_from_session(server_name, session)
            logger.info(f"Successfully connected to MCP server '{server_name}'")

        except Exception as e:
            logger.error(f"Failed to connect to MCP server '{server_name}': {str(e)}")
            self._sessions.pop(server_name, None)
            self._task_locals.pop(server_name, None)
            self._server_configs.pop(server_name, None)
            raise

    async def _load_tools_from_session(
        self, server_name: str, session: ClientSession
    ) -> None:
        """
        Load tools from a given MCP session and convert them to AgentTools.

        Args:
            server_name: Unique identifier for this server
            session: The MCP client session
        """
        logger.debug(f"Loading tools from server '{server_name}'")

        try:
            # Get tools from the server
            tools_response = await session.list_tools()

            # Convert MCP tools to agent tools
            converted_tools = []
            for mcp_tool in tools_response.tools:
                # Skip tools not in allowed_tools if filtering is enabled
                if self.allowed_tools and mcp_tool.name not in self.allowed_tools:
                    logger.debug(
                        f"Skipping tool '{mcp_tool.name}' (not in allowed_tools)"
                    )
                    continue

                try:
                    agent_tool = await self.wrap_mcp_tool(server_name, mcp_tool)
                    converted_tools.append(agent_tool)
                except Exception as e:
                    logger.warning(
                        f"Failed to convert tool '{mcp_tool.name}': {str(e)}"
                    )

            self._server_tools[server_name] = converted_tools
            logger.info(
                f"Loaded {len(converted_tools)} tools from server '{server_name}'"
            )
        except Exception as e:
            logger.warning(
                f"Failed to load tools from server '{server_name}': {str(e)}"
            )
            self._server_tools[server_name] = []

    async def _load_prompts_from_session(
        self, server_name: str, session: ClientSession
    ) -> None:
        """
        Load prompts from a given MCP session.

        Args:
            server_name: Unique identifier for this server
            session: The MCP client session
        """
        logger.debug(f"Loading prompts from server '{server_name}'")
        try:
            response = await session.list_prompts()
            prompt_dict = {prompt.name: prompt for prompt in response.prompts}
            self._server_prompts[server_name] = prompt_dict

            loaded = [
                f"{p.name} ({len(p.arguments or [])} args)" for p in response.prompts
            ]
            logger.info(
                f"Loaded {len(loaded)} prompts from server '{server_name}': "
                + ", ".join(loaded)
            )
        except Exception as e:
            logger.warning(
                f"Failed to load prompts from server '{server_name}': {str(e)}"
            )
            self._server_prompts[server_name] = []

    async def _create_ephemeral_session(self, server_name: str) -> ClientSession:
        """
        Create a fresh session for a single tool call.

        Args:
            server_name: The MCP server to connect to.

        Returns:
            A fully initialized ephemeral ClientSession.
        """
        config = self._server_configs.get(server_name)
        if not config:
            raise ToolError(f"No stored config found for server '{server_name}'")

        try:
            if config["type"] == "stdio":
                from dapr_agents.tool.mcp.transport import connect_stdio

                session = await connect_stdio(
                    **config["params"], stack=self._exit_stack
                )
            elif config["type"] == "sse":
                from dapr_agents.tool.mcp.transport import connect_sse

                session = await connect_sse(**config["params"], stack=self._exit_stack)
            else:
                raise ToolError(f"Unknown transport type: {config['type']}")

            await session.initialize()
            return session
        except Exception as e:
            logger.error(f"Failed to create ephemeral session: {e}")
            raise ToolError(f"Could not create session for '{server_name}': {e}") from e

    async def wrap_mcp_tool(self, server_name: str, mcp_tool: MCPTool) -> AgentTool:
        """
        Wrap an MCPTool as an AgentTool with dynamic session creation at runtime,
        based on stored server configuration.

        Args:
            server_name: The MCP server that registered the tool.
            mcp_tool: The MCPTool object describing the tool.

        Returns:
            An AgentTool instance that can be executed by the agent.

        Raises:
            ToolError: If the tool cannot be executed or configuration is missing.
        """
        tool_name = f"{server_name}_{mcp_tool.name}"
        tool_docs = f"{mcp_tool.description or ''} (from MCP server: {server_name})"

        logger.debug(f"Wrapping MCP tool: {tool_name}")

        def build_executor(client: MCPClient, server_name: str, tool_name: str):
            async def executor(**kwargs: Any) -> Any:
                """
                Execute the tool using a dynamically created session context.

                Args:
                    kwargs: Input arguments to the tool.

                Returns:
                    Result from the tool execution.

                Raises:
                    ToolError: If execution fails or response is malformed.
                """
                logger.info(f"[MCP] Executing tool '{tool_name}' with args: {kwargs}")
                try:
                    async with client.create_session(server_name) as session:
                        result = await session.call_tool(tool_name, kwargs)
                        logger.debug(f"[MCP] Received result from tool '{tool_name}'")
                        return client._process_tool_result(result)
                except Exception as e:
                    logger.exception(f"Execution failed for '{tool_name}'")
                    raise ToolError(
                        f"Error executing tool '{tool_name}': {str(e)}"
                    ) from e

            return executor

        # Build executor using dynamic context-managed session resolution
        tool_func = build_executor(self, server_name, mcp_tool.name)

        # Optionally generate args model from input schema
        args_model = None
        if getattr(mcp_tool, "inputSchema", None):
            try:
                from dapr_agents.tool.mcp.schema import (
                    create_pydantic_model_from_schema,
                )

                args_model = create_pydantic_model_from_schema(
                    mcp_tool.inputSchema, f"{tool_name}Args"
                )
                logger.debug(f"Generated argument model for tool '{tool_name}'")
            except Exception as e:
                logger.warning(
                    f"Failed to create schema for tool '{tool_name}': {str(e)}"
                )

        return AgentTool(
            name=tool_name,
            description=tool_docs,
            func=tool_func,
            args_model=args_model,
        )

    def _process_tool_result(self, result: Any) -> Any:
        """
        Process the result from an MCP tool call.

        Args:
            result: The result from calling an MCP tool

        Returns:
            Processed result in a format expected by AgentTool

        Raises:
            ToolError: If the result indicates an error
        """
        # Handle error result
        if hasattr(result, "isError") and result.isError:
            error_message = "Unknown error"
            if hasattr(result, "content") and result.content:
                for content in result.content:
                    if hasattr(content, "text"):
                        error_message = content.text
                        break
            raise ToolError(f"MCP tool error: {error_message}")

        # Extract text content from result
        if hasattr(result, "content") and result.content:
            text_contents = []
            for content in result.content:
                if hasattr(content, "text"):
                    text_contents.append(content.text)

            # Return single string if only one content item
            if len(text_contents) == 1:
                return text_contents[0]
            elif text_contents:
                return text_contents
        # Fallback for unexpected formats
        return str(result)

    def get_all_tools(self) -> List[AgentTool]:
        """
        Get all tools from all connected MCP servers.

        Returns:
            A list of all available AgentTools from MCP servers
        """
        all_tools = []
        for server_tools in self._server_tools.values():
            all_tools.extend(server_tools)
        return all_tools

    def get_server_tools(self, server_name: str) -> List[AgentTool]:
        """
        Get tools from a specific MCP server.

        Args:
            server_name: The name of the server to get tools from

        Returns:
            A list of AgentTools from the specified server
        """
        return self._server_tools.get(server_name, [])

    def get_server_prompts(self, server_name: str) -> List[Prompt]:
        """
        Get all prompt definitions from a specific MCP server.

        Args:
            server_name: The name of the server to retrieve prompts from

        Returns:
            A list of Prompt objects available on the specified server.
            Returns an empty list if no prompts are available.
        """
        return list(self._server_prompts.get(server_name, {}).values())

    def get_all_prompts(self) -> Dict[str, List[Prompt]]:
        """
        Get all prompt definitions from all connected MCP servers.

        Returns:
            A dictionary mapping each server name to a list of Prompt objects.
            Returns an empty dictionary if no servers are connected.
        """
        return {
            server: list(prompts.values())
            for server, prompts in self._server_prompts.items()
        }

    def get_prompt_names(self, server_name: str) -> List[str]:
        """
        Get the names of all prompts from a specific MCP server.

        Args:
            server_name: The name of the server

        Returns:
            A list of prompt names registered on the server.
        """
        return list(self._server_prompts.get(server_name, {}).keys())

    def get_all_prompt_names(self) -> Dict[str, List[str]]:
        """
        Get prompt names from all connected servers.

        Returns:
            A dictionary mapping server names to lists of prompt names.
        """
        return {
            server: list(prompts.keys())
            for server, prompts in self._server_prompts.items()
        }

    def get_prompt_metadata(
        self, server_name: str, prompt_name: str
    ) -> Optional[Prompt]:
        """
        Retrieve the full metadata for a given prompt from a connected MCP server.

        Args:
            server_name: The server that registered the prompt
            prompt_name: The name of the prompt to retrieve

        Returns:
            The full Prompt object if available, otherwise None.
        """
        return self._server_prompts.get(server_name, {}).get(prompt_name)

    def get_prompt_arguments(
        self, server_name: str, prompt_name: str
    ) -> Optional[List[Dict[str, Any]]]:
        """
        Get the list of arguments defined for a prompt, if available.

        Useful for generating forms or validating prompt input.

        Args:
            server_name: The server where the prompt is registered
            prompt_name: The name of the prompt to inspect

        Returns:
            A list of argument definitions, or None if the prompt is not found.
        """
        prompt = self.get_prompt_metadata(server_name, prompt_name)
        return prompt.arguments if prompt else None

    def describe_prompt(self, server_name: str, prompt_name: str) -> Optional[str]:
        """
        Retrieve a human-readable description of a specific prompt.

        Args:
            server_name: The name of the server where the prompt is registered
            prompt_name: The name of the prompt to describe

        Returns:
            The description string if available, otherwise None.
        """
        prompt = self.get_prompt_metadata(server_name, prompt_name)
        return prompt.description if prompt else None

    def get_connected_servers(self) -> List[str]:
        """
        Get a list of all connected server names.

        Returns:
            List of server names that are currently connected
        """
        return list(self._sessions.keys())

    async def close(self) -> None:
        """
        Close all connections to MCP servers and clean up resources.

        This method should be called when the client is no longer needed to
        ensure proper cleanup of all resources and connections.
        """
        logger.info("Closing MCP client and all server connections")

        # Verify we're in the same task as the one that created the connections
        current_task = asyncio.current_task()
        for server_name, server_task in self._task_locals.items():
            if server_task != current_task:
                logger.warning(
                    f"Attempting to close server '{server_name}' in a different task "
                    f"than it was created in. This may cause errors."
                )

        # Close all connections
        try:
            await self._exit_stack.aclose()
            self._sessions.clear()
            self._server_tools.clear()
            self._task_locals.clear()
            logger.info("MCP client successfully closed")
        except Exception as e:
            logger.error(f"Error closing MCP client: {str(e)}")
            raise

    async def __aenter__(self) -> "MCPClient":
        """Context manager entry point."""
        return self

    async def __aexit__(
        self,
        exc_type: Optional[Type[BaseException]],
        exc_val: Optional[BaseException],
        exc_tb: Optional[TracebackType],
    ) -> None:
        """Context manager exit - close all connections."""
        await self.close()


def create_sync_mcp_client(*args, **kwargs) -> MCPClient:
    """
    Create an MCPClient with synchronous wrapper methods for each async method.

    This allows the client to be used in synchronous code.

    Args:
        *args: Positional arguments for MCPClient constructor
        **kwargs: Keyword arguments for MCPClient constructor

    Returns:
        An MCPClient with additional sync_* methods
    """
    client = MCPClient(*args, **kwargs)

    # Add sync versions of async methods
    def create_sync_wrapper(async_func):
        def sync_wrapper(*args, **kwargs):
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    raise RuntimeError(
                        f"Cannot call {async_func.__name__} synchronously in an async context. "
                        f"Use {async_func.__name__} directly instead."
                    )
            except RuntimeError:
                pass  # No event loop, which is fine for sync operation

            return asyncio.run(async_func(*args, **kwargs))

        # Copy metadata
        sync_wrapper.__name__ = f"sync_{async_func.__name__}"
        sync_wrapper.__doc__ = (
            f"Synchronous version of {async_func.__name__}.\n\n{async_func.__doc__}"
        )

        return sync_wrapper

    # Add sync wrappers for all async methods
    client.sync_connect_stdio = create_sync_wrapper(client.connect_stdio)
    client.sync_connect_sse = create_sync_wrapper(client.connect_sse)
    client.sync_close = create_sync_wrapper(client.close)

    return client

================
File: tool/mcp/prompt.py
================
from typing import Optional, Any, List, Dict
import logging

from mcp import ClientSession
from mcp.types import PromptMessage

from dapr_agents.types import UserMessage, AssistantMessage, BaseMessage

logger = logging.getLogger(__name__)


def convert_prompt_message(message: PromptMessage) -> BaseMessage:
    """
    Convert an MCP PromptMessage to a compatible internal BaseMessage.

    Args:
        message: The MCP PromptMessage instance

    Returns:
        A compatible BaseMessage subclass (UserMessage or AssistantMessage)

    Raises:
        ValueError: If the message contains unsupported content type or role
    """
    # Verify text content type is supported
    if message.content.type != "text":
        error_msg = f"Unsupported content type: {message.content.type}"
        logger.error(error_msg)
        raise ValueError(error_msg)

    # Convert based on role
    if message.role == "user":
        return UserMessage(content=message.content.text)
    elif message.role == "assistant":
        return AssistantMessage(content=message.content.text)
    else:
        # Fall back to generic message with role preserved
        logger.warning(f"Converting message with non-standard role: {message.role}")
        return BaseMessage(content=message.content.text, role=message.role)


async def load_prompt(
    session: ClientSession, prompt_name: str, arguments: Optional[Dict[str, Any]] = None
) -> List[BaseMessage]:
    """
    Fetch and convert a prompt from the MCP server to internal message format.

    Args:
        session: An initialized MCP client session
        prompt_name: The registered prompt name
        arguments: Optional dictionary of arguments to format the prompt

    Returns:
        A list of internal BaseMessage-compatible messages

    Raises:
        Exception: If prompt retrieval fails
    """
    logger.info(f"Loading prompt '{prompt_name}' from MCP server")

    try:
        # Get prompt from server
        response = await session.get_prompt(prompt_name, arguments or {})

        # Convert all messages
        converted_messages = [convert_prompt_message(m) for m in response.messages]
        logger.info(
            f"Loaded prompt '{prompt_name}' with {len(converted_messages)} messages"
        )

        return converted_messages
    except Exception as e:
        logger.error(f"Failed to load prompt '{prompt_name}': {str(e)}")
        raise

================
File: tool/mcp/schema.py
================
from typing import Any, Dict, Optional, Type, List
import logging

from pydantic import BaseModel, Field, create_model

logger = logging.getLogger(__name__)

# Mapping from JSON Schema types to Python types
TYPE_MAPPING = {
    "string": str,
    "number": float,
    "integer": int,
    "boolean": bool,
    "object": dict,
    "array": list,
    "null": type(None),
}


def create_pydantic_model_from_schema(
    schema: Dict[str, Any], model_name: str
) -> Type[BaseModel]:
    """
    Create a Pydantic model from a JSON schema definition.

    This function converts a JSON Schema object (commonly used in MCP tool definitions)
    to a Pydantic model that can be used for validation in the Dapr agent framework.

    Args:
        schema: JSON Schema dictionary containing type information
        model_name: Name for the generated model class

    Returns:
        A dynamically created Pydantic model class

    Raises:
        ValueError: If the schema is invalid or cannot be converted
    """
    logger.debug(f"Creating Pydantic model '{model_name}' from schema")

    try:
        properties = schema.get("properties", {})
        required = set(schema.get("required", []))
        fields = {}

        # Process each property in the schema
        for field_name, field_props in properties.items():
            # Get field type information
            json_type = field_props.get("type", "string")

            # Handle complex type definitions (arrays, unions, etc.)
            if isinstance(json_type, list):
                # Process union types (e.g., ["string", "null"])
                has_null = "null" in json_type
                non_null_types = [t for t in json_type if t != "null"]

                if not non_null_types:
                    # Only null type specified
                    field_type = Optional[str]
                else:
                    # Use the first non-null type
                    # TODO: Proper union type handling would be better but more complex
                    primary_type = non_null_types[0]
                    field_type = TYPE_MAPPING.get(primary_type, str)

                    # Make optional if null is included
                    if has_null:
                        field_type = Optional[field_type]
            else:
                # Simple type
                field_type = TYPE_MAPPING.get(json_type, str)

            # Handle arrays with item type information
            if json_type == "array" or (
                isinstance(json_type, list) and "array" in json_type
            ):
                # Get the items type if specified
                if "items" in field_props:
                    items_type = field_props["items"].get("type", "string")
                    if isinstance(items_type, str):
                        item_py_type = TYPE_MAPPING.get(items_type, str)
                        field_type = List[item_py_type]

            # Set default value based on required status
            if field_name in required:
                default = ...  # Required field
            else:
                default = None
                # Make optional if not already
                if not isinstance(field_type, type(Optional)):
                    field_type = Optional[field_type]

            # Add field with description and default value
            field_description = field_props.get("description", "")
            fields[field_name] = (
                field_type,
                Field(default, description=field_description),
            )

        # Create and return the model class
        return create_model(model_name, **fields)

    except Exception as e:
        logger.error(f"Failed to create model from schema: {str(e)}")
        raise ValueError(f"Invalid schema: {str(e)}")

================
File: tool/mcp/transport.py
================
from contextlib import AsyncExitStack
from typing import Optional, Dict
import logging

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.client.sse import sse_client

logger = logging.getLogger(__name__)


async def connect_stdio(
    command: str, args: list[str], env: Optional[Dict[str, str]], stack: AsyncExitStack
) -> ClientSession:
    """
    Connect to an MCP server using stdio transport.

    Args:
        command: The executable to run
        args: Command line arguments
        env: Optional environment variables
        stack: AsyncExitStack for resource management

    Returns:
        An initialized MCP client session

    Raises:
        Exception: If connection fails
    """
    logger.debug(f"Establishing stdio connection: {command} {args}")

    # Create server parameters
    params = StdioServerParameters(command=command, args=args, env=env)

    # Establish connection via stdio
    try:
        read_stream, write_stream = await stack.enter_async_context(
            stdio_client(params)
        )
        session = await stack.enter_async_context(
            ClientSession(read_stream, write_stream)
        )
        logger.debug("Stdio connection established successfully")
        return session
    except Exception as e:
        logger.error(f"Failed to establish stdio connection: {str(e)}")
        raise


async def connect_sse(
    url: str,
    headers: Optional[Dict[str, str]],
    timeout: float,
    read_timeout: float,
    stack: AsyncExitStack,
) -> ClientSession:
    """
    Connect to an MCP server using Server-Sent Events (SSE) transport.

    Args:
        url: The SSE endpoint URL
        headers: Optional HTTP headers
        timeout: Connection timeout in seconds
        read_timeout: Read timeout in seconds
        stack: AsyncExitStack for resource management

    Returns:
        An initialized MCP client session

    Raises:
        Exception: If connection fails
    """
    logger.debug(f"Establishing SSE connection to: {url}")

    # Establish connection via SSE
    try:
        read_stream, write_stream = await stack.enter_async_context(
            sse_client(url, headers, timeout, read_timeout)
        )
        session = await stack.enter_async_context(
            ClientSession(read_stream, write_stream)
        )
        logger.debug("SSE connection established successfully")
        return session
    except Exception as e:
        logger.error(f"Failed to establish SSE connection: {str(e)}")
        raise

================
File: tool/storage/__init__.py
================
from .vectorstore import VectorToolStore

================
File: tool/storage/vectorstore.py
================
from typing import List, Dict, Any
from pydantic import BaseModel, Field, ConfigDict
from dapr_agents.storage.vectorstores import VectorStoreBase
import logging

logger = logging.getLogger(__name__)


class VectorToolStore(BaseModel):
    """
    Manages tool information within a vector store, providing methods for adding tools and
    retrieving similar tools based on queries.
    """

    vector_store: VectorStoreBase = Field(
        ..., description="The vector store instance for tool data storage."
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def add_tools(self, tools: List[Dict[str, Any]]):
        """
        Adds tool information to the vector store.

        Args:
            tools (List[Dict[str, Any]]): A list of dictionaries representing tools, each containing
                definitions and optional metadata for each tool.
        """
        logger.info("Adding tools to Vector Tool Store.")

        documents = []
        metadatas = []

        for tool in tools:
            func_name = tool["definition"]["function"]["name"]
            description = tool["definition"]["function"]["description"]
            parameters = tool["definition"]["function"]["parameters"]

            # Prepare the document for each tool
            documents.append(f"{func_name}: {description}. Args schema: {parameters}")

            # Prepare metadata, ensuring 'name' is always set
            metadata = tool.get("metadata", {}).copy()
            metadata.setdefault("name", func_name)  # Ensure name is set in metadata
            metadatas.append(metadata)

        self.vector_store.add(documents=documents, metadatas=metadatas)

    def get_similar_tools(self, query_texts: str, k: int = 4) -> List[Dict[str, Any]]:
        """
        Retrieves tools from the vector store similar to the query text.

        Args:
            query_texts (str): The query string to find similar tools.
            k (int): The number of similar results to return. Defaults to 4.

        Returns:
            List[Dict[str, Any]]: List of similar tool entries based on the query.
        """
        logger.info(f"Searching for tools similar to query: {query_texts}")
        similar_docs = self.vector_store.search_similar(query_texts=query_texts, k=k)
        return similar_docs

    def get_tool_names(self) -> List[str]:
        """
        Retrieves the names of all tools stored in the vector store.

        Returns:
            List[str]: A list of tool names.
        """
        logger.info("Retrieving all tool names from Vector Tool Store.")
        tools = self.vector_store.get()
        return [tool["metadata"]["name"] for tool in tools]

================
File: tool/utils/__init__.py
================
from .openapi import OpenAPISpecParser
from .tool import ToolHelper

================
File: tool/utils/function_calling.py
================
from dapr_agents.types import (
    OAIFunctionDefinition,
    OAIToolDefinition,
    ClaudeToolDefinition,
)
from dapr_agents.types.exceptions import FunCallBuilderError
from pydantic import BaseModel, ValidationError
from typing import Dict, Any, Optional

import logging

logger = logging.getLogger(__name__)


def custom_function_schema(model: BaseModel) -> Dict:
    """
    Generates a JSON schema for the provided Pydantic model but filters out the 'title' key
    from both the main schema and from each property in the schema.

    Args:
        model (BaseModel): The Pydantic model from which to generate the JSON schema.

    Returns:
        Dict: The JSON schema of the model, excluding any 'title' keys.
    """
    schema = model.model_json_schema()
    schema.pop("title", None)

    # Remove the 'title' key from each property in the schema
    for property_details in schema.get("properties", {}).values():
        property_details.pop("title", None)

    return schema


def to_openai_function_call_definition(
    name: str,
    description: str,
    args_schema: BaseModel,
    use_deprecated: Optional[bool] = False,
) -> Dict[str, Any]:
    """
    Generates a dictionary representing either a deprecated function or a tool specification of type function
    in the OpenAI API format. It utilizes a Pydantic schema (`args_schema`) to extract parameters and types,
    which are then structured according to the OpenAI specification requirements.

    Args:
        name (str): The name of the function.
        description (str): A brief description of what the function does.
        args_schema (BaseModel): The Pydantic schema representing the function's parameters.
        use_deprecated (bool, optional): A flag to determine if the deprecated function format should be used.
                                         Defaults to False, using the tool format.

    Returns:
        Dict[str, Any]: A dictionary containing the function's specification. If 'use_deprecated' is False,
                        it includes its type as 'function' under a tool specification; otherwise, it returns
                        the function specification alone.
    """
    base_function = OAIFunctionDefinition(
        name=name,
        description=description,
        strict=True,
        parameters=custom_function_schema(args_schema),
    )

    if use_deprecated:
        # Return the function definition directly for deprecated use
        return base_function.model_dump()
    else:
        # Wrap in a tool definition for current API usage
        function_tool = OAIToolDefinition(type="function", function=base_function)
        return function_tool.model_dump()


def to_claude_function_call_definition(
    name: str, description: str, args_schema: BaseModel
) -> Dict[str, Any]:
    """
    Generates a dictionary representing a function call specification in the Claude API format. Similar to the
    OpenAI function definition, it structures the function's details such as name, description, and input parameters
    according to the Claude API specification requirements.

    Args:
        name (str): The name of the function.
        description (str): A brief description of what the function does.
        args_schema (BaseModel): The Pydantic schema representing the function's parameters.

    Returns:
        Dict[str, Any]: A dictionary containing the function's specification, including its name,
                        description, and a JSON schema of parameters formatted for Claude's API.
    """
    function_definition = ClaudeToolDefinition(
        name=name,
        description=description,
        input_schema=custom_function_schema(args_schema),
    )

    return function_definition.model_dump()


def to_gemini_function_call_definition(
    name: str, description: str, args_schema: BaseModel
) -> Dict[str, Any]:
    """
    Generates a dictionary representing a function call specification in the OpenAI API format. It utilizes
    a Pydantic schema (`args_schema`) to extract parameters and types, which are then structured according
    to the OpenAI specification requirements.

    Args:
        name (str): The name of the function.
        description (str): A brief description of what the function does.
        args_schema (BaseModel): The Pydantic schema representing the function's parameters.

    Returns:
        Dict[str, Any]: A dictionary containing the function's specification, including its name,
                        description, and a JSON schema of parameters formatted for the OpenAI API.
    """
    function_definition = OAIFunctionDefinition(
        name=name,
        description=description,
        parameters=custom_function_schema(args_schema),
    )

    return function_definition.model_dump()


def to_function_call_definition(
    name: str,
    description: str,
    args_schema: BaseModel,
    format_type: str = "openai",
    use_deprecated: bool = False,
) -> Dict:
    """
    Generates a dictionary representing a function call specification, supporting various API formats.
    The 'use_deprecated' flag is applicable only for the 'openai' format and is ignored for others.

    Args:
        name (str): The name of the function.
        description (str): A brief description of what the function does.
        args_schema (BaseModel): The Pydantic schema representing the function's parameters.
        format_type (str, optional): The API format to convert to ('openai', 'claude', or 'gemini'). Defaults to 'openai'.
        use_deprecated (bool): Flag to use the deprecated function format, only effective for 'openai'.

    Returns:
        Dict: A dictionary containing the function definition in the specified format.

    Raises:
        FunCallBuilderError: If an unsupported format type is specified.
    """
    if format_type.lower() in ("openai", "nvidia"):
        return to_openai_function_call_definition(
            name, description, args_schema, use_deprecated
        )
    elif format_type.lower() == "claude":
        if use_deprecated:
            logger.warning(
                f"'use_deprecated' flag is ignored for the '{format_type}' format."
            )
        return to_claude_function_call_definition(name, description, args_schema)
    else:
        logger.error(f"Unsupported format type: {format_type}")
        raise FunCallBuilderError(f"Unsupported format type: {format_type}")


def validate_and_format_tool(
    tool: Dict[str, Any], tool_format: str = "openai", use_deprecated: bool = False
) -> dict:
    """
    Validates and formats a tool (provided as a dictionary) based on the specified API request format.

    Args:
        tool: The tool to validate and format.
        tool_format: The API format to convert to ('openai', 'azure_openai', 'claude', 'llama').
        use_deprecated: Whether to use deprecated functions format for OpenAI. Defaults to False.

    Returns:
        dict: The formatted tool dictionary.

    Raises:
        ValueError: If the tool definition format is invalid.
        ValidationError: If the tool doesn't pass validation.
    """
    try:
        if tool_format in ["openai", "azure_openai", "nvidia"]:
            validated_tool = (
                OAIFunctionDefinition(**tool)
                if use_deprecated
                else OAIToolDefinition(**tool)
            )
        elif tool_format == "claude":
            validated_tool = ClaudeToolDefinition(**tool)
        elif tool_format == "llama":
            validated_tool = OAIFunctionDefinition(**tool)
        else:
            logger.error(f"Unsupported tool format: {tool_format}")
            raise ValueError(f"Unsupported tool format: {tool_format}")
        return validated_tool.model_dump()
    except ValidationError as e:
        logger.error(f"Validation error for {tool_format} tool definition: {e}")
        raise ValueError(f"Invalid tool definition format: {tool}")

================
File: tool/utils/openapi.py
================
from __future__ import annotations
import json
import yaml
from pathlib import Path
from typing import Union, Type, List, Dict, Callable, Any, Tuple, Optional
import requests
from pydantic import ValidationError
from openapi_pydantic import (
    OpenAPI as OpenAPI_3_1_0,
    Operation as Operation_3_1_0,
    Reference as Reference_3_1_0,
    Parameter as Parameter_3_1_0,
    Schema as Schema_3_1_0,
    RequestBody as RequestBody_3_1_0,
)

from openapi_pydantic.v3.v3_0 import (
    OpenAPI as OpenAPI_3_0,
    Operation as Operation_3_0,
    Reference as Reference_3_0,
    Parameter as Parameter_3_0,
    Schema as Schema_3_0,
    RequestBody as RequestBody_3_0,
)
from collections import defaultdict
import logging

logger = logging.getLogger(__name__)


class OpenAPISpecParser:
    """
    A class to parse and handle OpenAPI specifications with support for multiple versions.
    """

    openapi_versions = {
        "3.1.0": OpenAPI_3_1_0,
        "3.0.3": OpenAPI_3_0,
    }

    def __init__(
        self, openapi_spec: Union[OpenAPI_3_1_0, OpenAPI_3_0], openapi_version: str
    ):
        self.spec = openapi_spec
        self.openapi_version = openapi_version

    @classmethod
    def from_file(cls, file_path: Union[str, Path]) -> OpenAPISpecParser:
        """Load an OpenAPI spec from a local file."""
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"No file found at {file_path}")
        with path.open("r") as file:
            return cls.from_string(file.read())

    @classmethod
    def from_url(cls, url: str) -> OpenAPISpecParser:
        """Load an OpenAPI spec from a URL."""
        response = requests.get(url)
        response.raise_for_status()
        return cls.from_string(response.text)

    @classmethod
    def from_string(cls, data: str) -> OpenAPISpecParser:
        """Parse a string containing an OpenAPI spec in JSON or YAML format."""
        try:
            spec_dict = json.loads(data)
        except json.JSONDecodeError:
            spec_dict = yaml.safe_load(data)

        openapi_version = spec_dict.get("openapi", "")
        OpenAPI_class: Type[
            Union[OpenAPI_3_1_0, OpenAPI_3_0]
        ] = cls.openapi_versions.get(openapi_version, OpenAPI_3_0)
        if OpenAPI_class is None:
            raise ValueError(f"Unsupported OpenAPI version: {openapi_version}")

        return cls(OpenAPI_class.model_validate(spec_dict), openapi_version)

    @property
    def endpoints(self) -> List[Tuple[str, str, dict]]:
        """Generate a list of endpoints with method, path, description, and detailed spec.

        Returns:
            A list of tuples, each containing the HTTP method, path, description, and the full spec for that operation.
        """
        endpoints = []
        for path, path_item in self.spec.paths.items():
            for method in [
                "get",
                "post",
                "put",
                "delete",
                "patch",
                "options",
                "head",
                "trace",
            ]:
                operation = getattr(path_item, method, None)
                if operation:
                    _ = getattr(operation, "operationId", f"{method.upper()} {path}")
                    description = getattr(
                        operation, "description", "No description provided."
                    )
                    # Collect full operation details or specific parts of the operation as needed
                    endpoints.append(
                        (f"{method.upper()} {path}", description, operation)
                    )
        return endpoints

    def validate_spec(self):
        """
        Validate the entire OpenAPI specification. Raises ValidationError on issues.
        """
        try:
            self.spec.model_validate(self.spec.model_dump())
            print("Specification is valid.")
        except ValidationError as e:
            print("Specification validation failed:", e)

    def get_operation(
        self, path: str, method: str
    ) -> Union[Operation_3_1_0, Operation_3_0]:
        """
        Get a specific operation for a given path and HTTP method.
        """
        path_item = self.spec.paths.get(path)
        if not path_item:
            raise ValueError(f"Path '{path}' not found in the specification.")
        operation = getattr(path_item, method, None)
        if isinstance(operation, Operation_3_1_0) or isinstance(
            operation, Operation_3_0
        ):
            return operation

    def get_parameters_for_operation(
        self, operation: Union[Operation_3_1_0, Operation_3_0]
    ) -> List[Union[Parameter_3_1_0, Parameter_3_0, dict]]:
        """
        Retrieve all parameters for a given operation, resolving references if necessary.
        """
        parameters = []
        if hasattr(operation, "parameters") and operation.parameters:
            for param in operation.parameters:
                if isinstance(param, Reference_3_1_0) or isinstance(
                    param, Reference_3_0
                ):
                    resolved_param = self.resolve_reference(param)
                    if resolved_param:
                        parameters.append(resolved_param)
                else:
                    parameters.append(param)
        return parameters

    def get_request_body_for_operation(
        self, operation: Union[Operation_3_1_0, Operation_3_0]
    ) -> Optional[Union[RequestBody_3_1_0, RequestBody_3_0, Dict]]:
        """
        Retrieve a requestBody for a given operation, resolving references if necessary.
        """
        if hasattr(operation, "requestBody") and operation.requestBody:
            request_body = operation.requestBody
            if isinstance(request_body, Reference_3_1_0) or isinstance(
                request_body, Reference_3_0
            ):
                resolved_request_body = self.resolve_reference(request_body)
                if resolved_request_body:
                    return resolved_request_body
            else:
                return request_body

    def get_methods_for_path(self, path: str) -> List[str]:
        """
        Retrieve all HTTP methods available for a specified path.
        Ensures that the methods are present in the path item.
        """
        path_item = self.spec.paths.get(path)
        if not path_item:
            raise ValueError(f"Path '{path}' not found in the specification.")

        results = []
        # List of possible HTTP methods that could be present in an OpenAPI path item
        possible_methods = [
            "get",
            "put",
            "post",
            "delete",
            "options",
            "head",
            "patch",
            "trace",
        ]
        for method in possible_methods:
            operation = getattr(path_item, method, None)
            if operation is not None:
                results.append(method)
        return results

    def get_parameters_for_path(
        self, path: str
    ) -> List[Union[Parameter_3_1_0, Parameter_3_0, dict]]:
        """
        Retrieve all parameters associated with a given path.
        """
        path_item = self.spec.paths.get(path)
        if not path_item:
            raise ValueError(f"Path '{path}' not found in the specification.")

        # Aggregate parameters from all operations in the path item
        parameters = []
        for operation in [
            getattr(path_item, method) for method in self.get_methods_for_path(path)
        ]:
            if hasattr(operation, "parameters") and operation.parameters:
                for param in operation.parameters:
                    if isinstance(param, Reference_3_1_0) or isinstance(
                        param, Reference_3_0
                    ):
                        resolved_param = self.resolve_reference(param)
                        if resolved_param:
                            parameters.append(resolved_param)
                    else:
                        parameters.append(param)
        return parameters

    def resolve_reference(
        self, ref: Union[Reference_3_1_0, Reference_3_0]
    ) -> Union[Parameter_3_1_0, Parameter_3_0, Schema_3_1_0, Schema_3_0]:
        """
        Resolve a reference to a Parameter or Schema within the spec components.
        """
        parts = ref.ref.split("/")
        if parts[0] != "#":
            raise ValueError("Currently only local references are supported.")
        try:
            return getattr(self.spec.components, parts[-2])[parts[-1]]
        except KeyError:
            raise ValueError(f"Reference not found: {ref.ref}")

    def get_schema(
        self,
        ref_or_schema: Union[
            Reference_3_1_0, Reference_3_0, Schema_3_1_0, Schema_3_0, dict
        ],
    ) -> str:
        """
        Retrieve and return a schema from a Reference or a direct schema dictionary.
        """
        if isinstance(ref_or_schema, Reference_3_1_0) or isinstance(
            ref_or_schema, Reference_3_0
        ):
            return json.loads(
                self.resolve_reference(ref_or_schema).model_dump_json(exclude_none=True)
            )
        elif isinstance(ref_or_schema, dict):
            if self.openapi_version == "3.1.0":
                return json.loads(
                    Schema_3_1_0(**ref_or_schema).model_dump_json(exclude_none=True)
                )
            else:
                return json.loads(
                    Schema_3_0(**ref_or_schema).model_dump_json(exclude_none=True)
                )
        elif isinstance(ref_or_schema, Schema_3_1_0) or isinstance(
            ref_or_schema, Schema_3_0
        ):
            return json.loads(ref_or_schema.model_dump_json(exclude_none=True))
        else:
            raise TypeError("Invalid type for schema retrieval.")

    def get_endpoint_definitions(self) -> List[Dict[str, Any]]:
        """
        Generate a list of endpoint definitions with documents and metadata.

        Each document includes the API name, HTTP method, path, description, and summary.
        Metadata includes the API name and tags.

        Returns:
            List[Dict[str, Any]]: A list of dictionaries containing documents and metadata for each endpoint.
        """
        endpoint_definitions = []
        for path, description, details in self.endpoints:
            method, _ = path.split()
            api_name = details.get("operationId", "")
            summary = details.get("summary", "")

            # If description is not provided, use summary, if summary is not provided, use empty string
            if not description:
                description = summary if summary else ""

            # Construct the document
            document = (
                (
                    f"API Name: {api_name}. "
                    f"Method: {method}. "
                    f"Path: {path}. "
                    f"Summary: {summary}. "
                    f"Description: {description}"
                )
                .replace("\n", " ")
                .strip()
            )

            # Construct metadata
            tags = ", ".join(details.get("tags", []))
            metadata = {
                "endpoint_path": path,
                "description": description,
                "api_name": api_name,
                "tags": tags,
            }
            endpoint_definitions.append({"document": document, "metadata": metadata})

        return endpoint_definitions


def openapi_params_to_json_schema(
    params: List[Union[Parameter_3_1_0, Parameter_3_0, dict]], spec: OpenAPISpecParser
) -> dict:
    properties = {}
    required = []
    for p in params:
        schema_dict = None
        if hasattr(p, "param_schema") and p.param_schema:
            schema_dict = spec.get_schema(p.param_schema)
        elif hasattr(p, "content") and p.content:
            media_type_schema = list(p.content.values())[
                0
            ]  # Accessing first media type schema.
            schema_dict = spec.get_schema(media_type_schema)

        if schema_dict:
            if (
                p.description
            ):  # Optionally add description if it's not already in the schema
                schema_dict["description"] = (
                    p.description
                    if "description" not in schema_dict
                    else schema_dict["description"]
                )
            properties[p.name] = schema_dict

        if p.required:
            required.append(p.name)

    return {"type": "object", "properties": properties, "required": required}


def openapi_spec_to_openai_fn(
    spec_parser: OpenAPISpecParser,
) -> Tuple[List[Dict[str, Any]], Callable]:
    """Convert a valid OpenAPI spec to the JSON Schema format expected for OpenAI functions.
        Reference: https://github.com/langchain-ai/langchain/blob/fd546196ef0fafa4a4cd7bb7ebb1771ef599f372/libs/langchain/langchain/chains/openai_functions/openapi.py#L90

    Args:
        spec: OpenAPI spec to convert.

    Returns:
        Tuple of the OpenAI functions JSON schema and a default function for executing
            a request based on the OpenAI function schema.
    """
    if not spec_parser.spec.paths:
        return [], lambda: None

    functions = []

    for path in spec_parser.spec.paths:
        path_params = {
            (p.name, p.param_in): p for p in spec_parser.get_parameters_for_path(path)
        }
        for method in spec_parser.get_methods_for_path(path):
            request_args = {}
            op = spec_parser.get_operation(path, method)
            op_params = path_params.copy()
            for param in spec_parser.get_parameters_for_operation(op):
                op_params[(param.name, param.param_in)] = param
            params_by_type = defaultdict(list)
            for name_loc, p in op_params.items():
                params_by_type[name_loc[1]].append(p)
            param_loc_to_arg_name = {
                "query": "params",
                "header": "headers",
                "cookie": "cookies",
                "path": "path_params",
            }
            for param_loc, arg_name in param_loc_to_arg_name.items():
                if params_by_type[param_loc]:
                    request_args[arg_name] = openapi_params_to_json_schema(
                        params_by_type[param_loc], spec_parser
                    )
            request_body = spec_parser.get_request_body_for_operation(op)
            # TODO: Support more MIME types.
            if request_body and request_body.content:
                media_types = {}
                for media_type, media_type_object in request_body.content.items():
                    if media_type_object.media_type_schema:
                        schema = spec_parser.get_schema(
                            media_type_object.media_type_schema
                        )
                        media_types[media_type] = schema
                if len(media_types) == 1:
                    media_type, schema_dict = list(media_types.items())[0]
                    key = "json" if media_type == "application/json" else "data"
                    request_args[key] = schema_dict
                elif len(media_types) > 1:
                    request_args["data"] = {"anyOf": list(media_types.values())}

            # Add method and url as part of the parameters in the function metadata
            func_name = getattr(op, "operationId", f"{path}_{method}")
            functions.append(
                {
                    "definition": {
                        "type": "function",
                        "function": {
                            "name": func_name,
                            "description": op.description,
                            "parameters": {
                                "type": "object",
                                "properties": request_args,
                            },
                        },
                    },
                    "metadata": {
                        "method": method,
                        "url": spec_parser.spec.servers[0].url + path,
                    },
                }
            )
    return functions

================
File: tool/utils/tool.py
================
from dapr_agents.tool.utils.function_calling import validate_and_format_tool
from typing import Any, Union, Dict, Callable, Optional, Type
from inspect import signature, Parameter
from pydantic import BaseModel, create_model, Field
from dapr_agents.types import ToolError
import logging

logger = logging.getLogger(__name__)


class ToolHelper:
    """
    Utility class for common operations related to agent tools, such as validating docstrings,
    formatting tools for specific APIs, and inferring Pydantic schemas from function signatures.
    """

    @staticmethod
    def check_docstring(func: Callable) -> None:
        """
        Ensures a function has a docstring, raising an error if missing.

        Args:
            func (Callable): The function to verify.

        Raises:
            ToolError: Raised if the function lacks a docstring.
        """
        if not func.__doc__:
            raise ToolError(
                f"Function '{func.__name__}' must have a docstring for documentation."
            )

    @staticmethod
    def format_tool(
        tool: Union[Dict[str, Any], Callable],
        tool_format: str = "openai",
        use_deprecated: bool = False,
    ) -> dict:
        """
        Validates and formats a tool for a specific API format.

        Args:
            tool (Union[Dict[str, Any], Callable]): The tool to format.
            tool_format (str): Format type, e.g., 'openai'.
            use_deprecated (bool): Set to use a deprecated format.

        Returns:
            dict: A formatted representation of the tool.
        """
        from dapr_agents.tool.base import AgentTool

        if callable(tool) and not isinstance(tool, AgentTool):
            tool = AgentTool.from_func(tool)
        elif isinstance(tool, dict):
            return validate_and_format_tool(tool, tool_format, use_deprecated)
        if not isinstance(tool, AgentTool):
            raise TypeError(f"Unsupported tool type: {type(tool).__name__}")
        return tool.to_function_call(
            format_type=tool_format, use_deprecated=use_deprecated
        )

    @staticmethod
    def infer_func_schema(
        func: Callable, name: Optional[str] = None
    ) -> Type[BaseModel]:
        """
        Generates a Pydantic schema based on the function’s signature and type hints.

        Args:
            func (Callable): The function from which to derive the schema.
            name (Optional[str]): An optional name for the generated Pydantic model.

        Returns:
            Type[BaseModel]: A Pydantic model representing the function’s parameters.
        """
        sig = signature(func)
        fields = {}
        has_type_hints = False

        for name, param in sig.parameters.items():
            field_type = (
                param.annotation if param.annotation != Parameter.empty else str
            )
            has_type_hints = has_type_hints or param.annotation != Parameter.empty
            fields[name] = (
                field_type,
                Field(default=param.default)
                if param.default != Parameter.empty
                else Field(...),
            )

        model_name = name or f"{func.__name__}Model"
        if not has_type_hints:
            logger.warning(
                f"No type hints provided for function '{func.__name__}'. Defaulting to 'str'."
            )
        return (
            create_model(model_name, **fields)
            if fields
            else create_model(model_name, __base__=BaseModel)
        )

================
File: tool/__init__.py
================
from .base import AgentTool, tool
from .executor import AgentToolExecutor

================
File: tool/base.py
================
import inspect
import logging
from typing import Callable, Type, Optional, Any, Dict
from inspect import signature, Parameter
from pydantic import BaseModel, Field, ValidationError, model_validator, PrivateAttr

from dapr_agents.tool.utils.tool import ToolHelper
from dapr_agents.tool.utils.function_calling import to_function_call_definition
from dapr_agents.types import ToolError

logger = logging.getLogger(__name__)


class AgentTool(BaseModel):
    """
    Base class for agent tools, supporting both synchronous and asynchronous execution.

    Attributes:
        name (str): The tool's name.
        description (str): A brief description of the tool's purpose.
        args_model (Optional[Type[BaseModel]]): Model for validating tool arguments.
        func (Optional[Callable]): Function defining tool behavior.
    """

    name: str = Field(
        ...,
        description="The name of the tool, formatted with capitalization and no spaces.",
    )
    description: str = Field(
        ..., description="A brief description of the tool's functionality."
    )
    args_model: Optional[Type[BaseModel]] = Field(
        None, description="Pydantic model for validating tool arguments."
    )
    func: Optional[Callable] = Field(
        None, description="Optional function implementing the tool's behavior."
    )

    _is_async: bool = PrivateAttr(default=False)

    @model_validator(mode="before")
    @classmethod
    def set_name_and_description(cls, values: dict) -> dict:
        """
        Validator to dynamically set `name` and `description` before validation.
        """
        func = values.get("func")
        if func:
            values.setdefault("name", func.__name__)
            values.setdefault("description", func.__doc__ or "")
        return values

    @classmethod
    def from_func(cls, func: Callable) -> "AgentTool":
        """
        Creates an instance of `AgentTool` from a raw Python function.

        Args:
            func (Callable): The function to wrap in the tool.

        Returns:
            AgentTool: An instance of `AgentTool`.
        """
        ToolHelper.check_docstring(func)
        return cls(func=func)

    def model_post_init(self, __context: Any) -> None:
        """
        Handles post-initialization logic for both class-based and function-based tools.
        Ensures `name` formatting and infers `args_model` if necessary.
        """
        self.name = self.name.replace(" ", "_").title().replace("_", "")

        if self.func:
            self._is_async = inspect.iscoroutinefunction(self.func)
            self._initialize_from_func(self.func)
        else:
            self._initialize_from_run()
        return super().model_post_init(__context)

    def _initialize_from_func(self, func: Callable) -> None:
        """Initialize Tool fields from a provided function."""
        if self.args_model is None:
            self.args_model = ToolHelper.infer_func_schema(func)

    def _initialize_from_run(self) -> None:
        """Initialize Tool fields based on the abstract `_run` method."""
        if self.args_model is None:
            self.args_model = ToolHelper.infer_func_schema(self._run)

    def _validate_and_prepare_args(
        self, func: Callable, *args, **kwargs
    ) -> Dict[str, Any]:
        """
        Normalize and validate arguments for the given function.

        Args:
            func (Callable): The function whose signature is used.
            *args: Positional arguments.
            **kwargs: Keyword arguments.

        Returns:
            Dict[str, Any]: Validated and prepared arguments.

        Raises:
            ToolError: If argument validation fails.
        """
        sig = signature(func)
        if args:
            arg_names = list(sig.parameters.keys())
            kwargs.update(dict(zip(arg_names, args)))

        if self.args_model:
            try:
                validated_args = self.args_model(**kwargs)
                return validated_args.model_dump()
            except ValidationError as ve:
                logger.debug(f"Validation failed for tool '{self.name}': {ve}")
                raise ToolError(f"Validation error in tool '{self.name}': {ve}") from ve

        return kwargs

    def run(self, *args, **kwargs) -> Any:
        """
        Execute the tool synchronously.

        Raises:
            ToolError if the tool is async or execution fails.
        """
        if self._is_async:
            raise ToolError(
                f"Tool '{self.name}' is async and must be awaited. Use `await tool.arun(...)` instead."
            )
        try:
            func = self.func or self._run
            kwargs = self._validate_and_prepare_args(func, *args, **kwargs)
            return func(**kwargs)
        except Exception as e:
            self._log_and_raise_error(e)

    async def arun(self, *args, **kwargs) -> Any:
        """
        Execute the tool asynchronously (whether it's sync or async under the hood).
        """
        try:
            func = self.func or self._run
            kwargs = self._validate_and_prepare_args(func, *args, **kwargs)
            return await func(**kwargs) if self._is_async else func(**kwargs)
        except Exception as e:
            self._log_and_raise_error(e)

    def _run(self, *args, **kwargs) -> Any:
        """Fallback default run logic if no `func` is set."""
        if self.func:
            return self.func(*args, **kwargs)
        raise NotImplementedError("No function or _run method defined for this tool.")

    def _log_and_raise_error(self, error: Exception) -> None:
        """Log the error and raise a ToolError."""
        logger.error(f"Error executing tool '{self.name}': {str(error)}")
        raise ToolError(
            f"An error occurred during the execution of tool '{self.name}': {str(error)}"
        )

    def __call__(self, *args, **kwargs) -> Any:
        """
        Enables `tool(...)` syntax.

        Raises:
            ToolError: if async tool is called without `await`.
        """
        if self._is_async:
            raise ToolError(
                f"Tool '{self.name}' is async and must be awaited. Use `await tool.arun(...)`."
            )
        return self.run(*args, **kwargs)

    def to_function_call(
        self, format_type: str = "openai", use_deprecated: bool = False
    ) -> Dict:
        """
        Converts the tool to a specified function call format.

        Args:
            format_type (str): The format type (e.g., 'openai').
            use_deprecated (bool): Whether to use deprecated format.

        Returns:
            Dict: The function call representation.
        """
        return to_function_call_definition(
            self.name, self.description, self.args_model, format_type, use_deprecated
        )

    def __repr__(self) -> str:
        """Returns a string representation of the AgentTool."""
        return f"AgentTool(name={self.name}, description={self.description})"

    @property
    def args_schema(self) -> dict:
        """Returns a JSON-serializable dictionary of the tool's function args_model."""
        if self.args_model:
            schema = self.args_model.model_json_schema()
            for prop in schema.get("properties", {}).values():
                prop.pop("title", None)
            return schema.get("properties", {})
        return {}

    @property
    def signature(self) -> str:
        """Provides a dynamic and detailed string representation of the tool's function signature."""
        func_to_inspect = self.func if self.func else self._run
        params = signature(func_to_inspect).parameters
        args = [
            f"{name}: {param.annotation.__name__ if param.annotation != Parameter.empty else 'Any'}"
            f"{' = ' + repr(param.default) if param.default != Parameter.empty else ''}"
            for name, param in params.items()
        ]
        return f"{self.name}({', '.join(args)})"


def tool(
    func: Optional[Callable] = None, *, args_model: Optional[Type[BaseModel]] = None
) -> AgentTool:
    """
    A decorator to wrap a function with an `AgentTool` for validation and metadata.

    Args:
        func (Optional[Callable]): The function to wrap.
        args_model (Optional[Type[BaseModel]]): Optional Pydantic model for argument validation.

    Returns:
        AgentTool: The wrapped function as an `AgentTool`.
    """

    def decorator(f: Callable) -> AgentTool:
        ToolHelper.check_docstring(f)
        return AgentTool(func=f, args_model=args_model)

    return decorator(func) if func else decorator

================
File: tool/executor.py
================
import logging
from typing import Any, Dict, List, Optional
from pydantic import BaseModel, Field, PrivateAttr
from rich.table import Table
from rich.console import Console

from dapr_agents.tool import AgentTool
from dapr_agents.types import AgentToolExecutorError, ToolError

logger = logging.getLogger(__name__)


class AgentToolExecutor(BaseModel):
    """
    Manages the registration and execution of tools, providing both sync and async interfaces.

    Attributes:
        tools (List[AgentTool]): List of tools to register and manage.
    """

    tools: List[AgentTool] = Field(
        default_factory=list, description="List of tools to register and manage."
    )
    _tools_map: Dict[str, AgentTool] = PrivateAttr(default_factory=dict)

    def model_post_init(self, __context: Any) -> None:
        """Initializes the internal tools map after model creation."""
        for tool in self.tools:
            self.register_tool(tool)
        logger.info(f"Tool Executor initialized with {len(self._tools_map)} tool(s).")
        super().model_post_init(__context)

    def register_tool(self, tool: AgentTool) -> None:
        """
        Registers a tool instance, ensuring no duplicate names.

        Args:
            tool (AgentTool): The tool to register.

        Raises:
            AgentToolExecutorError: If the tool name is already registered.
        """
        if tool.name in self._tools_map:
            logger.error(f"Attempted to register duplicate tool: {tool.name}")
            raise AgentToolExecutorError(f"Tool '{tool.name}' is already registered.")
        self._tools_map[tool.name] = tool
        logger.info(f"Tool registered: {tool.name}")

    def get_tool(self, tool_name: str) -> Optional[AgentTool]:
        """
        Retrieves a tool by name.

        Args:
            tool_name (str): Name of the tool to retrieve.

        Returns:
            AgentTool or None if not found.
        """
        return self._tools_map.get(tool_name)

    def get_tool_names(self) -> List[str]:
        """
        Lists all registered tool names.

        Returns:
            List[str]: Names of all registered tools.
        """
        return list(self._tools_map.keys())

    def get_tool_signatures(self) -> str:
        """
        Retrieves the signatures of all registered tools.

        Returns:
            str: Tool signatures, each on a new line.
        """
        return "\n".join(tool.signature for tool in self._tools_map.values())

    def get_tool_details(self) -> str:
        """
        Retrieves names, descriptions, and argument schemas of all tools.

        Returns:
            str: Detailed tool information, each on a new line.
        """
        return "\n".join(
            f"{tool.name}: {tool.description}. Args schema: {tool.args_schema}"
            for tool in self._tools_map.values()
        )

    async def run_tool(self, tool_name: str, *args, **kwargs) -> Any:
        """
        Executes a tool by name, automatically handling both sync and async tools.

        Args:
            tool_name (str): Tool name to execute.
            *args: Positional arguments.
            **kwargs: Keyword arguments.

        Returns:
            Any: Result of tool execution.

        Raises:
            AgentToolExecutorError: If the tool is not found or execution fails.
        """
        tool = self.get_tool(tool_name)
        if not tool:
            logger.error(f"Tool not found: {tool_name}")
            raise AgentToolExecutorError(f"Tool '{tool_name}' not found.")
        try:
            logger.info(f"Running tool (auto): {tool_name}")
            if tool._is_async:
                return await tool.arun(*args, **kwargs)
            return tool(*args, **kwargs)
        except ToolError as e:
            logger.error(f"Tool execution error in '{tool_name}': {e}")
            raise AgentToolExecutorError(str(e)) from e
        except Exception as e:
            logger.error(f"Unexpected error in '{tool_name}': {e}")
            raise AgentToolExecutorError(
                f"Unexpected error in tool '{tool_name}': {e}"
            ) from e

    @property
    def help(self) -> None:
        """Displays a rich-formatted table of registered tools."""
        table = Table(title="Available Tools")
        table.add_column("Name", style="bold cyan")
        table.add_column("Description")
        table.add_column("Signature")

        for name, tool in self._tools_map.items():
            table.add_row(name, tool.description, tool.signature)

        console = Console()
        console.print(table)

================
File: types/__init__.py
================
from .tools import OAIFunctionDefinition, OAIToolDefinition, ClaudeToolDefinition
from .message import (
    BaseMessage,
    MessageContent,
    ChatCompletion,
    SystemMessage,
    UserMessage,
    AssistantMessage,
    AssistantFinalMessage,
    ToolMessage,
    ToolCall,
    FunctionCall,
    MessagePlaceHolder,
    EventMessageMetadata,
)
from .llm import OpenAIChatCompletionParams, OpenAIModelConfig
from .exceptions import (
    ToolError,
    AgentError,
    AgentToolExecutorError,
    StructureError,
    FunCallBuilderError,
)
from .graph import Node, Relationship
from .schemas import OAIJSONSchema, OAIResponseFormatSchema

================
File: types/agent.py
================
from pydantic import BaseModel, Field
from typing import List, Optional
from datetime import datetime
from enum import Enum
import uuid


class AgentStatus(str, Enum):
    """Enumeration of possible agent statuses for standardized tracking."""

    ACTIVE = "active"  # The agent is actively working on tasks
    IDLE = "idle"  # The agent is idle and waiting for tasks
    PAUSED = "paused"  # The agent is temporarily paused
    COMPLETE = "complete"  # The agent has completed all assigned tasks
    ERROR = "error"  # The agent encountered an error and needs attention


class AgentActorMessage(BaseModel):
    """Represents an individual message exchanged by the agent."""

    id: str = Field(
        default_factory=lambda: str(uuid.uuid4()),
        description="Unique identifier for the message",
    )
    role: str = Field(
        ..., description="The role of the message sender, e.g., 'user' or 'assistant'"
    )
    content: str = Field(..., description="The content of the message")
    timestamp: datetime = Field(
        default_factory=datetime.now,
        description="Timestamp of when the message was created",
    )
    name: Optional[str] = Field(
        default=None,
        description="Optional name of the assistant or user sending the message",
    )


class AgentTaskStatus(str, Enum):
    """Enumeration of possible task statuses for standardizing task tracking."""

    IN_PROGRESS = "in-progress"  # Task is currently in progress
    COMPLETE = "complete"  # Task has been completed successfully
    FAILED = "failed"  # Task has failed to complete as expected
    PENDING = "pending"  # Task is awaiting to be started
    CANCELED = "canceled"  # Task was canceled and will not be completed


class AgentTaskEntry(BaseModel):
    """Represents a task handled by the agent, including its input, output, and status."""

    task_id: str = Field(
        default_factory=lambda: str(uuid.uuid4()),
        description="Unique identifier for the task",
    )
    input: str = Field(
        ..., description="The input or description of the task to be performed"
    )
    output: Optional[str] = Field(
        None, description="The output or result of the task, if completed"
    )
    status: AgentTaskStatus = Field(..., description="Current status of the task")
    timestamp: datetime = Field(
        default_factory=datetime.now,
        description="Timestamp of task initiation or update",
    )


class AgentActorState(BaseModel):
    """Represents the state of an agent, tracking message history, task history, and overall status."""

    messages: Optional[List[AgentActorMessage]] = Field(
        default_factory=list, description="History of messages exchanged by the agent"
    )
    message_count: int = Field(
        0, description="Total number of messages exchanged by the agent"
    )
    task_history: Optional[List[AgentTaskEntry]] = Field(
        default_factory=list, description="History of tasks the agent has performed"
    )
    overall_status: AgentStatus = Field(
        AgentStatus.IDLE, description="Current operational status of the agent"
    )

================
File: types/document.py
================
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any


class Document(BaseModel):
    """
    Represents a document with text content and associated metadata.
    """

    metadata: Optional[Dict[str, Any]] = Field(
        default=None,
        description="A dictionary containing metadata about the document (e.g., source, page number).",
    )
    text: str = Field(..., description="The main content of the document.")

================
File: types/exceptions.py
================
class AgentToolExecutorError(Exception):
    """Custom exception for AgentToolExecutor specific errors."""


class AgentError(Exception):
    """Custom exception for Agent specific errors, used to handle errors specific to agent operations."""


class ToolError(Exception):
    """Custom exception for tool-related errors."""


class StructureError(Exception):
    """Custom exception for errors related to structured handling."""


class FunCallBuilderError(Exception):
    """Custom exception for errors related to structured handling."""

================
File: types/executor.py
================
from typing import List
from pydantic import BaseModel, Field


class ExecutionResult(BaseModel):
    """Stores the outcome of a code execution."""

    status: str = Field(
        ..., description="The execution status, either 'success' or 'error'."
    )
    output: str = Field(
        ...,
        description="The standard output or error message resulting from execution.",
    )
    exit_code: int = Field(
        ...,
        description="The exit code returned by the executed process (0 indicates success, non-zero indicates an error).",
    )


class CodeSnippet(BaseModel):
    """Represents a block of code extracted for execution."""

    language: str = Field(
        ...,
        description="The programming language of the code snippet (e.g., 'python', 'javascript').",
    )
    code: str = Field(..., description="The actual source code to be executed.")
    timeout: int = Field(
        5,
        description="Per-snippet timeout (seconds). Executor falls back to the request-level timeout if omitted.",
    )


class ExecutionRequest(BaseModel):
    """Represents a request to execute a code snippet."""

    snippets: List[CodeSnippet] = Field(
        ...,
        description="A list of code snippets to be executed sequentially or in parallel.",
    )
    timeout: int = Field(
        5,
        description="The maximum time (in seconds) allowed for execution before timing out (default is 5 seconds).",
    )

================
File: types/graph.py
================
from pydantic import BaseModel, Field, ConfigDict
from typing import Any, Dict, Optional, List


class Node(BaseModel):
    id: Any = Field(description="The unique identifier of the node.")
    label: str = Field(description="The primary label or type of the node.")
    properties: Dict[str, Any] = Field(
        description="A dictionary of properties associated with the node."
    )
    additional_labels: Optional[List[str]] = Field(
        default=[],
        description="Additional labels or categories associated with the node.",
    )
    embedding: Optional[List[float]] = Field(
        default=None,
        description="Optional embedding vector for the node, useful for vector-based similarity searches.",
    )

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "id": "1",
                "label": "Person",
                "properties": {"name": "Alice", "age": 30},
                "additional_labels": ["Employee"],
                "embedding": [0.1, 0.2, 0.3],
            }
        }
    )


class Relationship(BaseModel):
    source_node_id: Any = Field(
        description="The unique identifier of the source node in the relationship."
    )
    target_node_id: Any = Field(
        description="The unique identifier of the target node in the relationship."
    )
    type: str = Field(
        description="The type or label of the relationship, describing the connection between nodes."
    )
    properties: Optional[Dict[str, Any]] = Field(
        default={}, description="Optional properties associated with the relationship."
    )

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "source_node_id": "1",
                "target_node_id": "2",
                "type": "FRIEND",
                "properties": {"since": 2020},
            }
        }
    )

================
File: types/llm.py
================
from typing import List, Union, Optional, Dict, Any, Literal, IO, Tuple
from pydantic import BaseModel, Field, model_validator, field_validator, ConfigDict
from pydantic_core import PydanticUseDefault
from pathlib import Path
from io import BytesIO, BufferedReader


class ElevenLabsClientConfig(BaseModel):
    base_url: Literal[
        "https://api.elevenlabs.io", "https://api.us.elevenlabs.io"
    ] = Field(
        default="https://api.elevenlabs.io",
        description="Base URL for the ElevenLabs API. Defaults to the production environment.",
    )
    api_key: Optional[str] = Field(
        None, description="API key to authenticate with the ElevenLabs API."
    )

    @field_validator("*", mode="before")
    @classmethod
    def none_to_default(cls, v):
        if v is None:
            raise PydanticUseDefault()
        return v


class NVIDIAClientConfig(BaseModel):
    base_url: Optional[str] = Field(
        "https://integrate.api.nvidia.com/v1", description="Base URL for the NVIDIA API"
    )
    api_key: Optional[str] = Field(
        None, description="API key to authenticate the NVIDIA API"
    )

    @field_validator("*", mode="before")
    @classmethod
    def none_to_default(cls, v):
        if v is None:
            raise PydanticUseDefault()
        return v


class DaprInferenceClientConfig:
    @field_validator("*", mode="before")
    @classmethod
    def none_to_default(cls, v):
        if v is None:
            raise PydanticUseDefault()
        return v


class HFInferenceClientConfig(BaseModel):
    model: Optional[str] = Field(
        None,
        description="Model ID on Hugging Face Hub or URL to a deployed Inference Endpoint. Defaults to a recommended model if not provided.",
    )
    api_key: Optional[Union[str, bool]] = Field(
        None,
        description="Hugging Face API key for authentication. Defaults to the locally saved token. Pass False to skip token.",
    )
    token: Optional[Union[str, bool]] = Field(
        None,
        description="Alias for api_key. Defaults to the locally saved token. Pass False to avoid sending the token.",
    )
    base_url: Optional[str] = Field(
        None,
        description="Base URL to run inference. Cannot be used if model is set. Defaults to None.",
    )
    timeout: Optional[float] = Field(
        None,
        description="Maximum time in seconds to wait for a server response. Defaults to None, meaning it will wait indefinitely.",
    )
    headers: Optional[Dict[str, str]] = Field(
        None,
        description="Additional headers to send to the server. Overrides default headers such as authorization and user-agent.",
    )
    cookies: Optional[Dict[str, str]] = Field(
        None, description="Additional cookies to send with the request."
    )
    proxies: Optional[Any] = Field(None, description="Proxies to use for the request.")

    @field_validator("*", mode="before")
    @classmethod
    def none_to_default(cls, v):
        if v is None:
            raise PydanticUseDefault()
        return v


class OpenAIClientConfig(BaseModel):
    base_url: Optional[str] = Field(None, description="Base URL for the OpenAI API")
    api_key: Optional[str] = Field(
        None, description="API key to authenticate the OpenAI API"
    )
    organization: Optional[str] = Field(
        None, description="Organization name for OpenAI"
    )
    project: Optional[str] = Field(None, description="OpenAI project name.")

    @field_validator("*", mode="before")
    @classmethod
    def none_to_default(cls, v):
        if v is None:
            raise PydanticUseDefault()
        return v


class AzureOpenAIClientConfig(BaseModel):
    api_key: Optional[str] = Field(
        None, description="API key to authenticate the Azure OpenAI API"
    )
    azure_ad_token: Optional[str] = Field(
        None, description="Azure Active Directory token for authentication"
    )
    organization: Optional[str] = Field(
        None, description="Azure organization associated with the OpenAI resource"
    )
    project: Optional[str] = Field(
        None, description="Azure project associated with the OpenAI resource"
    )
    api_version: Optional[str] = Field(
        "2024-07-01-preview", description="API version for Azure OpenAI models"
    )
    azure_endpoint: Optional[str] = Field(
        None, description="Azure endpoint for Azure OpenAI models"
    )
    azure_deployment: Optional[str] = Field(
        default=None, description="Azure deployment for Azure OpenAI models"
    )
    azure_client_id: Optional[str] = Field(
        default=None, description="Client ID for Managed Identity authentication."
    )

    @field_validator("*", mode="before")
    @classmethod
    def none_to_default(cls, v):
        if v is None:
            raise PydanticUseDefault()
        return v


class OpenAIModelConfig(OpenAIClientConfig):
    type: Literal["openai"] = Field(
        "openai", description="Type of the model, must always be 'openai'"
    )
    name: str = Field(default=None, description="Name of the OpenAI model")


class AzureOpenAIModelConfig(AzureOpenAIClientConfig):
    type: Literal["azure_openai"] = Field(
        "azure_openai", description="Type of the model, must always be 'azure_openai'"
    )


class HFHubModelConfig(HFInferenceClientConfig):
    type: Literal["huggingface"] = Field(
        "huggingface", description="Type of the model, must always be 'huggingface'"
    )
    name: str = Field(
        default=None, description="Name of the model available through Hugging Face"
    )


class NVIDIAModelConfig(NVIDIAClientConfig):
    type: Literal["nvidia"] = Field(
        "nvidia", description="Type of the model, must always be 'nvidia'"
    )
    name: str = Field(
        default=None, description="Name of the model available through NVIDIA"
    )


class OpenAIParamsBase(BaseModel):
    """
    Common request settings for OpenAI services.
    """

    model: Optional[str] = Field(None, description="ID of the model to use")
    temperature: Optional[float] = Field(
        0, ge=0.0, le=2.0, description="Sampling temperature"
    )
    max_tokens: Optional[int] = Field(
        None,
        description="Maximum number of tokens to generate. Can be None or a positive integer.",
    )
    top_p: Optional[float] = Field(
        1.0, ge=0.0, le=1.0, description="Nucleus sampling probability mass"
    )
    frequency_penalty: Optional[float] = Field(
        0.0, ge=-2.0, le=2.0, description="Frequency penalty"
    )
    presence_penalty: Optional[float] = Field(
        0.0, ge=-2.0, le=2.0, description="Presence penalty"
    )
    stop: Optional[Union[str, List[str]]] = Field(None, description="Stop sequences")
    stream: Optional[bool] = Field(False, description="Whether to stream responses")

    @field_validator("*", mode="before")
    @classmethod
    def none_to_default(cls, v):
        if v is None:
            raise PydanticUseDefault()
        return v


class OpenAITextCompletionParams(OpenAIParamsBase):
    """
    Specific configs for the text completions endpoint.
    """

    best_of: Optional[int] = Field(
        None, ge=1, description="Number of best completions to generate"
    )
    echo: Optional[bool] = Field(False, description="Whether to echo the prompt")
    logprobs: Optional[int] = Field(
        None, ge=0, le=5, description="Include log probabilities"
    )
    suffix: Optional[str] = Field(None, description="Suffix to append to the prompt")

    @field_validator("*", mode="before")
    @classmethod
    def none_to_default(cls, v):
        if v is None:
            raise PydanticUseDefault()
        return v


class OpenAIChatCompletionParams(OpenAIParamsBase):
    """
    Specific settings for the Chat Completion endpoint.
    """

    logit_bias: Optional[Dict[Union[str, int], float]] = Field(
        None, description="Modify likelihood of specified tokens"
    )
    logprobs: Optional[bool] = Field(
        False, description="Whether to return log probabilities"
    )
    top_logprobs: Optional[int] = Field(
        None, ge=0, le=20, description="Number of top log probabilities to return"
    )
    n: Optional[int] = Field(
        1, ge=1, le=128, description="Number of chat completion choices to generate"
    )
    response_format: Optional[
        Dict[Literal["type"], Literal["text", "json_object"]]
    ] = Field(None, description="Format of the response")
    tools: Optional[List[Dict[str, Any]]] = Field(
        None, max_length=64, description="List of tools the model may call"
    )
    tool_choice: Optional[Union[str, Dict[str, Any]]] = Field(
        None, description="Controls which tool is called"
    )
    function_call: Optional[Union[str, Dict[str, Any]]] = Field(
        None, description="Controls which function is called"
    )
    seed: Optional[int] = Field(None, description="Seed for deterministic sampling")
    user: Optional[str] = Field(
        None, description="Unique identifier representing the end-user"
    )

    @field_validator("*", mode="before")
    @classmethod
    def none_to_default(cls, v):
        if v is None:
            raise PydanticUseDefault()
        return v


class HFHubChatCompletionParams(BaseModel):
    """
    Specific settings for Hugging Face Hub Chat Completion endpoint.
    """

    model: Optional[str] = Field(
        None,
        description="The model to use for chat-completion. Can be a model ID or a URL to a deployed Inference Endpoint.",
    )
    frequency_penalty: Optional[float] = Field(
        0.0,
        description="Penalizes new tokens based on their existing frequency in the text so far.",
    )
    logit_bias: Optional[Dict[Union[str, int], float]] = Field(
        None,
        description="Modify the likelihood of specified tokens appearing in the completion.",
    )
    logprobs: Optional[bool] = Field(
        False,
        description="Whether to return log probabilities of the output tokens or not.",
    )
    max_tokens: Optional[int] = Field(
        100, ge=1, description="Maximum number of tokens allowed in the response."
    )
    n: Optional[int] = Field(None, description="UNUSED. Included for compatibility.")
    presence_penalty: Optional[float] = Field(
        0.0,
        description="Penalizes new tokens based on their presence in the text so far.",
    )
    response_format: Optional[Union[Dict[str, Any], str]] = Field(
        None, description="Grammar constraints. Can be either a JSONSchema or a regex."
    )
    seed: Optional[int] = Field(None, description="Seed for reproducible control flow.")
    stop: Optional[Union[str, List[str]]] = Field(
        None, description="Up to four strings which trigger the end of the response."
    )
    stream: Optional[bool] = Field(
        False, description="Enable realtime streaming of responses."
    )
    stream_options: Optional[Dict[str, Any]] = Field(
        None, description="Options for streaming completions."
    )
    temperature: Optional[float] = Field(
        1.0, description="Controls randomness of the generations."
    )
    top_logprobs: Optional[int] = Field(
        None, description="Number of most likely tokens to return at each position."
    )
    top_p: Optional[float] = Field(
        None, description="Fraction of the most likely next words to sample from."
    )
    tool_choice: Optional[Union[str, Dict[str, Any]]] = Field(
        None, description="The tool to use for the completion. Defaults to 'auto'."
    )
    tool_prompt: Optional[str] = Field(
        None, description="A prompt to be appended before the tools."
    )
    tools: Optional[List[Dict[str, Any]]] = Field(
        None, description="A list of tools the model may call."
    )

    @field_validator("*", mode="before")
    @classmethod
    def none_to_default(cls, v):
        if v is None:
            raise PydanticUseDefault()
        return v


class NVIDIAChatCompletionParams(OpenAIParamsBase):
    """
    Specific settings for the Chat Completion endpoint.
    """

    logit_bias: Optional[Dict[Union[str, int], float]] = Field(
        None, description="Modify likelihood of specified tokens"
    )
    logprobs: Optional[bool] = Field(
        False, description="Whether to return log probabilities"
    )
    top_logprobs: Optional[int] = Field(
        None, ge=0, le=20, description="Number of top log probabilities to return"
    )
    n: Optional[int] = Field(
        1, ge=1, le=128, description="Number of chat completion choices to generate"
    )
    tools: Optional[List[Dict[str, Any]]] = Field(
        None, max_length=64, description="List of tools the model may call"
    )
    tool_choice: Optional[Union[str, Dict[str, Any]]] = Field(
        None, description="Controls which tool is called"
    )

    @field_validator("*", mode="before")
    @classmethod
    def none_to_default(cls, v):
        if v is None:
            raise PydanticUseDefault()
        return v


class PromptyModelConfig(BaseModel):
    api: Literal["chat", "completion"] = Field(
        "chat", description="The API to use, either 'chat' or 'completion'"
    )
    configuration: Union[
        OpenAIModelConfig, AzureOpenAIModelConfig, HFHubModelConfig, NVIDIAModelConfig
    ] = Field(..., description="Model configuration settings")
    parameters: Union[
        OpenAITextCompletionParams,
        OpenAIChatCompletionParams,
        HFHubChatCompletionParams,
        NVIDIAChatCompletionParams,
    ] = Field(..., description="Parameters for the model request")
    response: Literal["first", "full"] = Field(
        "first",
        description="Determines if full response or just the first one is returned",
    )

    @field_validator("*", mode="before")
    @classmethod
    def none_to_default(cls, v):
        if v is None:
            raise PydanticUseDefault()
        return v

    @model_validator(mode="before")
    def sync_model_name(cls, values: dict):
        """
        Ensure that the parameters model name matches the configuration model name.
        """
        configuration = values.get("configuration")
        parameters = values.get("parameters")

        # Ensure the 'configuration' is properly validated as a model, not a dict
        if isinstance(configuration, dict):
            if configuration.get("type") == "openai":
                configuration = OpenAIModelConfig(**configuration)
            elif configuration.get("type") == "azure_openai":
                configuration = AzureOpenAIModelConfig(**configuration)
            elif configuration.get("type") == "huggingface":
                configuration = HFHubModelConfig(**configuration)
            elif configuration.get("type") == "nvidia":
                configuration = NVIDIAModelConfig(**configuration)

        # Ensure 'parameters' is properly validated as a model, not a dict
        if isinstance(parameters, dict):
            if configuration and isinstance(configuration, OpenAIModelConfig):
                parameters = OpenAIChatCompletionParams(**parameters)
            elif configuration and isinstance(configuration, AzureOpenAIModelConfig):
                parameters = OpenAIChatCompletionParams(**parameters)
            elif configuration and isinstance(configuration, HFHubModelConfig):
                parameters = HFHubChatCompletionParams(**parameters)
            elif configuration and isinstance(configuration, NVIDIAModelConfig):
                parameters = NVIDIAChatCompletionParams(**parameters)

        if configuration and parameters:
            # Check if 'name' or 'azure_deployment' is explicitly set
            if "name" in configuration.model_fields_set:
                parameters.model = configuration.name
            elif "azure_deployment" in configuration.model_fields_set:
                parameters.model = configuration.azure_deployment

        values["configuration"] = configuration
        values["parameters"] = parameters
        return values


class PromptyDefinition(BaseModel):
    """Schema for a Prompty definition."""

    name: Optional[str] = Field("", description="Name of the Prompty file.")
    description: Optional[str] = Field(
        "", description="Description of the Prompty file."
    )
    version: Optional[str] = Field("1.0", description="Version of the Prompty.")
    authors: Optional[List[str]] = Field(
        [], description="List of authors for the Prompty."
    )
    tags: Optional[List[str]] = Field([], description="Tags to categorize the Prompty.")
    model: PromptyModelConfig = Field(
        ..., description="Model configuration. Can be either OpenAI or Azure OpenAI."
    )
    inputs: Dict[str, Any] = Field(
        {},
        description="Input parameters for the Prompty. These define the expected inputs.",
    )
    sample: Optional[Union[Dict[str, Any], str]] = Field(
        None,
        description="Sample input or the path to a sample file for testing the Prompty.",
    )
    outputs: Optional[Dict[str, Any]] = Field(
        {},
        description="Optional outputs for the Prompty. Defines expected output format.",
    )
    content: str = Field(
        ..., description="The prompt messages defined in the Prompty file."
    )

    @field_validator("*", mode="before")
    @classmethod
    def none_to_default(cls, v):
        if v is None:
            raise PydanticUseDefault()
        return v


class AudioSpeechRequest(BaseModel):
    model: Optional[Literal["tts-1", "tts-1-hd"]] = Field(
        "tts-1", description="TTS model to use. Defaults to 'tts-1'."
    )
    input: str = Field(
        ...,
        description="Text to generate audio for. If the input exceeds 4096 characters, it will be split into chunks.",
    )
    voice: Optional[
        Literal["alloy", "echo", "fable", "onyx", "nova", "shimmer"]
    ] = Field("alloy", description="Voice to use.")
    response_format: Optional[
        Literal["mp3", "opus", "aac", "flac", "wav", "pcm"]
    ] = Field("mp3", description="Audio format.")
    speed: Optional[float] = Field(
        1.0, ge=0.25, le=4.0, description="Speed of the audio."
    )


class AudioTranscriptionRequest(BaseModel):
    model: Optional[Literal["whisper-1"]] = Field(
        "whisper-1", description="Model to use. Defaults to 'whisper-1'."
    )
    file: Union[
        bytes,
        BytesIO,
        IO[bytes],
        BufferedReader,
        str,
        Path,
        Tuple[Optional[str], bytes],
        Tuple[Optional[str], bytes, Optional[str]],
    ] = Field(..., description="Audio file content.")
    language: Optional[str] = Field(
        None, description="Language of the audio in ISO-639-1 format."
    )
    prompt: Optional[str] = Field(
        None, description="Optional prompt for the transcription."
    )
    response_format: Optional[
        Literal["json", "text", "srt", "verbose_json", "vtt"]
    ] = Field("json", description="Response format.")
    temperature: Optional[float] = Field(
        0.0, ge=0.0, le=1.0, description="Sampling temperature."
    )
    timestamp_granularities: Optional[List[Literal["word", "segment"]]] = Field(
        None, description="Granularity of timestamps."
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @field_validator("file", mode="before")
    @classmethod
    def validate_file(
        cls, value: Union[bytes, IO, str, Path, Tuple]
    ) -> Union[IO[bytes], Tuple[str, bytes]]:
        """
        Ensure the file field is valid and prepare it for OpenAI API.
        """
        if isinstance(value, (str, Path)):
            # Convert file path to a readable file-like object
            try:
                return open(value, "rb")
            except Exception as e:
                raise ValueError(f"Invalid file path: {value}. Error: {e}")
        elif isinstance(value, bytes):
            # Wrap raw bytes with a default filename
            return "file.mp3", value
        elif isinstance(value, BufferedReader) or (
            hasattr(value, "read") and callable(value.read)
        ):
            # Allow BufferedReader or other file-like objects as-is
            return value
        elif isinstance(value, BufferedReader) or (
            hasattr(value, "read") and callable(value.read)
        ):
            if value.closed:
                raise ValueError("File-like object must remain open during request.")
            return value
        elif isinstance(value, tuple):
            # Handle tuples with (filename, bytes or file-like object)
            if len(value) == 2:
                filename, file_obj = value
                if isinstance(file_obj, bytes):
                    return filename, file_obj
                elif hasattr(file_obj, "read") and callable(file_obj.read):
                    return filename, file_obj.read()
            raise ValueError(
                "File tuple must be of the form (filename, bytes) or (filename, file-like object)."
            )
        else:
            raise ValueError(f"Unsupported file type: {type(value)}.")


class AudioTranslationRequest(BaseModel):
    model: Optional[Literal["whisper-1"]] = Field(
        "whisper-1", description="Model to use. Defaults to 'whisper-1'."
    )
    file: Union[
        bytes,
        BytesIO,
        IO[bytes],
        BufferedReader,
        str,
        Path,
        Tuple[Optional[str], bytes],
        Tuple[Optional[str], bytes, Optional[str]],
    ] = Field(..., description="Audio file content.")
    prompt: Optional[str] = Field(
        None, description="Optional prompt for the translation."
    )
    response_format: Optional[
        Literal["json", "text", "srt", "verbose_json", "vtt"]
    ] = Field("json", description="Response format.")
    temperature: Optional[float] = Field(
        0.0, ge=0.0, le=1.0, description="Sampling temperature."
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    @field_validator("file", mode="before")
    @classmethod
    def validate_file(
        cls, value: Union[bytes, IO, str, Path, Tuple]
    ) -> Union[IO[bytes], Tuple[str, bytes]]:
        """
        Ensure the file field is valid and prepare it for OpenAI API.
        """
        if isinstance(value, (str, Path)):
            # Convert file path to a readable file-like object
            try:
                return open(value, "rb")
            except Exception as e:
                raise ValueError(f"Invalid file path: {value}. Error: {e}")
        elif isinstance(value, bytes):
            # Wrap raw bytes with a default filename
            return "file.mp3", value
        elif isinstance(value, BufferedReader) or (
            hasattr(value, "read") and callable(value.read)
        ):
            if value.closed:  # Reopen if closed
                raise ValueError("File-like object must remain open during request.")
            return value
        elif isinstance(value, tuple):
            # Handle tuples with (filename, bytes or file-like object)
            if len(value) == 2:
                filename, file_obj = value
                if isinstance(file_obj, bytes):
                    return filename, file_obj
                elif hasattr(file_obj, "read") and callable(file_obj.read):
                    return filename, file_obj.read()
            raise ValueError(
                "File tuple must be of the form (filename, bytes) or (filename, file-like object)."
            )
        else:
            raise ValueError(f"Unsupported file type: {type(value)}.")


class AudioTranscriptionResponse(BaseModel):
    text: str
    language: Optional[str]
    duration: Optional[float]
    segments: Optional[List[Dict[str, Union[str, float, List[int]]]]]


class AudioTranslationResponse(BaseModel):
    text: str

================
File: types/message.py
================
from pydantic import (
    BaseModel,
    field_validator,
    ValidationError,
    model_validator,
    ConfigDict,
)
from typing import List, Optional, Dict
import json


class BaseMessage(BaseModel):
    """
    Base class for creating and processing message objects. This class provides common attributes that are shared across different types of messages.

    Attributes:
        content (Optional[str]): The main text content of the message. If provided, it initializes the message with this content.
        role (str): The role associated with the message (e.g., 'user', 'system', 'assistant'). This needs to be set by derived classes.
        name (Optional[str]): An optional name identifier for the message.

    Args:
        text (Optional[str]): An alternate way to provide text content during initialization.
        **data: Additional keyword arguments that are passed directly to the Pydantic model's constructor.
    """

    content: Optional[str]
    role: str
    name: Optional[str] = None

    def __init__(self, text: Optional[str] = None, **data):
        """
        Initializes a new BaseMessage instance. If 'text' is provided, it initializes the 'content' attribute with this value.

        Args:
            text (Optional[str]): Text content for the 'content' attribute.
            **data: Additional fields that can be set during initialization, passed as keyword arguments.
        """
        super().__init__(
            content=text, **data
        ) if text is not None else super().__init__(**data)

    @model_validator(mode="after")
    def remove_empty_name(self):
        attrList = []
        for attribute in self.__dict__:
            if attribute == "name":
                if self.__dict__[attribute] is None:
                    attrList.append(attribute)

        for item in attrList:
            delattr(self, item)

        return self


class FunctionCall(BaseModel):
    """
    Represents a function call with its name and arguments, which are stored as a JSON string.

    Attributes:
        name (str): Name of the function.
        arguments (str): A JSON string containing arguments for the function.
    """

    name: str
    arguments: str

    @field_validator("arguments", mode="before")
    @classmethod
    def validate_json(cls, v):
        """
        Ensures that the arguments are stored as a JSON string. If a dictionary is provided,
        it converts it to a JSON string. If a string is provided, it validates whether it's a proper JSON string.

        Args:
            v (Union[str, dict]): The JSON string or dictionary of arguments to validate and convert.

        Raises:
            ValueError: If the provided string is not valid JSON or if a type other than str or dict is provided.

        Returns:
            str: The JSON string representation of the arguments.
        """
        if isinstance(v, dict):
            try:
                return json.dumps(v)
            except TypeError as e:
                raise ValidationError(f"Invalid data type in dictionary: {e}")
        elif isinstance(v, str):
            try:
                json.loads(v)  # This is to check if it's valid JSON
                return v
            except json.JSONDecodeError as e:
                raise ValidationError(f"Invalid JSON format: {e}")
        else:
            raise TypeError(f"Unsupported type for field: {type(v)}")

    @property
    def arguments_dict(self):
        """
        Property to safely return arguments as a dictionary.
        """
        return json.loads(self.arguments) if self.arguments else {}


class ToolCall(BaseModel):
    """
    Represents a tool call within a message, detailing the tool that should be called.

    Attributes:
        id (str): Unique identifier of the tool call.
        type (str): Type of tool being called.
        function (Function): The function that should be called as part of the tool call.
    """

    id: str
    type: str
    function: FunctionCall


class MessageContent(BaseMessage):
    """
    Extends BaseMessage to include dynamic optional fields for tool and function calls.

    Utilizes post-initialization validation to dynamically manage the inclusion of `tool_calls` and `function_call` fields based on their presence in the initialization data. Fields are only retained if they contain data, thus preventing serialization or display of `None` values, which helps maintain clean and concise object representations.

    Attributes:
        tool_calls (List[ToolCall], optional): A list of tool calls added dynamically if provided in the initialization data.
        function_call (FunctionCall, optional): A function call added dynamically if provided in the initialization data.
    """

    tool_calls: Optional[List[ToolCall]] = None
    function_call: Optional[FunctionCall] = None

    @model_validator(mode="after")
    def remove_empty_calls(self):
        attrList = []
        for attribute in self.__dict__:
            if attribute in ("tool_calls", "function_call"):
                if self.__dict__[attribute] is None:
                    attrList.append(attribute)

        for item in attrList:
            delattr(self, item)

        return self


class Choice(BaseModel):
    """
    Represents a choice made by the model, detailing the reason for completion, its index, and the message content.

    Attributes:
        finish_reason (str): Reason why the model stopped generating text.
        index (int): Index of the choice in a list of potential choices.
        message (MessageContent): Content of the message chosen by the model.
        logprobs (Optional[dict]): Log probabilities associated with the choice.
    """

    finish_reason: str
    index: int
    message: MessageContent
    logprobs: Optional[dict]


class ChatCompletion(BaseModel):
    """
    Represents the full response from the chat API, including all choices, metadata, and usage information.

    Attributes:
        choices (List[Choice]): List of choices provided by the model.
        created (int): Timestamp when the response was created.
        id (str): Unique identifier for the response.
        model (str): Model used for generating the response.
        object (str): Type of object returned.
        usage (dict): Information about API usage for this request.
    """

    choices: List[Choice]
    created: int
    id: Optional[str] = None
    model: str
    object: Optional[str] = None
    usage: dict

    def get_message(self) -> Optional[str]:
        """
        Retrieve the main message content from the first choice.
        """
        return self.choices[0].message.model_dump() if self.choices else None

    def get_reason(self) -> Optional[str]:
        """
        Retrieve the reason for completion from the first choice.
        """
        return self.choices[0].finish_reason if self.choices else None

    def get_tool_calls(self) -> Optional[List[ToolCall]]:
        """
        Retrieve tool calls from the first choice, if available.
        """
        return (
            self.choices[0].message.tool_calls
            if self.choices and self.choices[0].message.tool_calls
            else None
        )

    def get_content(self) -> Optional[str]:
        """
        Retrieve the content from the first choice's message.
        """
        message = self.get_message()
        return message.get("content") if message else None


class SystemMessage(BaseMessage):
    """
    Represents a system message, automatically assigning the role to 'system'.

    Attributes:
        role (str): The role of the message, set to 'system' by default.
    """

    role: str = "system"


class UserMessage(BaseMessage):
    """
    Represents a user message, automatically assigning the role to 'user'.

    Attributes:
        role (str): The role of the message, set to 'user' by default.
    """

    role: str = "user"


class AssistantMessage(BaseMessage):
    """
    Represents an assistant message, potentially including tool calls, automatically assigning the role to 'assistant'.
    This message type is commonly used for responses generated by an assistant.

    Attributes:
        role (str): The role of the message, set to 'assistant' by default.
        tool_calls (List[ToolCall], optional): A list of tool calls added dynamically if provided in the initialization data.
        function_call (FunctionCall, optional): A function call added dynamically if provided in the initialization data.
    """

    role: str = "assistant"
    tool_calls: Optional[List[ToolCall]] = None
    function_call: Optional[FunctionCall] = None

    @model_validator(mode="after")
    def remove_empty_calls(self):
        attrList = []
        for attribute in self.__dict__:
            if attribute in ("tool_calls", "function_call"):
                if self.__dict__[attribute] is None:
                    attrList.append(attribute)

        for item in attrList:
            delattr(self, item)

        return self


class ToolMessage(BaseMessage):
    """
    Represents a message specifically used for carrying tool interaction information, automatically assigning the role to 'tool'.

    Attributes:
        role (str): The role of the message, set to 'tool' by default.
        tool_call_id (str): Identifier for the specific tool call associated with the message.
    """

    role: str = "tool"
    tool_call_id: str


class AssistantFinalMessage(BaseModel):
    """
    Represents a custom final message from the assistant, encapsulating a conclusive response to the user.

    Attributes:
        prompt (str): The initial prompt that led to the final answer.
        final_answer (str): The definitive answer or conclusion provided by the assistant.
    """

    prompt: str
    final_answer: str


class MessagePlaceHolder(BaseModel):
    """
    A placeholder for a list of messages in the prompt template.

    This allows dynamic insertion of message lists into the prompt, such as chat history or
    other sequences of messages.
    """

    variable_name: str
    model_config = ConfigDict(frozen=True)

    def __repr__(self):
        return f"MessagePlaceHolder(variable_name={self.variable_name})"


class EventMessageMetadata(BaseModel):
    """
    Represents CloudEvent metadata for describing event context and attributes.

    This class encapsulates core attributes as defined by the CloudEvents specification.
    Each field corresponds to a CloudEvent context attribute, providing additional metadata
    about the event.

    Attributes:
        id (Optional[str]):
            Identifies the event. Producers MUST ensure that source + id is unique for each
            distinct event. Required and must be a non-empty string.
        datacontenttype (Optional[str]):
            Content type of the event data value, e.g., 'application/json'.
            Optional and must adhere to RFC 2046.
        pubsubname (Optional[str]):
            Name of the Pub/Sub system delivering the event. Optional and specific to implementation.
        source (Optional[str]):
            Identifies the context in which an event happened. Required and must be a non-empty URI-reference.
        specversion (Optional[str]):
            The version of the CloudEvents specification used by this event. Required and must be non-empty.
        time (Optional[str]):
            The timestamp of when the occurrence happened in RFC 3339 format. Optional.
        topic (Optional[str]):
            The topic name that categorizes the event within the Pub/Sub system. Optional and specific to implementation.
        traceid (Optional[str]):
            The identifier for tracing systems to correlate events. Optional.
        traceparent (Optional[str]):
            Parent identifier in the tracing system. Optional and adheres to the W3C Trace Context standard.
        type (Optional[str]):
            Describes the type of event related to the originating occurrence. Required and must be a non-empty string.
        tracestate (Optional[str]):
            Vendor-specific tracing information. Optional and adheres to the W3C Trace Context standard.
        headers (Optional[Dict[str, str]]):
            HTTP headers or transport metadata. Optional and contains key-value pairs.
    """

    id: Optional[str]
    datacontenttype: Optional[str]
    pubsubname: Optional[str]
    source: Optional[str]
    specversion: Optional[str]
    time: Optional[str]
    topic: Optional[str]
    traceid: Optional[str]
    traceparent: Optional[str]
    type: Optional[str]
    tracestate: Optional[str]
    headers: Optional[Dict[str, str]]

================
File: types/schemas.py
================
from typing import Any, Dict, Optional
from typing_extensions import Literal
from pydantic import BaseModel, Field


class OAIJSONSchema(BaseModel):
    """
    Defines the content for a JSON schema used in OpenAI's response_format.
    """

    name: str = Field(
        ...,
        description="The name of the response format.",
        json_schema_extra={
            "maxLength": 64,
            "pattern": "^[a-zA-Z0-9_-]+$",
            "examples": ["example_schema_name"],
        },
    )
    description: Optional[str] = Field(
        None,
        description="Explains the purpose of this response format so the model knows how to respond.",
    )
    schema_: Optional[Dict[str, Any]] = Field(
        None,
        serialization_alias="schema",
        description="The underlying JSON Schema object describing the response format structure.",
    )
    strict: bool = Field(
        True,
        description=(
            "Whether to enforce strict schema adherence when generating the output. "
            "If set to True, only a subset of JSON Schema features is supported."
        ),
    )


class OAIResponseFormatSchema(BaseModel):
    """
    Represents the top-level structure for OpenAI's 'json_schema' response format.
    """

    type: Literal["json_schema"] = Field(
        "json_schema",
        description="Specifies that this response format is a JSON schema definition.",
    )
    json_schema: OAIJSONSchema = Field(
        ...,
        description="Contains metadata and the actual JSON schema for the structured output.",
    )

================
File: types/tools.py
================
from pydantic import BaseModel, field_validator, ValidationInfo
from typing import Dict, Literal, Optional, List


class OAIFunctionDefinition(BaseModel):
    """
    Represents a callable function in the OpenAI API format.

    Attributes:
        name (str): The name of the function.
        description (str): A detailed description of what the function does.
        parameters (Dict): A dictionary describing the parameters that the function accepts.
    """

    name: str
    description: str
    parameters: Dict


class OAIToolDefinition(BaseModel):
    """
    Represents a tool (callable function) in the OpenAI API format. This can be a function, code interpreter, or file search tool.

    Attributes:
        type (Literal["function", "code_interpreter", "file_search"]): The type of the tool.
        function (Optional[OAIBaseFunctionDefinition]): The function definition, required if type is 'function'.
    """

    type: Literal["function", "code_interpreter", "file_search"]
    function: Optional[OAIFunctionDefinition] = None

    @field_validator("function")
    def check_function_requirements(cls, v, info: ValidationInfo):
        if info.data.get("type") == "function" and not v:
            raise ValueError(
                "Function definition must be provided for function type tools."
            )
        return v


class ClaudeToolDefinition(BaseModel):
    """
    Represents a tool (callable function) in the Anthropic's Claude API format, suitable for integration with Claude's API services.

    Attributes:
        name (str): The name of the function.
        description (str): A description of the function's purpose and usage.
        input_schema (Dict): A dictionary defining the input schema for the function.
    """

    name: str
    description: str
    input_schema: Dict


class GeminiFunctionDefinition(BaseModel):
    """
    Represents a callable function in the Google's Gemini API format.

    Attributes:
        name (str): The name of the function to call. Must start with a letter or an underscore. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
        description (str): The description and purpose of the function. The model uses this to decide how and whether to call the function. For the best results, we recommend that you include a description.
        parameters (Dict): Describes the parameters of the function in the OpenAPI JSON Schema Object format: OpenAPI 3.0 specification.
    """

    name: str
    description: str
    parameters: Dict


class GeminiToolDefinition(BaseModel):
    """
    Represents a tool (callable function) in the Google's Gemini API format, suitable for integration with Gemini's API services.

    Attributes:
        function_declarations (List): A structured representation of a function declaration as defined by the OpenAPI 3.0 specification that represents a function the model may generate JSON inputs for.
    """

    function_declarations: List[GeminiFunctionDefinition]

================
File: types/workflow.py
================
from enum import Enum


class DaprWorkflowStatus(str, Enum):
    """Enumeration of possible workflow statuses for standardized tracking."""

    UNKNOWN = "unknown"  # Workflow is in an undefined state
    RUNNING = "running"  # Workflow is actively running
    COMPLETED = "completed"  # Workflow has completed
    FAILED = "failed"  # Workflow encountered an error
    TERMINATED = "terminated"  # Workflow was canceled or forcefully terminated
    SUSPENDED = "suspended"  # Workflow was temporarily paused
    PENDING = "pending"  # Workflow is waiting to start

================
File: workflow/agents/assistant/__init__.py
================
from .agent import AssistantAgent

================
File: workflow/agents/assistant/agent.py
================
import json
import logging
from datetime import datetime
from typing import Any, Dict, List, Optional, Union

from pydantic import Field

from dapr.ext.workflow import DaprWorkflowContext

from dapr_agents.types import (
    AgentError,
    ChatCompletion,
    ToolMessage,
)
from dapr_agents.workflow.agents.assistant.schemas import (
    AgentTaskResponse,
    BroadcastMessage,
    TriggerAction,
)
from dapr_agents.workflow.agents.assistant.state import (
    AssistantWorkflowEntry,
    AssistantWorkflowMessage,
    AssistantWorkflowState,
    AssistantWorkflowToolMessage,
)
from dapr_agents.workflow.agents.base import AgentWorkflowBase
from dapr_agents.workflow.decorators import task, workflow
from dapr_agents.workflow.messaging.decorator import message_router

logger = logging.getLogger(__name__)


class AssistantAgent(AgentWorkflowBase):
    """
    A conversational AI agent that responds to user messages, engages in discussions,
    and dynamically utilizes external tools when needed.

    The AssistantAgent follows an agentic workflow, iterating on responses based on
    contextual understanding, reasoning, and tool-assisted execution. It ensures
    meaningful interactions by selecting the right tools, generating relevant responses,
    and refining outputs through iterative feedback loops.
    """

    tool_history: List[ToolMessage] = Field(
        default_factory=list, description="Executed tool calls during the conversation."
    )
    tool_choice: Optional[str] = Field(
        default=None,
        description="Strategy for selecting tools ('auto', 'required', 'none'). Defaults to 'auto' if tools are provided.",
    )

    def model_post_init(self, __context: Any) -> None:
        """Initializes the workflow with agentic execution capabilities."""

        # Initialize Agent State
        self.state = AssistantWorkflowState()

        # Name of main Workflow
        self._workflow_name = "ToolCallingWorkflow"

        # Define Tool Selection Strategy
        self.tool_choice = self.tool_choice or ("auto" if self.tools else None)

        super().model_post_init(__context)

    @message_router
    @workflow(name="ToolCallingWorkflow")
    def tool_calling_workflow(self, ctx: DaprWorkflowContext, message: TriggerAction):
        """
        Executes a tool-calling workflow, determining the task source (either an agent or an external user).
        """
        # Step 0: Retrieve task and iteration input
        task = message.get("task")
        iteration = message.get("iteration", 0)
        instance_id = ctx.instance_id

        if not ctx.is_replaying:
            logger.info(
                f"Workflow iteration {iteration + 1} started (Instance ID: {instance_id})."
            )

        # Step 1: Initialize instance entry on first iteration
        if iteration == 0:
            metadata = message.get("_message_metadata", {})

            # Ensure "instances" key exists
            self.state.setdefault("instances", {})

            # Extract workflow metadata with proper defaults
            source = metadata.get("source") or None
            source_workflow_instance_id = message.get("workflow_instance_id") or None

            # Create a new workflow entry
            workflow_entry = AssistantWorkflowEntry(
                input=task or "Triggered without input.",
                source=source,
                source_workflow_instance_id=source_workflow_instance_id,
            )

            # Store in state, converting to JSON only if necessary
            self.state["instances"].setdefault(
                instance_id, workflow_entry.model_dump(mode="json")
            )

            if not ctx.is_replaying:
                logger.info(
                    f"Initial message from {self.state['instances'][instance_id]['source']} -> {self.name}"
                )

        # Step 2: Retrieve workflow entry for this instance
        workflow_entry = self.state["instances"][instance_id]
        source = workflow_entry["source"]
        source_workflow_instance_id = workflow_entry["source_workflow_instance_id"]

        # Step 3: Generate Response
        response = yield ctx.call_activity(
            self.generate_response, input={"instance_id": instance_id, "task": task}
        )
        response_message = yield ctx.call_activity(
            self.get_response_message, input={"response": response}
        )

        # Step 4: Extract Finish Reason
        finish_reason = yield ctx.call_activity(
            self.get_finish_reason, input={"response": response}
        )

        # Step 5: Choose execution path based on LLM response
        if finish_reason == "tool_calls":
            if not ctx.is_replaying:
                logger.info(
                    "Tool calls detected in LLM response, extracting and preparing for execution.."
                )

            # Retrieve the list of tool calls extracted from the LLM response
            tool_calls = yield ctx.call_activity(
                self.get_tool_calls, input={"response": response}
            )

            # Execute tool calls in parallel
            if not ctx.is_replaying:
                logger.info(f"Executing {len(tool_calls)} tool call(s)..")

            parallel_tasks = [
                ctx.call_activity(
                    self.execute_tool,
                    input={"instance_id": instance_id, "tool_call": tool_call},
                )
                for tool_call in tool_calls
            ]
            yield self.when_all(parallel_tasks)
        else:
            if not ctx.is_replaying:
                logger.info("Agent generating response without tool execution..")

            # No Tool Calls → Clear tools
            self.tool_history.clear()

        # Step 6: Determine if Workflow Should Continue
        next_iteration_count = iteration + 1
        max_iterations_reached = next_iteration_count > self.max_iterations

        if finish_reason == "stop" or max_iterations_reached:
            # Determine the reason for stopping
            if max_iterations_reached:
                verdict = "max_iterations_reached"
                if not ctx.is_replaying:
                    logger.warning(
                        f"Workflow {instance_id} reached the max iteration limit ({self.max_iterations}) before finishing naturally."
                    )

                # Modify the response message to indicate forced stop
                response_message[
                    "content"
                ] += "\n\nThe workflow was terminated because it reached the maximum iteration limit. The task may not be fully complete."

            else:
                verdict = "model hit a natural stop point."

            # Step 8: Broadcasting Response to all agents if available
            yield ctx.call_activity(
                self.broadcast_message_to_agents, input={"message": response_message}
            )

            # Step 9: Respond to source agent if available
            yield ctx.call_activity(
                self.send_response_back,
                input={
                    "response": response_message,
                    "target_agent": source,
                    "target_instance_id": source_workflow_instance_id,
                },
            )

            # Step 10: Share Final Message
            yield ctx.call_activity(
                self.finish_workflow,
                input={"instance_id": instance_id, "message": response_message},
            )

            if not ctx.is_replaying:
                logger.info(
                    f"Workflow {instance_id} has been finalized with verdict: {verdict}"
                )

            return response_message

        # Step 7: Continue Workflow Execution
        message.update({"task": None, "iteration": next_iteration_count})
        ctx.continue_as_new(message)

    @task
    async def generate_response(
        self, instance_id: str, task: Union[str, Dict[str, Any]] = None
    ) -> ChatCompletion:
        """
        Generates a response using a language model based on the provided task input.

        Args:
            instance_id (str): The unique identifier of the workflow instance.
            task (Union[str, Dict[str, Any]], optional): The task description or structured input
                used to generate the response. Defaults to None.

        Returns:
            ChatCompletion: The generated AI response encapsulated in a ChatCompletion object.
        """
        # Contruct prompt messages
        messages = self.construct_messages(task or {})

        # Store message in workflow state and local memory
        if task:
            task_message = {"role": "user", "content": task}
            await self.update_workflow_state(
                instance_id=instance_id, message=task_message
            )

        # Process conversation iterations
        messages += self.tool_history

        # Generate Tool Calls
        response: ChatCompletion = self.llm.generate(
            messages=messages, tools=self.tools, tool_choice=self.tool_choice
        )

        # Return chat completion as a dictionary
        return response.model_dump()

    @task
    def get_response_message(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extracts the response message from the first choice in the LLM response.

        Args:
            response (Dict[str, Any]): The response dictionary from the LLM, expected to contain a "choices" key.

        Returns:
            Dict[str, Any]: The extracted response message with the agent's name added.
        """
        choices = response.get("choices", [])
        response_message = choices[0].get("message", {})

        return response_message

    @task
    def get_finish_reason(self, response: Dict[str, Any]) -> str:
        """
        Extracts the finish reason from the LLM response, indicating why generation stopped.

        Args:
            response (Dict[str, Any]): The response dictionary from the LLM, expected to contain a "choices" key.

        Returns:
            str: The reason the model stopped generating tokens. Possible values include:
                - "stop": Natural stop point or stop sequence encountered.
                - "length": Maximum token limit reached.
                - "content_filter": Content flagged by filters.
                - "tool_calls": The model called a tool.
                - "function_call" (deprecated): The model called a function.
                - None: If no valid choice exists in the response.
        """
        choices = response.get("choices", [])

        # Ensure there is at least one choice available
        if not choices:
            logger.warning("No choices found in LLM response.")
            return None  # Explicit return to avoid returning 'None' implicitly

        # Extract finish reason safely
        choice = choices[0].get("finish_reason", None)

        return choice

    @task
    def get_tool_calls(
        self, response: Dict[str, Any]
    ) -> Optional[List[Dict[str, Any]]]:
        """
        Extracts tool calls from the first choice in the LLM response, if available.

        Args:
            response (Dict[str, Any]): The response dictionary from the LLM, expected to contain "choices"
                                    and potentially tool call information.

        Returns:
            Optional[List[Dict[str, Any]]]: A list of tool calls if present, otherwise None.
        """
        choices = response.get("choices", [])

        if not choices:
            logger.warning("No choices found in LLM response.")
            return None

        # Save Tool Call Response Message
        response_message = choices[0].get("message", {})
        self.tool_history.append(response_message)

        # Extract tool calls safely
        tool_calls = choices[0].get("message", {}).get("tool_calls")

        if not tool_calls:
            logger.info("No tool calls found in LLM response.")
            return None

        return tool_calls

    @task
    async def execute_tool(self, instance_id: str, tool_call: Dict[str, Any]):
        """
        Executes a tool call by invoking the specified function with the provided arguments.

        Args:
            instance_id (str): The unique identifier of the workflow instance.
            tool_call (Dict[str, Any]): A dictionary containing tool execution details, including the function name and arguments.

        Raises:
            AgentError: If the tool call is malformed or execution fails.
        """
        function_details = tool_call.get("function", {})
        function_name = function_details.get("name")

        if not function_name:
            raise AgentError("Missing function name in tool execution request.")

        try:
            function_args = function_details.get("arguments", "")
            function_args_as_dict = json.loads(function_args) if function_args else {}

            # Execute tool function
            result = await self.tool_executor.run_tool(
                function_name, **function_args_as_dict
            )
            # Construct tool execution message payload
            workflow_tool_message = {
                "tool_call_id": tool_call.get("id"),
                "function_name": function_name,
                "function_args": function_args,
                "content": str(result),
            }

            # Update workflow state and agent tool history
            await self.update_workflow_state(
                instance_id=instance_id, tool_message=workflow_tool_message
            )

        except json.JSONDecodeError:
            logger.error(
                f"Invalid JSON in tool arguments for function '{function_name}'"
            )
            raise AgentError(
                f"Invalid JSON format in arguments for tool '{function_name}'."
            )

        except Exception as e:
            logger.error(f"Error executing tool '{function_name}': {e}", exc_info=True)
            raise AgentError(f"Error executing tool '{function_name}': {e}") from e

    @task
    async def broadcast_message_to_agents(self, message: Dict[str, Any]):
        """
        Broadcasts it to all registered agents.

        Args:
            message (Dict[str, Any]): A message to append to the workflow state and broadcast to all agents.
        """
        # Format message for broadcasting
        message["role"] = "user"
        message["name"] = self.name
        response_message = BroadcastMessage(**message)

        # Broadcast message to all agents
        await self.broadcast_message(message=response_message)

    @task
    async def send_response_back(
        self, response: Dict[str, Any], target_agent: str, target_instance_id: str
    ):
        """
        Sends a task response back to a target agent within a workflow.

        Args:
            response (Dict[str, Any]): The response payload to be sent.
            target_agent (str): The name of the agent that should receive the response.
            target_instance_id (str): The workflow instance ID associated with the response.

        Raises:
            ValidationError: If the response does not match the expected structure for `AgentTaskResponse`.
        """
        # Format Response
        response["role"] = "user"
        response["name"] = self.name
        response["workflow_instance_id"] = target_instance_id
        agent_response = AgentTaskResponse(**response)

        # Send the message to the target agent
        await self.send_message_to_agent(name=target_agent, message=agent_response)

    @task
    async def finish_workflow(self, instance_id: str, message: Dict[str, Any]):
        """
        Finalizes the workflow by storing the provided message as the final output.

        Args:
            instance_id (str): The unique identifier of the workflow instance.
            summary (Dict[str, Any]): The final summary to be stored in the workflow state.
        """
        # Store message in workflow state
        await self.update_workflow_state(instance_id=instance_id, message=message)

        # Store final output
        await self.update_workflow_state(
            instance_id=instance_id, final_output=message["content"]
        )

    async def update_workflow_state(
        self,
        instance_id: str,
        message: Optional[Dict[str, Any]] = None,
        tool_message: Optional[Dict[str, Any]] = None,
        final_output: Optional[str] = None,
    ):
        """
        Updates the workflow state by appending a new message or setting the final output.

        Args:
            instance_id (str): The unique identifier of the workflow instance.
            message (Optional[Dict[str, Any]]): A dictionary representing a user/assistant message.
            tool_message (Optional[Dict[str, Any]]): A dictionary representing a tool execution message.
            final_output (Optional[str]): The final output of the workflow, marking its completion.

        Raises:
            ValueError: If no workflow entry is found for the given instance_id.
        """
        workflow_entry: AssistantWorkflowEntry = self.state["instances"].get(
            instance_id
        )
        if not workflow_entry:
            raise ValueError(
                f"No workflow entry found for instance_id {instance_id} in local state."
            )

        # Store user/assistant messages separately
        if message is not None:
            serialized_message = AssistantWorkflowMessage(**message).model_dump(
                mode="json"
            )
            workflow_entry["messages"].append(serialized_message)
            workflow_entry["last_message"] = serialized_message

            # Add to memory only if it's a user/assistant message
            self.memory.add_message(message)

        # Store tool execution messages separately in tool_history
        if tool_message is not None:
            serialized_tool_message = AssistantWorkflowToolMessage(
                **tool_message
            ).model_dump(mode="json")
            workflow_entry["tool_history"].append(serialized_tool_message)

            # Also update agent-level tool history (execution tracking)
            agent_tool_message = ToolMessage(
                tool_call_id=tool_message["tool_call_id"],
                name=tool_message["function_name"],
                content=tool_message["content"],
            )
            self.tool_history.append(agent_tool_message)

        # Store final output
        if final_output is not None:
            workflow_entry["output"] = final_output
            workflow_entry["end_time"] = datetime.now().isoformat()

        # Persist updated state
        self.save_state()

    @message_router(broadcast=True)
    async def process_broadcast_message(self, message: BroadcastMessage):
        """
        Processes a broadcast message, filtering out messages sent by the same agent
        and updating local memory with valid messages.

        Args:
            message (BroadcastMessage): The received broadcast message.

        Returns:
            None: The function updates the agent's memory and ignores unwanted messages.
        """
        try:
            # Extract metadata safely from message["_message_metadata"]
            metadata = message.get("_message_metadata", {})

            if not isinstance(metadata, dict):
                logger.warning(
                    f"{self.name} received a broadcast message with invalid metadata format. Ignoring."
                )
                return

            source = metadata.get("source", "unknown_source")
            message_type = metadata.get("type", "unknown_type")
            message_content = message.get("content", "No Data")

            logger.info(
                f"{self.name} received broadcast message of type '{message_type}' from '{source}'."
            )

            # Ignore messages sent by this agent
            if source == self.name:
                logger.info(
                    f"{self.name} ignored its own broadcast message of type '{message_type}'."
                )
                return

            # Log and process the valid broadcast message
            logger.debug(
                f"{self.name} processing broadcast message from '{source}'. Content: {message_content}"
            )

            # Store the message in local memory
            self.memory.add_message(message)

        except Exception as e:
            logger.error(f"Error processing broadcast message: {e}", exc_info=True)

================
File: workflow/agents/assistant/schemas.py
================
from dapr_agents.types.message import BaseMessage
from pydantic import BaseModel, Field
from typing import Optional


class BroadcastMessage(BaseMessage):
    """
    Represents a broadcast message from an agent.
    """


class AgentTaskResponse(BaseMessage):
    """
    Represents a response message from an agent after completing a task.
    """

    workflow_instance_id: Optional[str] = Field(
        default=None, description="Dapr workflow instance id from source if available"
    )


class TriggerAction(BaseModel):
    """
    Represents a message used to trigger an agent's activity within the workflow.
    """

    task: Optional[str] = Field(
        None,
        description="The specific task to execute. If not provided, the agent will act based on its memory or predefined behavior.",
    )
    iteration: Optional[int] = Field(0, description="")
    workflow_instance_id: Optional[str] = Field(
        default=None, description="Dapr workflow instance id from source if available"
    )

================
File: workflow/agents/assistant/state.py
================
from pydantic import BaseModel, Field
from typing import List, Optional, Dict
from dapr_agents.types import ToolMessage
from datetime import datetime
import uuid


class AssistantWorkflowMessage(BaseModel):
    """Represents a message exchanged within the workflow."""

    id: str = Field(
        default_factory=lambda: str(uuid.uuid4()),
        description="Unique identifier for the message",
    )
    role: str = Field(
        ..., description="The role of the message sender, e.g., 'user' or 'assistant'"
    )
    content: str = Field(..., description="Content of the message")
    timestamp: datetime = Field(
        default_factory=datetime.now,
        description="Timestamp when the message was created",
    )
    name: Optional[str] = Field(
        default=None,
        description="Optional name of the assistant or user sending the message",
    )


class AssistantWorkflowToolMessage(ToolMessage):
    """Represents a Tool message exchanged within the workflow."""

    id: str = Field(
        default_factory=lambda: str(uuid.uuid4()),
        description="Unique identifier for the message",
    )
    function_name: str = Field(
        ...,
        description="Name of tool suggested by the model to run for a specific task.",
    )
    function_args: Optional[str] = Field(
        None,
        description="Tool arguments suggested by the model to run for a specific task.",
    )
    timestamp: datetime = Field(
        default_factory=datetime.now,
        description="Timestamp when the message was created",
    )


class AssistantWorkflowEntry(BaseModel):
    """Represents a workflow and its associated data, including metadata on the source of the task request."""

    input: str = Field(
        ..., description="The input or description of the Workflow to be performed"
    )
    output: Optional[str] = Field(
        None, description="The output or result of the Workflow, if completed"
    )
    start_time: datetime = Field(
        default_factory=datetime.now,
        description="Timestamp when the workflow was started",
    )
    end_time: Optional[datetime] = Field(
        None, description="Timestamp when the workflow was completed or failed"
    )
    messages: List[AssistantWorkflowMessage] = Field(
        default_factory=list, description="Messages exchanged during the workflow"
    )
    last_message: Optional[AssistantWorkflowMessage] = Field(
        default=None, description="Last processed message in the workflow"
    )
    tool_history: List[AssistantWorkflowToolMessage] = Field(
        default_factory=list, description="Tool message exchanged during the workflow"
    )
    source: Optional[str] = Field(None, description="Entity that initiated the task.")
    source_workflow_instance_id: Optional[str] = Field(
        None,
        description="The workflow instance ID associated with the original request.",
    )


class AssistantWorkflowState(BaseModel):
    """Represents the state of multiple Assistant workflows."""

    instances: Dict[str, AssistantWorkflowEntry] = Field(
        default_factory=dict,
        description="Workflow entries indexed by their instance_id.",
    )

================
File: workflow/agents/__init__.py
================
from .base import AgentWorkflowBase
from .assistant import AssistantAgent

================
File: workflow/agents/base.py
================
import logging
from datetime import datetime
from typing import Any, Callable, Dict, List, Literal, Optional, Union

from pydantic import Field, model_validator

from dapr_agents.llm import ChatClientBase, OpenAIChatClient
from dapr_agents.prompt import ChatPromptTemplate
from dapr_agents.prompt.base import PromptTemplateBase
from dapr_agents.tool.base import AgentTool
from dapr_agents.tool.executor import AgentToolExecutor
from dapr_agents.types import MessagePlaceHolder
from dapr_agents.workflow.agentic import AgenticWorkflow

logger = logging.getLogger(__name__)


class AgentWorkflowBase(AgenticWorkflow):
    role: Optional[str] = Field(
        default="Assistant",
        description="The agent's role in the interaction (e.g., 'Weather Expert').",
    )
    goal: Optional[str] = Field(
        default="Help humans",
        description="The agent's main objective (e.g., 'Provide Weather information').",
    )
    instructions: Optional[List[str]] = Field(
        default=None, description="Instructions guiding the agent's tasks."
    )
    system_prompt: Optional[str] = Field(
        default=None,
        description="A custom system prompt, overriding name, role, goal, and instructions.",
    )
    llm: ChatClientBase = Field(
        default_factory=OpenAIChatClient,
        description="Language model client for generating responses.",
    )
    prompt_template: Optional[PromptTemplateBase] = Field(
        default=None, description="The prompt template for the agent."
    )
    template_format: Literal["f-string", "jinja2"] = Field(
        default="jinja2",
        description="The format used for rendering the prompt template.",
    )
    tools: List[Union[AgentTool, Callable]] = Field(
        default_factory=list,
        description="Tools available for the agent to assist with tasks.",
    )
    agent_topic_name: Optional[str] = Field(
        None,
        description="The topic name dedicated to this specific agent, derived from the agent's name if not provided.",
    )

    # Fields Initialized during class initialization
    tool_executor: Optional[AgentToolExecutor] = Field(
        default=None, init=False, description="Client to execute and manage tools."
    )

    @model_validator(mode="before")
    def set_agent_and_topic_name(cls, values: dict):
        # Set name to role if name is not provided
        if not values.get("name") and values.get("role"):
            values["name"] = values["role"]

        # Derive agent_topic_name from agent name
        if not values.get("agent_topic_name") and values.get("name"):
            values["agent_topic_name"] = values["name"]

        return values

    def model_post_init(self, __context: Any) -> None:
        """
        Configure agentic workflows, set state parameters, and initialize metadata store.
        """

        # Initialize tool executor with provided tools
        self.tool_executor = AgentToolExecutor(tools=self.tools)

        # Check if both agent and LLM have a prompt template specified and raise an error if both exist
        if self.prompt_template and self.llm.prompt_template:
            raise ValueError(
                "Conflicting prompt templates: both an agent prompt_template and an LLM prompt_template are provided. "
                "Please set only one or ensure synchronization between the two."
            )

        # If the agent's prompt_template is provided, use it and skip further configuration
        if self.prompt_template:
            logger.info(
                "Using the provided agent prompt_template. Skipping system prompt construction."
            )
            self.llm.prompt_template = self.prompt_template

        # If the LLM client already has a prompt template, sync it and prefill/validate as needed
        elif self.llm.prompt_template:
            logger.info("Using existing LLM prompt_template. Synchronizing with agent.")
            self.prompt_template = self.llm.prompt_template

        else:
            if not self.system_prompt:
                logger.info("Constructing system_prompt from agent attributes.")
                self.system_prompt = self.construct_system_prompt()

            logger.info("Using system_prompt to create the prompt template.")
            self.prompt_template = self.construct_prompt_template()

        # Pre-fill Agent Attributes if needed
        self.prefill_agent_attributes()

        if not self.llm.prompt_template:
            # Assign the prompt template to the LLM client
            self.llm.prompt_template = self.prompt_template

        # Complete post-initialization
        super().model_post_init(__context)

        # Prepare agent metadata
        self._agent_metadata = {
            "name": self.name,
            "role": self.role,
            "goal": self.goal,
            "instructions": self.instructions,
            "topic_name": self.agent_topic_name,
            "pubsub_name": self.message_bus_name,
            "orchestrator": False,
        }

        # Register agent metadata
        self.register_agentic_system()

    def prefill_agent_attributes(self) -> None:
        """
        Pre-fill prompt template with agent attributes if specified in `input_variables`.
        Logs any agent attributes set but not used by the template.
        """
        # Start with a dictionary for attributes
        prefill_data = {}

        # Check if each attribute is defined in input_variables before adding
        if "name" in self.prompt_template.input_variables and self.name:
            prefill_data["name"] = self.name

        if "role" in self.prompt_template.input_variables:
            prefill_data["role"] = self.role

        if "goal" in self.prompt_template.input_variables:
            prefill_data["goal"] = self.goal

        if "instructions" in self.prompt_template.input_variables and self.instructions:
            prefill_data["instructions"] = "\n".join(self.instructions)

        # Collect attributes set but not in input_variables for informational logging
        set_attributes = {
            "name": self.name,
            "role": self.role,
            "goal": self.goal,
            "instructions": self.instructions,
        }

        # Use Pydantic's model_fields_set to detect if attributes were user-set
        user_set_attributes = {
            attr for attr in set_attributes if attr in self.model_fields_set
        }

        ignored_attributes = [
            attr
            for attr in set_attributes
            if attr not in self.prompt_template.input_variables
            and set_attributes[attr] is not None
            and attr in user_set_attributes
        ]

        # Apply pre-filled data only for attributes that are in input_variables
        if prefill_data:
            self.prompt_template = self.prompt_template.pre_fill_variables(
                **prefill_data
            )
            logger.info(
                f"Pre-filled prompt template with attributes: {list(prefill_data.keys())}"
            )
        elif ignored_attributes:
            raise ValueError(
                f"The following agent attributes were explicitly set by the user but are not considered by the prompt template: {', '.join(ignored_attributes)}. "
                "Please ensure that these attributes are included in the prompt template's input variables if they are needed."
            )
        else:
            logger.info(
                "No agent attributes were pre-filled, as the template did not require any."
            )

    def construct_system_prompt(self) -> str:
        """
        Constructs a system prompt with agent attributes like `name`, `role`, `goal`, and `instructions`.
        Sets default values for `role` and `goal` if not provided.

        Returns:
            str: A system prompt template string.
        """
        # Initialize prompt parts with the current date as the first entry
        prompt_parts = [f"# Today's date is: {datetime.now().strftime('%B %d, %Y')}"]

        # Append name if provided
        if self.name:
            prompt_parts.append("## Name\nYour name is {{name}}.")

        # Append role and goal with default values if not set
        prompt_parts.append("## Role\nYour role is {{role}}.")
        prompt_parts.append("## Goal\n{{goal}}.")

        # Append instructions if provided
        if self.instructions:
            prompt_parts.append("## Instructions\n{{instructions}}")

        return "\n\n".join(prompt_parts)

    def construct_prompt_template(self) -> ChatPromptTemplate:
        """
        Constructs a ChatPromptTemplate that includes the system prompt and a placeholder for chat history.
        Ensures that the template is flexible and adaptable to dynamically handle pre-filled variables.

        Returns:
            ChatPromptTemplate: A formatted prompt template for the agent.
        """
        # Construct the system prompt if not provided
        system_prompt = self.system_prompt or self.construct_system_prompt()

        # Create the template with placeholders for system message and chat history
        return ChatPromptTemplate.from_messages(
            messages=[
                ("system", system_prompt),
                MessagePlaceHolder(variable_name="chat_history"),
            ],
            template_format=self.template_format,
        )

    def construct_messages(
        self, input_data: Union[str, Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Constructs and formats initial messages based on input type, pre-filling chat history as needed.

        Args:
            input_data (Union[str, Dict[str, Any]]): User input, either as a string or dictionary.

        Returns:
            List[Dict[str, Any]]: List of formatted messages, including the user message if input_data is a string.
        """
        # Pre-fill chat history in the prompt template
        chat_history = self.get_chat_history()
        self.pre_fill_prompt_template(**{"chat_history": chat_history})

        # Handle string input by adding a user message
        if isinstance(input_data, str):
            formatted_messages = self.prompt_template.format_prompt()
            user_message = {"role": "user", "content": input_data}
            return formatted_messages + [user_message]

        # Handle dictionary input as dynamic variables for the template
        elif isinstance(input_data, dict):
            # Pass the dictionary directly, assuming it contains keys expected by the prompt template
            formatted_messages = self.prompt_template.format_prompt(**input_data)
            return formatted_messages

        else:
            raise ValueError("Input data must be either a string or dictionary.")

    def pre_fill_prompt_template(self, **kwargs: Union[str, Callable[[], str]]) -> None:
        """
        Pre-fills the prompt template with specified variables, updating input variables if applicable.

        Args:
            **kwargs: Variables to pre-fill in the prompt template. These can be strings or callables
                    that return strings.

        Notes:
            - Existing pre-filled variables will be overwritten by matching keys in `kwargs`.
            - This method does not affect the `chat_history` which is dynamically updated.
        """
        if not self.prompt_template:
            raise ValueError(
                "Prompt template must be initialized before pre-filling variables."
            )

        self.prompt_template = self.prompt_template.pre_fill_variables(**kwargs)
        logger.debug(f"Pre-filled prompt template with variables: {kwargs.keys()}")

================
File: workflow/messaging/__init__.py
================
from .routing import MessageRoutingMixin
from .decorator import message_router
from .pubsub import DaprPubSub

================
File: workflow/messaging/decorator.py
================
import logging
from copy import deepcopy
from typing import Any, Callable, Optional, get_type_hints
from dapr_agents.workflow.messaging.utils import (
    is_valid_routable_model,
    extract_message_models,
)

logger = logging.getLogger(__name__)


def message_router(
    func: Optional[Callable[..., Any]] = None,
    *,
    pubsub: Optional[str] = None,
    topic: Optional[str] = None,
    dead_letter_topic: Optional[str] = None,
    broadcast: bool = False,
) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    """
    Decorator for registering message handlers by inspecting type hints on the 'message' argument.

    This decorator:
    - Extracts the expected message model type from function annotations.
    - Stores metadata for routing messages by message schema instead of `event.type`.
    - Supports broadcast messaging.
    - Supports Union[...] and multiple models.

    Args:
        func (Optional[Callable]): The function to decorate.
        pubsub (Optional[str]): The name of the pub/sub component.
        topic (Optional[str]): The topic name for the handler.
        dead_letter_topic (Optional[str]): Dead-letter topic for failed messages.
        broadcast (bool): If True, the message is broadcast to all agents.

    Returns:
        Callable: The decorated function with additional metadata.
    """

    def decorator(f: Callable[..., Any]) -> Callable[..., Any]:
        is_workflow = hasattr(f, "_is_workflow")
        workflow_name = getattr(f, "_workflow_name", None)

        type_hints = get_type_hints(f)
        raw_hint = type_hints.get("message", None)

        message_models = extract_message_models(raw_hint)

        if not message_models:
            raise ValueError(
                f"Message handler '{f.__name__}' must have a 'message' parameter with a valid type hint."
            )

        for model in message_models:
            if not is_valid_routable_model(model):
                raise TypeError(
                    f"Handler '{f.__name__}' has unsupported message type: {model}"
                )

        logger.debug(
            f"@message_router: '{f.__name__}' => models {[m.__name__ for m in message_models]}"
        )

        # Attach metadata for later registration
        f._is_message_handler = True
        f._message_router_data = deepcopy(
            {
                "pubsub": pubsub,
                "topic": topic,
                "dead_letter_topic": dead_letter_topic
                or (f"{topic}_DEAD" if topic else None),
                "is_broadcast": broadcast,
                "message_schemas": message_models,
                "message_types": [model.__name__ for model in message_models],
            }
        )

        if is_workflow:
            f._is_workflow = True
            f._workflow_name = workflow_name

        return f

    return decorator(func) if func else decorator

================
File: workflow/messaging/parser.py
================
import logging
from typing import Any, Tuple, Type, Union, Optional
from dataclasses import is_dataclass
from dapr.common.pubsub.subscription import SubscriptionMessage
from dapr_agents.types.message import EventMessageMetadata
from dapr_agents.workflow.messaging.utils import is_supported_model, is_pydantic_model

logger = logging.getLogger(__name__)


def extract_cloudevent_data(
    message: Union[SubscriptionMessage, dict],
) -> Tuple[dict, dict]:
    """
    Extracts CloudEvent metadata and raw payload data from a SubscriptionMessage or dict.

    Args:
        message (Union[SubscriptionMessage, dict]): The raw message received from pub/sub.

    Returns:
        Tuple[dict, dict]: (event_data, metadata) where event_data is the message payload, and
                           metadata is the parsed CloudEvent metadata as a dictionary.

    Raises:
        ValueError: If message type is unsupported.
    """
    if isinstance(message, SubscriptionMessage):
        metadata = EventMessageMetadata(
            id=message.id(),
            datacontenttype=message.data_content_type(),
            pubsubname=message.pubsub_name(),
            source=message.source(),
            specversion=message.spec_version(),
            time=None,
            topic=message.topic(),
            traceid=None,
            traceparent=None,
            type=message.type(),
            tracestate=None,
            headers=message.extensions(),
        ).model_dump()
        event_data = message.data()

    elif isinstance(message, dict):
        metadata = EventMessageMetadata(
            id=message.get("id"),
            datacontenttype=message.get("datacontenttype"),
            pubsubname=message.get("pubsubname"),
            source=message.get("source"),
            specversion=message.get("specversion"),
            time=message.get("time"),
            topic=message.get("topic"),
            traceid=message.get("traceid"),
            traceparent=message.get("traceparent"),
            type=message.get("type"),
            tracestate=message.get("tracestate"),
            headers=message.get("extensions", {}),
        ).model_dump()
        event_data = message.get("data", {})

    else:
        raise ValueError(f"Unexpected message type: {type(message)}")

    return event_data, metadata


def validate_message_model(model: Type[Any], event_data: dict) -> Any:
    """
    Validates and parses event data against the provided message model.

    Args:
        model (Type[Any]): The message model class.
        event_data (dict): The raw event payload data.

    Returns:
        Any: An instance of the message model (or raw dict if `model` is `dict`).

    Raises:
        TypeError: If the model is not supported.
        ValueError: If model validation fails.
    """
    if not is_supported_model(model):
        raise TypeError(f"Unsupported model type: {model}")

    try:
        logger.info(f"Validating payload with model '{model.__name__}'...")

        if model is dict:
            return event_data
        elif is_dataclass(model):
            return model(**event_data)
        elif is_pydantic_model(model):
            return model.model_validate(event_data).model_dump()

    except Exception as e:
        logger.error(f"Message validation failed for model '{model.__name__}': {e}")
        raise ValueError(f"Message validation failed: {e}")


def parse_cloudevent(
    message: Union[SubscriptionMessage, dict], model: Optional[Type[Any]] = None
) -> Tuple[Any, dict]:
    """
    Parses and validates a CloudEvent from a SubscriptionMessage or dict.

    This combines both metadata extraction and message model validation for direct use.

    Args:
        message (Union[SubscriptionMessage, dict]): The incoming pub/sub message.
        model (Optional[Type[Any]]): The schema used to validate the message body.

    Returns:
        Tuple[Any, dict]: The validated message (or raw dict) and its metadata.

    Raises:
        ValueError: If metadata or validation fails.
    """
    try:
        event_data, metadata = extract_cloudevent_data(message)

        if model is None:
            raise ValueError("Message validation failed: No model provided.")

        validated_message = validate_message_model(model, event_data)

        logger.info("Message successfully parsed and validated")
        logger.debug(f"Data: {validated_message}")
        logger.debug(f"metadata: {metadata}")

        return validated_message, metadata

    except Exception as e:
        logger.error(f"Failed to parse CloudEvent: {e}", exc_info=True)
        raise ValueError(f"Invalid CloudEvent: {str(e)}")

================
File: workflow/messaging/pubsub.py
================
import logging
import json
from dataclasses import is_dataclass, asdict
from typing import Optional, Any, Dict, Union
from pydantic import BaseModel, Field
from dapr.aio.clients import DaprClient

logger = logging.getLogger(__name__)


class DaprPubSub(BaseModel):
    """
    Dapr-based implementation of pub/sub messaging.
    """

    message_bus_name: str = Field(
        ...,
        description="The name of the message bus component, defining the pub/sub base.",
    )

    async def serialize_message(self, message: Any) -> str:
        """
        Serializes a message to JSON format.

        Args:
            message (Any): The message content to serialize.

        Returns:
            str: JSON string of the message.

        Raises:
            ValueError: If the message is not serializable.
        """
        try:
            return json.dumps(message if message is not None else {})
        except TypeError as te:
            logger.error(f"Failed to serialize message: {message}. Error: {te}")
            raise ValueError(f"Message contains non-serializable data: {te}")

    async def publish_message(
        self,
        pubsub_name: str,
        topic_name: str,
        message: Any,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        Publishes a message to a specific topic with optional metadata.

        Args:
            pubsub_name (str): The pub/sub component to use.
            topic_name (str): The topic to publish the message to.
            message (Any): The message content, can be None or any JSON-serializable type.
            metadata (Optional[Dict[str, Any]]): Additional metadata to include in the publish event.

        Raises:
            ValueError: If the message contains non-serializable data.
            Exception: If publishing the message fails.
        """
        try:
            json_message = await self.serialize_message(message)

            async with DaprClient() as client:
                await client.publish_event(
                    pubsub_name=pubsub_name or self.message_bus_name,
                    topic_name=topic_name,
                    data=json_message,
                    data_content_type="application/json",
                    publish_metadata=metadata or {},
                )

            logger.debug(
                f"Message successfully published to topic '{topic_name}' on pub/sub '{pubsub_name}'."
            )
            logger.debug(f"Serialized Message: {json_message}, Metadata: {metadata}")
        except Exception as e:
            logger.error(
                f"Error publishing message to topic '{topic_name}' on pub/sub '{pubsub_name}'. "
                f"Message: {message}, Metadata: {metadata}, Error: {e}"
            )
            raise Exception(
                f"Failed to publish message to topic '{topic_name}' on pub/sub '{pubsub_name}': {str(e)}"
            )

    async def publish_event_message(
        self,
        topic_name: str,
        pubsub_name: str,
        source: str,
        message: Union[BaseModel, dict, Any],
        message_type: Optional[str] = None,
        **kwargs,
    ) -> None:
        """
        Publishes an event message to a specified topic with dynamic metadata.

        Args:
            topic_name (str): The topic to publish the message to.
            pubsub_name (str): The pub/sub component to use.
            source (str): The source of the message (e.g., service or agent name).
            message (Union[BaseModel, dict, dataclass, Any]): The message content, as a Pydantic model, dictionary, or dataclass instance.
            message_type (Optional[str]): The type of the message. Required if `message` is a dictionary.
            **kwargs: Additional metadata fields to include in the message.
        """
        if isinstance(message, BaseModel):
            message_type = message_type or message.__class__.__name__
            message_dict = message.model_dump()

        elif isinstance(message, dict):
            if not message_type:
                raise ValueError(
                    "message_type must be provided when message is a dictionary."
                )
            message_dict = message

        elif is_dataclass(message):
            message_type = message_type or message.__class__.__name__
            message_dict = asdict(message)

        else:
            raise ValueError(
                "Message must be a Pydantic BaseModel, a dictionary, or a dataclass instance."
            )

        metadata = {
            "cloudevent.type": message_type,
            "cloudevent.source": source,
        }
        metadata.update(kwargs)

        logger.debug(
            f"{source} preparing to publish '{message_type}' to topic '{topic_name}'."
        )
        logger.debug(f"Message: {message_dict}, Metadata: {metadata}")

        await self.publish_message(
            topic_name=topic_name,
            pubsub_name=pubsub_name or self.message_bus_name,
            message=message_dict,
            metadata=metadata,
        )

        logger.info(f"{source} published '{message_type}' to topic '{topic_name}'.")

================
File: workflow/messaging/routing.py
================
import asyncio
import inspect
import logging
import threading
import functools
from typing import Callable

from dapr.aio.clients.grpc.subscription import Subscription
from dapr.clients.grpc._response import TopicEventResponse
from dapr.clients.grpc.subscription import StreamInactiveError
from dapr.common.pubsub.subscription import StreamCancelledError, SubscriptionMessage
from dapr_agents.workflow.messaging.parser import (
    extract_cloudevent_data,
    validate_message_model,
)
from dapr_agents.workflow.messaging.utils import is_valid_routable_model
from dapr_agents.workflow.utils import get_decorated_methods

logger = logging.getLogger(__name__)


class MessageRoutingMixin:
    """
    Mixin class providing dynamic message routing capabilities for agentic services using Dapr pub/sub.

    This mixin enables:
    - Auto-registration of message handlers via the `@message_router` decorator.
    - CloudEvent-based dispatch to appropriate handlers based on `type`.
    - Topic subscription management and graceful shutdown.
    - Support for both synchronous and asynchronous handler methods.
    - Workflow-aware message handling for registered workflow entrypoints.

    Expected attributes provided by the consuming service:
    - `self._dapr_client`: A configured Dapr client instance.
    - `self.name`: The agent's name (used for default topic routing).
    - `self.message_bus_name`: Pub/Sub component name in Dapr.
    - `self.broadcast_topic_name`: Optional default topic name for broadcasts.
    - `self._topic_handlers`: Dict storing routing info by (pubsub, topic).
    - `self._subscriptions`: Dict storing unsubscribe functions for active subscriptions.
    """

    def register_message_routes(self) -> None:
        """
        Registers message handlers dynamically by subscribing once per topic.
        Incoming messages are dispatched by CloudEvent `type` to the appropriate handler.

        This function:
        - Scans all class methods for the `@message_router` decorator.
        - Extracts routing metadata and message model schemas.
        - Wraps each handler and maps it by `(pubsub_name, topic_name)` and schema name.
        - Ensures only one handler per schema per topic is allowed.
        """
        message_handlers = get_decorated_methods(self, "_is_message_handler")

        for method_name, method in message_handlers.items():
            try:
                router_data = method._message_router_data.copy()
                pubsub_name = router_data.get("pubsub") or self.message_bus_name
                is_broadcast = router_data.get("is_broadcast", False)
                topic_name = router_data.get("topic") or (
                    self.broadcast_topic_name if is_broadcast else self.name
                )
                message_schemas = router_data.get("message_schemas", [])

                if not message_schemas:
                    raise ValueError(
                        f"No message models found for handler '{method_name}'."
                    )

                wrapped_method = self._create_wrapped_method(method)
                topic_key = (pubsub_name, topic_name)

                self._topic_handlers.setdefault(topic_key, {})

                for schema in message_schemas:
                    if not is_valid_routable_model(schema):
                        raise ValueError(
                            f"Unsupported message model for handler '{method_name}': {schema}"
                        )

                    schema_name = schema.__name__
                    logger.debug(
                        f"Registering handler '{method_name}' for topic '{topic_name}' with model '{schema_name}'"
                    )

                    # Prevent multiple handlers for the same schema
                    if schema_name in self._topic_handlers[topic_key]:
                        raise ValueError(
                            f"Duplicate handler for model '{schema_name}' on topic '{topic_name}'. "
                            f"Each model can only be handled by one function per topic."
                        )

                    self._topic_handlers[topic_key][schema_name] = {
                        "schema": schema,
                        "handler": wrapped_method,
                    }

            except Exception as e:
                logger.error(
                    f"Failed to register handler '{method_name}': {e}", exc_info=True
                )

        # Subscribe once per topic
        for pubsub_name, topic_name in self._topic_handlers.keys():
            self._subscribe_with_router(pubsub_name, topic_name)

        logger.info("All message routes registered.")

    def _create_wrapped_method(self, method: Callable) -> Callable:
        """
        Wraps a message handler method to ensure it runs asynchronously,
        with special handling for workflows.
        """

        @functools.wraps(method)
        async def wrapped_method(message: dict):
            try:
                if getattr(method, "_is_workflow", False):
                    workflow_name = getattr(method, "_workflow_name", method.__name__)
                    instance_id = self.run_workflow(workflow_name, input=message)
                    asyncio.create_task(self.monitor_workflow_completion(instance_id))
                    return None

                if inspect.iscoroutinefunction(method):
                    return await method(message=message)
                else:
                    return method(message=message)

            except Exception as e:
                logger.error(
                    f"Error invoking handler '{method.__name__}': {e}", exc_info=True
                )
                return None

        return wrapped_method

    def _subscribe_with_router(self, pubsub_name: str, topic_name: str):
        subscription: Subscription = self._dapr_client.subscribe(
            pubsub_name, topic_name
        )
        loop = asyncio.get_running_loop()

        def stream_messages(sub: Subscription):
            while True:
                try:
                    for message in sub:
                        if message:
                            try:
                                future = asyncio.run_coroutine_threadsafe(
                                    self._route_message(
                                        pubsub_name, topic_name, message
                                    ),
                                    loop,
                                )
                                response = future.result()
                                sub.respond(message, response.status)
                            except Exception as e:
                                print(f"Error handling message: {e}")
                        else:
                            continue
                except (StreamInactiveError, StreamCancelledError):
                    break

        def close_subscription():
            subscription.close()

        self._subscriptions[(pubsub_name, topic_name)] = close_subscription
        threading.Thread(
            target=stream_messages, args=(subscription,), daemon=True
        ).start()

    async def _route_message(
        self, pubsub_name: str, topic_name: str, message: SubscriptionMessage
    ) -> TopicEventResponse:
        """
        Routes an incoming message to the correct handler based on CloudEvent `type`.

        Args:
            pubsub_name (str): The name of the pubsub component.
            topic_name (str): The topic from which the message was received.
            message (SubscriptionMessage): The incoming Dapr message.

        Returns:
            TopicEventResponse: The response status for the message (success, drop, retry).
        """
        try:
            handler_map = self._topic_handlers.get((pubsub_name, topic_name), {})
            if not handler_map:
                logger.warning(
                    f"No handlers for topic '{topic_name}' on pubsub '{pubsub_name}'. Dropping message."
                )
                return TopicEventResponse("drop")

            # Step 1: Extract CloudEvent metadata and data
            event_data, metadata = extract_cloudevent_data(message)
            event_type = metadata.get("type")

            route_entry = handler_map.get(event_type)
            if not route_entry:
                logger.warning(
                    f"No handler matched CloudEvent type '{event_type}' on topic '{topic_name}'"
                )
                return TopicEventResponse("drop")

            schema = route_entry["schema"]
            handler = route_entry["handler"]

            try:
                parsed_message = validate_message_model(schema, event_data)
                parsed_message["_message_metadata"] = metadata

                logger.info(
                    f"Dispatched to handler '{handler.__name__}' for event type '{event_type}'"
                )
                result = await handler(parsed_message)
                if result is not None:
                    return TopicEventResponse("success"), result

                return TopicEventResponse("success")

            except Exception as e:
                logger.warning(
                    f"Failed to validate message against schema '{schema.__name__}': {e}"
                )
                return TopicEventResponse("retry")

        except Exception as e:
            logger.error(f"Unexpected error during message routing: {e}", exc_info=True)
            return TopicEventResponse("retry")

================
File: workflow/messaging/utils.py
================
from pydantic import BaseModel
from dataclasses import is_dataclass
from typing import Any, Union, get_args, get_origin


def is_pydantic_model(cls: Any) -> bool:
    return isinstance(cls, type) and issubclass(cls, BaseModel)


def is_valid_routable_model(cls: Any) -> bool:
    return is_dataclass(cls) or is_pydantic_model(cls)


def is_supported_model(cls: Any) -> bool:
    """Checks if a class is a supported message schema (Pydantic, dataclass, or dict)."""
    return cls is dict or is_dataclass(cls) or is_pydantic_model(cls)


def extract_message_models(type_hint: Any) -> list[type]:
    """
    Extracts one or more message types from a type hint.

    Supports:
    - Single type hint: `MyMessage`
    - Union types: `Union[MessageA, MessageB]`
    - Fallback to empty list if not valid
    """
    if type_hint is None:
        return []

    origin = get_origin(type_hint)
    if origin is Union:
        return list(get_args(type_hint))
    else:
        return [type_hint]

================
File: workflow/orchestrators/llm/__init__.py
================
from .orchestrator import LLMOrchestrator

================
File: workflow/orchestrators/llm/orchestrator.py
================
import logging
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

from dapr.ext.workflow import DaprWorkflowContext
from dapr_agents.workflow.decorators import task, workflow
from dapr_agents.workflow.messaging.decorator import message_router
from dapr_agents.workflow.orchestrators.base import OrchestratorWorkflowBase
from dapr_agents.workflow.orchestrators.llm.schemas import (
    BroadcastMessage,
    TriggerAction,
    NextStep,
    AgentTaskResponse,
    ProgressCheckOutput,
    schemas,
)
from dapr_agents.workflow.orchestrators.llm.prompts import (
    TASK_INITIAL_PROMPT,
    TASK_PLANNING_PROMPT,
    NEXT_STEP_PROMPT,
    PROGRESS_CHECK_PROMPT,
    SUMMARY_GENERATION_PROMPT,
)
from dapr_agents.workflow.orchestrators.llm.state import (
    LLMWorkflowState,
    LLMWorkflowEntry,
    LLMWorkflowMessage,
    PlanStep,
    TaskResult,
)
from dapr_agents.workflow.orchestrators.llm.utils import (
    update_step_statuses,
    restructure_plan,
    find_step_in_plan,
)

logger = logging.getLogger(__name__)


class LLMOrchestrator(OrchestratorWorkflowBase):
    """
    Implements an agentic workflow where an LLM dynamically selects the next speaker.
    The workflow iterates through conversations, updating its state and persisting messages.

    Uses the `continue_as_new` pattern to restart the workflow with updated input at each iteration.
    """

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes and configures the LLM-based workflow service.
        """

        # Initializes local LLM Orchestrator State
        self.state = LLMWorkflowState()

        # Set main workflow name
        self._workflow_name = "LLMWorkflow"

        super().model_post_init(__context)

    @message_router
    @workflow(name="LLMWorkflow")
    def main_workflow(self, ctx: DaprWorkflowContext, message: TriggerAction):
        """
        Executes an LLM-driven agentic workflow where the next agent is dynamically selected
        based on task progress. The workflow iterates through execution cycles, updating state,
        handling agent responses, and determining task completion.

        Args:
            ctx (DaprWorkflowContext): The workflow execution context.
            message (TriggerAction): The current workflow state containing `message`, `iteration`, and `verdict`.

        Returns:
            str: The final processed message when the workflow terminates.

        Raises:
            RuntimeError: If the LLM determines the task is `failed`.
        """
        # Step 0: Retrieve iteration messages
        task = message.get("task")
        iteration = message.get("iteration")

        # Step 1:
        # Ensure 'instances' and the instance_id entry exist
        instance_id = ctx.instance_id
        self.state.setdefault("instances", {}).setdefault(
            instance_id, LLMWorkflowEntry(input=task).model_dump(mode="json")
        )
        # Retrieve the plan (will always exist after initialization)
        plan = self.state["instances"][instance_id].get("plan", [])

        if not ctx.is_replaying:
            logger.info(
                f"Workflow iteration {iteration + 1} started (Instance ID: {instance_id})."
            )

        # Step 2: Retrieve available agents
        agents = yield ctx.call_activity(self.get_agents_metadata_as_string)

        # Step 3: First iteration setup
        if iteration == 0:
            if not ctx.is_replaying:
                logger.info(f"Initial message from User -> {self.name}")

            # Generate the plan using a language model
            plan = yield ctx.call_activity(
                self.generate_plan,
                input={"task": task, "agents": agents, "plan_schema": schemas.plan},
            )

            # Prepare initial message with task, agents and plan context
            initial_message = yield ctx.call_activity(
                self.prepare_initial_message,
                input={
                    "instance_id": instance_id,
                    "task": task,
                    "agents": agents,
                    "plan": plan,
                },
            )

            # broadcast initial message to all agents
            yield ctx.call_activity(
                self.broadcast_message_to_agents,
                input={"instance_id": instance_id, "task": initial_message},
            )

        # Step 4: Identify agent and instruction for the next step
        next_step = yield ctx.call_activity(
            self.generate_next_step,
            input={
                "task": task,
                "agents": agents,
                "plan": plan,
                "next_step_schema": schemas.next_step,
            },
        )

        # Extract Additional Properties from NextStep
        next_agent = next_step["next_agent"]
        instruction = next_step["instruction"]
        step_id = next_step.get("step", None)
        substep_id = next_step.get("substep", None)

        # Step 5: Validate Step Before Proceeding
        valid_step = yield ctx.call_activity(
            self.validate_next_step,
            input={
                "instance_id": instance_id,
                "plan": plan,
                "step": step_id,
                "substep": substep_id,
            },
        )

        if valid_step:
            # Step 6: Broadcast Task to all Agents
            yield ctx.call_activity(
                self.broadcast_message_to_agents,
                input={"instance_id": instance_id, "task": instruction},
            )

            # Step 7: Trigger next agent
            plan = yield ctx.call_activity(
                self.trigger_agent,
                input={
                    "instance_id": instance_id,
                    "name": next_agent,
                    "step": step_id,
                    "substep": substep_id,
                },
            )

            # Step 8: Wait for agent response or timeout
            if not ctx.is_replaying:
                logger.info(f"Waiting for {next_agent}'s response...")

            event_data = ctx.wait_for_external_event("AgentTaskResponse")
            timeout_task = ctx.create_timer(timedelta(seconds=self.timeout))
            any_results = yield self.when_any([event_data, timeout_task])

            if any_results == timeout_task:
                logger.warning(
                    f"Agent response timed out (Iteration: {iteration + 1}, Instance ID: {instance_id})."
                )
                task_results = {
                    "name": self.name,
                    "role": "user",
                    "content": f"Timeout occurred. {next_agent} did not respond on time. We need to try again...",
                }
            else:
                task_results = yield event_data
                if not ctx.is_replaying:
                    logger.info(f"{task_results['name']} sent a response.")

            # Step 9: Save the task execution results to chat and task history
            yield ctx.call_activity(
                self.update_task_history,
                input={
                    "instance_id": instance_id,
                    "agent": next_agent,
                    "step": step_id,
                    "substep": substep_id,
                    "results": task_results,
                },
            )

            # Step 10: Check progress
            progress = yield ctx.call_activity(
                self.check_progress,
                input={
                    "task": task,
                    "plan": plan,
                    "step": step_id,
                    "substep": substep_id,
                    "results": task_results["content"],
                    "progress_check_schema": schemas.progress_check,
                },
            )

            if not ctx.is_replaying:
                logger.info(f"Tracking Progress: {progress}")

            verdict = progress["verdict"]
            status_updates = progress.get("plan_status_update", [])
            plan_updates = progress.get("plan_restructure", [])

        else:
            logger.warning(
                f"Step {step_id}, Substep {substep_id} not found in plan for instance {instance_id}. Recovering..."
            )

            # Recovery Task: No updates, just iterate again
            verdict = "continue"
            status_updates = []
            plan_updates = []
            task_results = {
                "name": "orchestrator",
                "role": "user",
                "content": f"Step {step_id}, Substep {substep_id} does not exist in the plan. Adjusting workflow...",
            }

        # Step 11: Process progress suggestions and next iteration count
        next_iteration_count = iteration + 1
        if verdict != "continue" or next_iteration_count > self.max_iterations:
            if next_iteration_count >= self.max_iterations:
                verdict = "max_iterations_reached"

            if not ctx.is_replaying:
                logger.info(f"Workflow ending with verdict: {verdict}")

            # Generate final summary based on execution
            summary = yield ctx.call_activity(
                self.generate_summary,
                input={
                    "task": task,
                    "verdict": verdict,
                    "plan": plan,
                    "step": step_id,
                    "substep": substep_id,
                    "agent": next_agent,
                    "result": task_results["content"],
                },
            )

            # Finalize the workflow properly
            yield ctx.call_activity(
                self.finish_workflow,
                input={
                    "instance_id": instance_id,
                    "plan": plan,
                    "step": step_id,
                    "substep": substep_id,
                    "verdict": verdict,
                    "summary": summary,
                },
            )

            if not ctx.is_replaying:
                logger.info(
                    f"Workflow {instance_id} has been finalized with verdict: {verdict}"
                )

            return summary

        if status_updates or plan_updates:
            yield ctx.call_activity(
                self.update_plan,
                input={
                    "instance_id": instance_id,
                    "plan": plan,
                    "status_updates": status_updates,
                    "plan_updates": plan_updates,
                },
            )

        # Step 12: Update TriggerAction state and continue workflow
        message["task"] = task_results["content"]
        message["iteration"] = next_iteration_count

        # Restart workflow with updated TriggerAction state
        ctx.continue_as_new(message)

    @task
    def get_agents_metadata_as_string(self) -> str:
        """
        Retrieves and formats metadata about available agents.

        Returns:
            str: A formatted string listing the available agents and their roles.
        """
        agents_metadata = self.get_agents_metadata(exclude_orchestrator=True)
        if not agents_metadata:
            return "No available agents to assign tasks."

        # Format agent details into a readable string
        agent_list = "\n".join(
            [
                f"- {name}: {metadata.get('role', 'Unknown role')} (Goal: {metadata.get('goal', 'Unknown')})"
                for name, metadata in agents_metadata.items()
            ]
        )

        return agent_list

    @task(description=TASK_PLANNING_PROMPT)
    async def generate_plan(
        self, task: str, agents: str, plan_schema: str
    ) -> List[PlanStep]:
        """
        Generates a structured execution plan for the given task.

        Args:
            task (str): The description of the task to be executed.
            agents (str): Formatted list of available agents and their roles.
            plan_schema (sr): Schema of the plan

        Returns:
            List[Dict[str, Any]]: A list of steps for the overall plan.
        """
        pass

    @task
    async def prepare_initial_message(
        self, instance_id: str, task: str, agents: str, plan: List[Dict[str, Any]]
    ) -> str:
        """
        Initializes the workflow entry and sends the first task briefing to all agents.

        Args:
            instance_id (str): The ID of the workflow instance.
            task (str): The initial input message describing the task.
            agents (str): The formatted list of available agents.
            plan (List[Dict[str, Any]]): The structured execution plan generated beforehand.
        """
        # Format Initial Message with the Plan
        formatted_message = TASK_INITIAL_PROMPT.format(
            task=task, agents=agents, plan=plan
        )

        # Save initial plan using update_workflow_state for consistency
        await self.update_workflow_state(instance_id=instance_id, plan=plan)

        # Return formatted prompt
        return formatted_message

    @task
    async def broadcast_message_to_agents(self, instance_id: str, task: str):
        """
        Saves message to workflow state and broadcasts it to all registered agents.

        Args:
            instance_id (str): Workflow instance ID for context.
            task (str): A task to append to the workflow state and broadcast to all agents.
        """
        # Ensure message is a string
        if not isinstance(task, str):
            raise ValueError("Message must be a string.")

        # Store message in workflow state
        await self.update_workflow_state(
            instance_id=instance_id,
            message={"name": self.name, "role": "user", "content": task},
        )

        # Format message for broadcasting
        task_message = BroadcastMessage(name=self.name, role="user", content=task)

        # Send broadcast message
        await self.broadcast_message(message=task_message, exclude_orchestrator=True)

    @task(description=NEXT_STEP_PROMPT, include_chat_history=True)
    async def generate_next_step(
        self, task: str, agents: str, plan: str, next_step_schema: str
    ) -> NextStep:
        """
        Determines the next agent to respond in a workflow.

        Args:
            task (str): The current task description.
            agents (str): A list of available agents.
            plan (str): The structured execution plan.
            next_step_schema (str): The next step schema.

        Returns:
            Dict: A structured response with the next agent, an instruction, and step ids.
        """
        pass

    @task
    async def validate_next_step(
        self,
        instance_id: str,
        plan: List[Dict[str, Any]],
        step: int,
        substep: Optional[float],
    ) -> bool:
        """
        Validates if the next step exists in the current execution plan.

        Args:
            instance_id (str): The workflow instance ID.
            plan (List[Dict[str, Any]]): The current execution plan.
            step (int): The step number.
            substep (Optional[float]): The substep number.

        Returns:
            bool: True if the step exists, False if it does not.
        """
        step_entry = find_step_in_plan(plan, step, substep)
        if not step_entry:
            logger.error(
                f"Step {step}, Substep {substep} not found in plan for instance {instance_id}."
            )
            return False
        return True

    @task
    async def trigger_agent(
        self, instance_id: str, name: str, step: int, substep: Optional[float]
    ) -> List[dict[str, Any]]:
        """
        Updates step status and triggers the specified agent to perform its activity.

        Args:
            instance_id (str): Workflow instance ID for context.
            name (str): Name of the agent to trigger.
            step (int): The step number associated with the task.
            substep (Optional[float]): The substep number, if applicable.

        Returns:
            List[Dict[str, Any]]: The updated execution plan.
        """
        logger.info(
            f"Triggering agent {name} for step {step}, substep {substep} (Instance ID: {instance_id})"
        )

        # Get the workflow entry from self.state
        workflow_entry = self.state["instances"].get(instance_id)
        if not workflow_entry:
            raise ValueError(f"No workflow entry found for instance_id: {instance_id}")

        plan = workflow_entry["plan"]

        # Ensure step or substep exists
        step_entry = find_step_in_plan(plan, step, substep)
        if not step_entry:
            if substep is not None:
                raise ValueError(
                    f"Substep {substep} in Step {step} not found in the current plan."
                )
            raise ValueError(f"Step {step} not found in the current plan.")

        # Mark step or substep as "in_progress"
        step_entry["status"] = "in_progress"
        logger.info(f"Marked step {step}, substep {substep} as 'in_progress'")

        # Apply global status updates to maintain consistency
        updated_plan = update_step_statuses(plan)

        # Save updated plan state
        await self.update_workflow_state(instance_id=instance_id, plan=updated_plan)

        # Send message to agent
        await self.send_message_to_agent(
            name=name, message=TriggerAction(workflow_instance_id=instance_id)
        )

        return updated_plan

    @task
    async def update_task_history(
        self,
        instance_id: str,
        agent: str,
        step: int,
        substep: Optional[float],
        results: Dict[str, Any],
    ):
        """
        Updates the task history for a workflow instance by recording the results of an agent's execution.

        Args:
            instance_id (str): The unique workflow instance ID.
            agent (str): The name of the agent who performed the task.
            step (int): The step number associated with the task.
            substep (Optional[float]): The substep number, if applicable.
            results (Dict[str, Any]): The result or response generated by the agent.

        Raises:
            ValueError: If the instance ID does not exist in the workflow state.
        """

        logger.info(
            f"Updating task history for {agent} at step {step}, substep {substep} (Instance ID: {instance_id})"
        )

        # Store the agent's response in the message history
        await self.update_workflow_state(instance_id=instance_id, message=results)

        # Retrieve Workflow state
        workflow_entry = self.state["instances"].get(instance_id)
        if not workflow_entry:
            raise ValueError(f"No workflow entry found for instance_id: {instance_id}")

        # Create a TaskResult object
        task_result = TaskResult(
            agent=agent, step=step, substep=substep, result=results["content"]
        )

        # Append the result to task history
        workflow_entry["task_history"].append(task_result.model_dump(mode="json"))

        # Persist state
        await self.update_workflow_state(
            instance_id=instance_id, plan=workflow_entry["plan"]
        )

    @task(description=PROGRESS_CHECK_PROMPT, include_chat_history=True)
    async def check_progress(
        self,
        task: str,
        plan: str,
        step: int,
        substep: Optional[float],
        results: str,
        progress_check_schema: str,
    ) -> ProgressCheckOutput:
        """
        Evaluates the current plan's progress and determines necessary updates.

        Args:
            task (str): The current task description.
            plan (str): The structured execution plan.
            step (int): The step number associated with the task.
            substep (Optional[float]): The substep number, if applicable.
            results (str): The result or response generated by the agent.
            progress_check_schema (str): The schema of the progress check

        Returns:
            ProgressCheckOutput: The plan update details, including status changes and restructuring if needed.
        """
        pass

    @task
    async def update_plan(
        self,
        instance_id: str,
        plan: List[Dict[str, Any]],
        status_updates: Optional[List[Dict[str, Any]]] = None,
        plan_updates: Optional[List[Dict[str, Any]]] = None,
    ):
        """
        Updates the execution plan based on status changes and/or plan restructures.

        Args:
            instance_id (str): The workflow instance ID.
            plan (List[Dict[str, Any]]): The current execution plan.
            status_updates (Optional[List[Dict[str, Any]]]): List of updates for step statuses.
            plan_updates (Optional[List[Dict[str, Any]]]): List of full step modifications.

        Raises:
            ValueError: If a specified step or substep is not found.
        """
        logger.info(f"Updating plan for instance {instance_id}")

        # Step 1: Apply status updates directly to `plan`
        if status_updates:
            for update in status_updates:
                step_id = update["step"]
                substep_id = update.get("substep")
                new_status = update["status"]

                step_entry = find_step_in_plan(plan, step_id, substep_id)
                if not step_entry:
                    error_msg = f"Step {step_id}, Substep {substep_id} not found in the current plan."
                    logger.error(error_msg)
                    raise ValueError(error_msg)

                # Apply status update
                step_entry["status"] = new_status
                logger.info(
                    f"Updated status of step {step_id}, substep {substep_id} to '{new_status}'"
                )

        # Step 2: Apply plan restructuring updates (if provided)
        if plan_updates:
            plan = restructure_plan(plan, plan_updates)
            logger.info(f"Applied restructuring updates for {len(plan_updates)} steps.")

        # Step 3: Apply global consistency checks for statuses
        plan = update_step_statuses(plan)

        # Save to state and update workflow
        await self.update_workflow_state(instance_id=instance_id, plan=plan)

        logger.info(f"Plan successfully updated for instance {instance_id}")

    @task(description=SUMMARY_GENERATION_PROMPT, include_chat_history=True)
    async def generate_summary(
        self,
        task: str,
        verdict: str,
        plan: str,
        step: int,
        substep: Optional[float],
        agent: str,
        result: str,
    ) -> str:
        """
        Generates a structured summary of task execution based on conversation history, execution results, and the task plan.

        Args:
            task (str): The original task description.
            verdict (str): The overall task status (e.g., "continue", "completed", or "failed").
            plan (str): The structured execution plan detailing task progress.
            step (int): The step number associated with the most recent action.
            substep (Optional[float]): The substep number, if applicable.
            agent (str): The name of the agent who executed the last action.
            result (str): The response or outcome generated by the agent.

        Returns:
            str: A concise but informative summary of task progress and results, structured for user readability.
        """
        pass

    @task
    async def finish_workflow(
        self,
        instance_id: str,
        plan: List[Dict[str, Any]],
        step: int,
        substep: Optional[float],
        verdict: str,
        summary: str,
    ):
        """
        Finalizes the workflow by updating the plan, marking the provided step/substep as completed if applicable,
        and storing the summary and verdict.

        Args:
            instance_id (str): The workflow instance ID.
            plan (List[Dict[str, Any]]): The current execution plan.
            step (int): The step that was last worked on.
            substep (Optional[float]): The substep that was last worked on (if applicable).
            verdict (str): The final workflow verdict (`completed`, `failed`, or `max_iterations_reached`).
            summary (str): The generated summary of the workflow execution.

        Returns:
            None
        """
        status_updates = []

        if verdict == "completed":
            # Find and validate the step or substep
            step_entry = find_step_in_plan(plan, step, substep)
            if not step_entry:
                raise ValueError(
                    f"Step {step}, Substep {substep} not found in the current plan. Cannot mark as completed."
                )

            # Mark the step or substep as completed
            step_entry["status"] = "completed"
            status_updates.append(
                {"step": step, "substep": substep, "status": "completed"}
            )

            # If it's a substep, check if all sibling substeps are completed
            parent_step = find_step_in_plan(
                plan, step
            )  # Retrieve parent without `substep`
            if parent_step:
                # Ensure "substeps" is a valid list before iteration
                if not isinstance(parent_step.get("substeps"), list):
                    parent_step["substeps"] = []

                all_substeps_completed = all(
                    ss.get("status") == "completed" for ss in parent_step["substeps"]
                )
                if all_substeps_completed:
                    parent_step["status"] = "completed"
                    status_updates.append({"step": step, "status": "completed"})

        # Apply updates in one call
        if status_updates:
            await self.update_plan(
                instance_id=instance_id, plan=plan, status_updates=status_updates
            )

        # Store the final summary and verdict in workflow state
        await self.update_workflow_state(instance_id=instance_id, final_output=summary)

    async def update_workflow_state(
        self,
        instance_id: str,
        message: Optional[Dict[str, Any]] = None,
        final_output: Optional[str] = None,
        plan: Optional[List[Dict[str, Any]]] = None,
    ):
        """
        Updates the workflow state with a new message, execution plan, or final output.

        Args:
            instance_id (str): The unique identifier of the workflow instance.
            message (Optional[Dict[str, Any]]): A structured message to be added to the workflow state.
            final_output (Optional[str]): The final result of the workflow execution.
            plan (Optional[List[Dict[str, Any]]]): The execution plan associated with the workflow instance.

        Raises:
            ValueError: If the workflow instance ID is not found in the local state.
        """
        workflow_entry = self.state["instances"].get(instance_id)
        if not workflow_entry:
            raise ValueError(
                f"No workflow entry found for instance_id {instance_id} in local state."
            )

        # Only update the provided fields
        if plan is not None:
            workflow_entry["plan"] = plan
        if message is not None:
            serialized_message = LLMWorkflowMessage(**message).model_dump(mode="json")

            # Update workflow state messages
            workflow_entry["messages"].append(serialized_message)
            workflow_entry["last_message"] = serialized_message

            # Update the local chat history
            self.memory.add_message(message)

        if final_output is not None:
            workflow_entry["output"] = final_output
            workflow_entry["end_time"] = datetime.now().isoformat()

        # Persist updated state
        self.save_state()

    @message_router
    async def process_agent_response(self, message: AgentTaskResponse):
        """
        Processes agent response messages sent directly to the agent's topic.

        Args:
            message (AgentTaskResponse): The agent's response containing task results.

        Returns:
            None: The function raises a workflow event with the agent's response.
        """
        try:
            workflow_instance_id = message.get("workflow_instance_id")

            if not workflow_instance_id:
                logger.error(
                    f"{self.name} received an agent response without a valid workflow_instance_id. Ignoring."
                )
                return

            logger.info(
                f"{self.name} processing agent response for workflow instance '{workflow_instance_id}'."
            )

            # Raise a workflow event with the Agent's Task Response
            self.raise_workflow_event(
                instance_id=workflow_instance_id,
                event_name="AgentTaskResponse",
                data=message,
            )

        except Exception as e:
            logger.error(f"Error processing agent response: {e}", exc_info=True)

================
File: workflow/orchestrators/llm/prompts.py
================
TASK_PLANNING_PROMPT = """## Task Overview
We need to develop a structured, **logically ordered** plan to accomplish the following task:

{task}

### Team of Agents
{agents}

### Planning Instructions:
1. **Break the task into a structured, step-by-step plan** ensuring logical sequencing.
2. **Each step should be described in a neutral, objective manner**, focusing on the action required rather than specifying who performs it.
3. **The plan should be designed to be executed by the provided team**, but **DO NOT reference specific agents** in step descriptions.
4. **Do not introduce additional roles, skills, or external resources** outside of the given team.
5. **Use clear and precise descriptions** for each step, focusing on **what** needs to be done rather than **who** does it.
6. **Avoid unnecessary steps or excessive granularity**. Keep the breakdown clear and efficient.
7. **If a task involves both code generation and execution, structure the plan as follows:**
   - **Code Generation Step:** Describe what code needs to be generated, specifying language and functionality.
   - **Code Execution Step:** If execution is required, describe how it should be performed separately.
   - **Feedback Handling Step:** If execution results need analysis, include a step for refining the code.
8. **Maintain a natural task flow** where each step logically follows the previous one.
9. **Each step must have a `status` field** to track progress (Initially all set to `not_started`).
10. **Sub-steps should be included only if a step requires multiple distinct actions.**
11. **Focus only on structuring the task execution**, NOT on assigning agents at this stage.

### Expected Output Format (JSON Schema):
{plan_schema}
"""

TASK_INITIAL_PROMPT = """## Mission Briefing

We have received the following task:

{task}

### Team of Agents
{agents}

### Execution Plan
Here is the structured approach the team will follow to accomplish the task:

{plan}
"""

NEXT_STEP_PROMPT = """## Task Context

The team is working on the following task:

{task}

### Team of Agents (ONLY these agents are available):
{agents}

### Current Execution Plan:
{plan}

### Next Steps:
- **Select the next best-suited agent** from the team of agents list who should respond **based on the execution plan above**.
- **DO NOT select an agent that is not explicitly listed in `{agents}`**.
- **You must always provide a valid agent name** from the team**. DO NOT return `null` or an empty agent name**.
- Provide a **clear, actionable instruction** for the next agent.
- **You must ONLY select step and substep IDs that EXIST in the plan.**
  - **DO NOT select a `"completed"` step or substep.**
  - **If the main step is `"not_started"` but has `"completed"` substeps, you must correctly identify the next `"not_started"` substep.**
  - **DO NOT create or assume non-existent step/substep IDs.**
  - **DO NOT reference any invalid step/substep identifiers. Always check the plan.**

### Expected Output Format (JSON Schema):
{next_step_schema}
"""

PROGRESS_CHECK_PROMPT = """## Progress Check

### Task Context
The team is working on the following task:

{task}

### Current Execution Plan:

{plan}

### Latest Execution Context:
- **Step ID:** {step}
- **Substep ID (if applicable):** {substep}
- **Step Execution Results:** "{results}"

### Task Evaluation:
Assess the task progress based on **conversation history**, execution results, and the structured plan.

1. **Determine Overall Task Verdict**
   - `"continue"` → **Use this if there are `"not_started"` or `"in_progress"` steps that still require execution.**
   - `"completed"` → The task is **done** (i.e., **all required steps and substeps have been completed**).
   - `"failed"` → The task cannot be completed due to an unresolved issue.

2. **Evaluate Step Completion**
   - If an **agent explicitly marks a step as `"completed"`**, then it **remains completed**, regardless of substeps.
   - If a **substep is completed**, check if **all** substeps are `"completed"` **before marking the parent step as "completed"**.
   - **If a step is "completed" but has "not_started" substeps, DO NOT modify those substeps.** They remain unchanged unless explicitly acted upon.

3. **Update Step & Sub-Step Status**
   - **Always update statuses based on the latest results**, regardless of whether the verdict is `"continue"` or `"completed"`.
   - If an **agent explicitly marks a step as `"completed"`**, then it **remains completed**, regardless of substeps.
   - If a **substep is completed**, check if **all** substeps are `"completed"` **before marking the parent step as "completed"**.
   - **If a step is "completed" but has "not_started" substeps, DO NOT modify those substeps.** They remain unchanged unless explicitly acted upon.

4. **Plan Adjustments (Only If Necessary)**
   - If the step descriptions are **unclear or incomplete**, update `"plan_restructure"` with a **single modified step**.
   - Do **not** introduce unnecessary modifications.

### Important:
- **Do NOT mark a step as `"completed"` unless explicitly confirmed based on execution results.**
- **Do NOT mark substeps as `"completed"` unless explicitly confirmed or all are already completed.**
- **Always apply step/substep status updates, even if the task is `"completed"`**.
- **Do not introduce unnecessary modifications to the plan.**

### Expected Output Format (JSON Schema):
{progress_check_schema}
"""

SUMMARY_GENERATION_PROMPT = """# Summary Generator

## Initial Task:
{task}

## Execution Overview:
- **Final Verdict:** {verdict}
  _(Possible values: `"continue"`, `"completed"`, `"failed"` `max_iterations_reached`)_
- **Execution Plan Status:**
  {plan}
- **Last Action Taken:**
  - **Step:** `{step}` (Sub-step `{substep}` if applicable)
  - **Executing Agent:** `{agent}`
  - **Execution Result:** {result}

## Instructions to Generate Best Summary
Based on the **conversation history** and **execution plan**, generate a **clear and structured** summary:

1. **If the task is `"completed"`**, provide a concise but complete final summary.
   - **Briefly describe the key steps taken** and the final outcome.
   - Ensure clarity, avoiding excessive details while **focusing on essential takeaways**.
   - Phrase it **as if reporting back to the user** rather than as a system log.

2. **If the task is `"failed"`**, explain why.
   - Summarize blockers, unresolved challenges, or missing steps.
   - If applicable, suggest potential next actions to resolve the issue.

3. **If the task is `"continue"`**, summarize progress so far.
   - **Highlight completed steps** and the current state of execution.
   - **Mention what remains unfinished** and the next logical step.
   - Keep it informative but **concise and forward-looking**.

4. **If the task is `"max_iterations_reached"`**, summarize the progress and note the limitation.
   - Highlight what has been **achieved so far** and what **remains unfinished**.
   - Clearly state that the **workflow reached its iteration limit** before completion.
   - If possible, **suggest next steps** (e.g., refining the execution plan, restarting the task, or adjusting agent strategies).

## Expected Output
A structured summary that is:
- **Clear and to the point**
- **Context-aware (includes results and execution progress)**
- **User-friendly (reads naturally rather than like system logs)**
- **Relevant (avoids unnecessary details while maintaining accuracy)**
"""

================
File: workflow/orchestrators/llm/schemas.py
================
from functools import cached_property
import json
from typing import List, Optional, Literal
from pydantic import BaseModel, Field

from dapr_agents.workflow.orchestrators.llm.state import PlanStep
from dapr_agents.types.message import BaseMessage
from dapr_agents.llm.utils import StructureHandler


class BroadcastMessage(BaseMessage):
    """
    Represents a broadcast message from an agent.
    """


class AgentTaskResponse(BaseMessage):
    """
    Represents a response message from an agent after completing a task.
    """

    workflow_instance_id: Optional[str] = Field(
        default=None, description="Dapr workflow instance id from source if available"
    )


class TriggerAction(BaseModel):
    """
    Represents a message used to trigger an agent's activity within the workflow.
    """

    task: Optional[str] = Field(
        None,
        description="The specific task to execute. If not provided, the agent can act based on its memory or predefined behavior.",
    )
    iteration: Optional[int] = Field(
        default=0, description="The current iteration of the workflow loop."
    )
    workflow_instance_id: Optional[str] = Field(
        default=None, description="Dapr workflow instance id from source if available"
    )


class NextStep(BaseModel):
    """
    Represents the next step in a workflow, including the next agent to respond,
    an instruction message for that agent and the step id and substep id if applicable.
    """

    next_agent: str = Field(
        ..., description="The name of the agent selected to respond next."
    )
    instruction: str = Field(
        ..., description="A direct message instructing the agent on their next action."
    )
    step: int = Field(..., description="The step number the agent will be working on.")
    substep: Optional[float] = Field(
        None,
        description="The substep number (if applicable) the agent will be working on.",
    )


class TaskPlan(BaseModel):
    """Encapsulates the structured execution plan."""

    plan: List[PlanStep] = Field(..., description="Structured execution plan.")


class PlanStatusUpdate(BaseModel):
    step: int = Field(..., description="Step identifier (integer).")
    substep: Optional[float] = Field(
        None,
        description="Substep identifier (float, e.g., 1.1, 2.3). Set to None if updating a step.",
    )
    status: Literal["not_started", "in_progress", "blocked", "completed"] = Field(
        ..., description="Updated status for the step or sub-step."
    )


class ProgressCheckOutput(BaseModel):
    verdict: Literal["continue", "completed", "failed"] = Field(
        ...,
        description="Task status: 'continue' (in progress), 'completed' (done), or 'failed' (unresolved issue).",
    )
    plan_needs_update: bool = Field(
        ..., description="Indicates whether the plan requires updates (true/false)."
    )
    plan_status_update: Optional[List[PlanStatusUpdate]] = Field(
        None,
        description="List of status updates for steps or sub-steps. Each entry must contain `step`, optional `substep`, and `status`.",
    )
    plan_restructure: Optional[List[PlanStep]] = Field(
        None,
        description="A list of restructured steps. Only one step should be modified at a time.",
    )


# Schemas used in Prompts
class Schemas:
    """Lazily evaluated JSON schemas used in prompt calls."""

    @cached_property
    def plan(self) -> str:
        return json.dumps(
            StructureHandler.enforce_strict_json_schema(TaskPlan.model_json_schema())[
                "properties"
            ]["plan"]
        )

    @cached_property
    def progress_check(self) -> str:
        return json.dumps(
            StructureHandler.enforce_strict_json_schema(
                ProgressCheckOutput.model_json_schema()
            )
        )

    @cached_property
    def next_step(self) -> str:
        return json.dumps(NextStep.model_json_schema())


schemas = Schemas()

================
File: workflow/orchestrators/llm/state.py
================
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Literal
from datetime import datetime
import uuid


class TaskResult(BaseModel):
    """
    Represents the result of an agent's task execution.
    """

    agent: str = Field(..., description="The agent who executed the task.")
    step: int = Field(..., description="The step number associated with the task.")
    substep: Optional[float] = Field(
        None, description="The substep number (if applicable)."
    )
    result: str = Field(
        ..., description="The response or outcome of the task execution."
    )
    timestamp: datetime = Field(
        default_factory=datetime.now,
        description="Timestamp of when the result was recorded.",
    )


class SubStep(BaseModel):
    substep: float = Field(
        ..., description="Substep identifier (float, e.g., 1.1, 2.3)."
    )
    description: str = Field(..., description="Detailed action to be performed.")
    status: Literal["not_started", "in_progress", "blocked", "completed"] = Field(
        ..., description="Current state of the sub-step."
    )


class PlanStep(BaseModel):
    step: int = Field(..., description="Step identifier (integer).")
    description: str = Field(..., description="Detailed action to be performed.")
    status: Literal["not_started", "in_progress", "blocked", "completed"] = Field(
        ..., description="Current state of the step."
    )
    substeps: Optional[List[SubStep]] = Field(
        None, description="Optional list of sub-steps."
    )


class LLMWorkflowMessage(BaseModel):
    """Represents a message exchanged within the workflow."""

    id: str = Field(
        default_factory=lambda: str(uuid.uuid4()),
        description="Unique identifier for the message",
    )
    role: str = Field(
        ..., description="The role of the message sender, e.g., 'user' or 'assistant'"
    )
    content: str = Field(..., description="Content of the message")
    timestamp: datetime = Field(
        default_factory=datetime.now,
        description="Timestamp when the message was created",
    )
    name: Optional[str] = Field(
        default=None,
        description="Optional name of the assistant or user sending the message",
    )


class LLMWorkflowEntry(BaseModel):
    """Represents a workflow and its associated data."""

    input: str = Field(
        ..., description="The input or description of the Workflow to be performed"
    )
    output: Optional[str] = Field(
        None, description="The output or result of the Workflow, if completed"
    )
    start_time: datetime = Field(
        default_factory=datetime.now,
        description="Timestamp when the workflow was started",
    )
    end_time: Optional[datetime] = Field(
        None, description="Timestamp when the workflow was completed or failed"
    )
    messages: List[LLMWorkflowMessage] = Field(
        default_factory=list, description="Messages exchanged during the workflow"
    )
    last_message: Optional[LLMWorkflowMessage] = Field(
        default=None, description="Last processed message in the workflow"
    )
    plan: Optional[List[PlanStep]] = Field(
        None, description="Structured execution plan for the workflow."
    )
    task_history: List[TaskResult] = Field(
        default_factory=list,
        description="A history of task executions and their results.",
    )


class LLMWorkflowState(BaseModel):
    """Represents the state of multiple LLM workflows."""

    instances: Dict[str, LLMWorkflowEntry] = Field(
        default_factory=dict,
        description="Workflow entries indexed by their instance_id.",
    )

================
File: workflow/orchestrators/llm/utils.py
================
from typing import List, Dict, Any, Optional


def update_step_statuses(plan: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Ensures step and sub-step statuses follow logical progression:
    - A step is marked "completed" if all substeps are "completed".
    - If any sub-step is "in_progress", the parent step must also be "in_progress".
    - If a sub-step is "completed" but the parent step is "not_started", update it to "in_progress".
    - If a parent step is "completed" but a substep is still "in_progress", downgrade it to "in_progress".
    - Steps without substeps should still progress logically.

    Args:
        plan (List[Dict[str, Any]]): The current execution plan.

    Returns:
        List[Dict[str, Any]]: The updated execution plan with correct statuses.
    """
    for step in plan:
        # Case 0: Handle steps that have NO substeps
        if "substeps" not in step or not step["substeps"]:
            if step["status"] == "not_started":
                step[
                    "status"
                ] = "in_progress"  # Independent steps should start when execution begins
            continue  # Skip further processing if no substeps exist

        substep_statuses = {ss["status"] for ss in step["substeps"]}

        # Case 1: If ALL substeps are "completed", parent step must be "completed".
        if all(status == "completed" for status in substep_statuses):
            step["status"] = "completed"

        # Case 2: If ANY substep is "in_progress", parent step must also be "in_progress".
        elif "in_progress" in substep_statuses:
            step["status"] = "in_progress"

        # Case 3: If a sub-step was completed but the step is still "not_started", update it.
        elif "completed" in substep_statuses and step["status"] == "not_started":
            step["status"] = "in_progress"

        # Case 4: If the step is already marked as "completed" but a substep is still "in_progress", downgrade it.
        elif step["status"] == "completed" and "in_progress" in substep_statuses:
            step["status"] = "in_progress"

    return plan


def validate_plan_structure(plan: List[Dict[str, Any]]) -> bool:
    """
    Validates if the plan structure follows the correct schema.

    Args:
        plan (List[Dict[str, Any]]): The execution plan.

    Returns:
        bool: True if the plan structure is valid, False otherwise.
    """
    required_keys = {"step", "description", "status"}
    for step in plan:
        if not required_keys.issubset(step.keys()):
            return False
        if "substeps" in step:
            for substep in step["substeps"]:
                if not {"substep", "description", "status"}.issubset(substep.keys()):
                    return False
    return True


def find_step_in_plan(
    plan: List[Dict[str, Any]], step: int, substep: Optional[float] = None
) -> Optional[Dict[str, Any]]:
    """
    Finds a specific step or substep in a plan.

    Args:
        plan (List[Dict[str, Any]]): The execution plan.
        step (int): The step number to find.
        substep (Optional[float]): The substep number (if applicable).

    Returns:
        Dict[str, Any] | None: The found step/substep dictionary or None if not found.
    """
    for step_entry in plan:
        if step_entry["step"] == step:
            if substep is None:
                return step_entry

            for sub in step_entry.get("substeps", []):
                if sub["substep"] == substep:
                    return sub
    return None


def restructure_plan(
    plan: List[Dict[str, Any]], updates: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Applies restructuring updates to the task execution plan.

    Args:
        plan (List[Dict[str, Any]]): The current execution plan.
        updates (List[Dict[str, Any]]): A list of updates to apply.

    Returns:
        List[Dict[str, Any]]: The updated execution plan.
    """
    for update in updates:
        step_id = update["step"]
        step_entry = find_step_in_plan(plan, step_id)
        if step_entry:
            step_entry.update(update)

    return plan

================
File: workflow/orchestrators/__init__.py
================
from .base import OrchestratorWorkflowBase
from .llm import LLMOrchestrator
from .random import RandomOrchestrator
from .roundrobin import RoundRobinOrchestrator

================
File: workflow/orchestrators/base.py
================
from abc import ABC, abstractmethod
from dapr_agents.workflow.agentic import AgenticWorkflow
from pydantic import Field, model_validator
import logging
from typing import Any, Optional
from dapr.ext.workflow import DaprWorkflowContext

logger = logging.getLogger(__name__)


class OrchestratorWorkflowBase(AgenticWorkflow, ABC):
    orchestrator_topic_name: Optional[str] = Field(
        None,
        description="The topic name dedicated to this specific orchestrator, derived from the orchestrator's name if not provided.",
    )

    @model_validator(mode="before")
    def set_orchestrator_topic_name(cls, values: dict):
        # Derive orchestrator_topic_name from agent name
        if not values.get("orchestrator_topic_name") and values.get("name"):
            values["orchestrator_topic_name"] = values["name"]

        return values

    def model_post_init(self, __context: Any) -> None:
        """
        Register agentic workflow.
        """

        # Complete post-initialization
        super().model_post_init(__context)

        # Prepare agent metadata
        self._agent_metadata = {
            "name": self.name,
            "topic_name": self.orchestrator_topic_name,
            "pubsub_name": self.message_bus_name,
            "orchestrator": True,
        }

        # Register agent metadata
        self.register_agentic_system()

    @abstractmethod
    def main_workflow(self, ctx: DaprWorkflowContext, message: Any) -> Any:
        """
        Execute the primary workflow that coordinates agent interactions.

        Args:
            ctx (DaprWorkflowContext): The workflow execution context
            message (Any): The input for this workflow iteration

        Returns:
            Any: The workflow result or continuation
        """
        pass

    @abstractmethod
    async def process_agent_response(self, message: Any) -> None:
        """Process responses from agents."""
        pass

    @abstractmethod
    async def broadcast_message_to_agents(self, **kwargs) -> None:
        """Broadcast a message to all registered agents."""
        pass

    @abstractmethod
    async def trigger_agent(self, name: str, instance_id: str, **kwargs) -> None:
        """Trigger a specific agent to perform an action."""
        pass

================
File: workflow/orchestrators/random.py
================
from dapr_agents.workflow.orchestrators.base import OrchestratorWorkflowBase
from dapr.ext.workflow import DaprWorkflowContext
from dapr_agents.types import BaseMessage
from dapr_agents.workflow.decorators import workflow, task
from dapr_agents.workflow.messaging.decorator import message_router
from typing import Any, Optional, Dict
from datetime import timedelta
from pydantic import BaseModel, Field
import random
import logging


logger = logging.getLogger(__name__)


class AgentTaskResponse(BaseMessage):
    """
    Represents a response message from an agent after completing a task.
    """

    workflow_instance_id: Optional[str] = Field(
        default=None, description="Dapr workflow instance id from source if available"
    )


class TriggerAction(BaseModel):
    """
    Represents a message used to trigger an agent's activity within the workflow.
    """

    task: Optional[str] = Field(
        None,
        description="The specific task to execute. If not provided, the agent will act based on its memory or predefined behavior.",
    )
    iteration: Optional[int] = Field(0, description="")
    workflow_instance_id: Optional[str] = Field(
        default=None, description="Dapr workflow instance id from source if available"
    )


class RandomOrchestrator(OrchestratorWorkflowBase):
    """
    Implements a random workflow where agents are selected randomly to perform tasks.
    The workflow iterates through conversations, selecting a random agent at each step.

    Uses `continue_as_new` to persist iteration state.
    """

    current_speaker: Optional[str] = Field(
        default=None, init=False, description="Current speaker in the conversation."
    )

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes and configures the random workflow service.
        Registers tasks and workflows, then starts the workflow runtime.
        """
        self._workflow_name = "RandomWorkflow"

        super().model_post_init(__context)

    @workflow(name="RandomWorkflow")
    def main_workflow(self, ctx: DaprWorkflowContext, input: TriggerAction):
        """
        Executes a random workflow where agents are selected randomly for interactions.
        Uses `continue_as_new` to persist iteration state.

        Args:
            ctx (DaprWorkflowContext): The workflow execution context.
            input (TriggerAction): The current workflow state containing `message` and `iteration`.

        Returns:
            str: The last processed message when the workflow terminates.
        """
        # Step 0: Retrieving Loop Context
        task = input.get("task")
        iteration = input.get("iteration")
        instance_id = ctx.instance_id

        if not ctx.is_replaying:
            logger.info(
                f"Random workflow iteration {iteration + 1} started (Instance ID: {instance_id})."
            )

        # First iteration: Process input and broadcast
        if iteration == 0:
            message = yield ctx.call_activity(self.process_input, input={"task": task})
            logger.info(f"Initial message from {message['role']} -> {self.name}")

            # Step 1: Broadcast initial message
            yield ctx.call_activity(
                self.broadcast_message_to_agents, input={"message": message}
            )

        # Step 2: Select a random speaker
        random_speaker = yield ctx.call_activity(
            self.select_random_speaker, input={"iteration": iteration}
        )

        # Step 3: Trigger agent
        yield ctx.call_activity(
            self.trigger_agent,
            input={"name": random_speaker, "instance_id": instance_id},
        )

        # Step 4: Wait for response or timeout
        logger.info("Waiting for agent response...")
        event_data = ctx.wait_for_external_event("AgentTaskResponse")
        timeout_task = ctx.create_timer(timedelta(seconds=self.timeout))
        any_results = yield self.when_any([event_data, timeout_task])

        if any_results == timeout_task:
            logger.warning(
                f"Agent response timed out (Iteration: {iteration + 1}, Instance ID: {instance_id})."
            )
            task_results = {
                "name": "timeout",
                "content": "Timeout occurred. Continuing...",
            }
        else:
            task_results = yield event_data
            logger.info(f"{task_results['name']} -> {self.name}")

        # Step 5: Check Iteration
        next_iteration_count = iteration + 1
        if next_iteration_count > self.max_iterations:
            logger.info(
                f"Max iterations reached. Ending random workflow (Instance ID: {instance_id})."
            )
            return task_results["content"]

        # Update ChatLoop for next iteration
        input["task"] = task_results["content"]
        input["iteration"] = iteration + 1

        # Restart workflow with updated state
        ctx.continue_as_new(input)

    @task
    async def process_input(self, task: str):
        """
        Processes the input message for the workflow.

        Args:
            task (str): The user-provided input task.
        Returns:
            dict: Serialized UserMessage with the content.
        """
        return {"role": "user", "name": self.name, "content": task}

    @task
    async def broadcast_message_to_agents(self, message: Dict[str, Any]):
        """
        Broadcasts a message to all agents.

        Args:
            message (Dict[str, Any]): The message content and additional metadata.
        """
        await self.broadcast_message(
            message=BaseMessage(**message), exclude_orchestrator=True
        )

    @task
    def select_random_speaker(self, iteration: int) -> str:
        """
        Selects a random speaker, ensuring that a different agent is chosen if possible.

        Args:
            iteration (int): The current iteration number.
        Returns:
            str: The name of the randomly selected agent.
        """
        agents_metadata = self.get_agents_metadata(exclude_orchestrator=True)
        if not agents_metadata:
            logger.warning("No agents available for selection.")
            raise ValueError(
                "Agents metadata is empty. Cannot select a random speaker."
            )

        agent_names = list(agents_metadata.keys())

        # Handle single-agent scenarios
        if len(agent_names) == 1:
            random_speaker = agent_names[0]
            logger.info(
                f"Only one agent available: {random_speaker}. Using the same agent."
            )
            return random_speaker

        # Select a random speaker, avoiding repeating the previous speaker when possible
        previous_speaker = getattr(self, "current_speaker", None)
        if previous_speaker in agent_names and len(agent_names) > 1:
            agent_names.remove(previous_speaker)

        random_speaker = random.choice(agent_names)
        self.current_speaker = random_speaker
        logger.info(
            f"{self.name} randomly selected agent {random_speaker} (Iteration: {iteration})."
        )
        return random_speaker

    @task
    async def trigger_agent(self, name: str, instance_id: str) -> None:
        """
        Triggers the specified agent to perform its activity.

        Args:
            name (str): Name of the agent to trigger.
            instance_id (str): Workflow instance ID for context.
        """
        logger.info(f"Triggering agent {name} (Instance ID: {instance_id})")

        await self.send_message_to_agent(
            name=name,
            message=TriggerAction(workflow_instance_id=instance_id),
        )

    @message_router
    async def process_agent_response(self, message: AgentTaskResponse):
        """
        Processes agent response messages sent directly to the agent's topic.

        Args:
            message (AgentTaskResponse): The agent's response containing task results.

        Returns:
            None: The function raises a workflow event with the agent's response.
        """
        try:
            workflow_instance_id = message.get("workflow_instance_id")

            if not workflow_instance_id:
                logger.error(
                    f"{self.name} received an agent response without a valid workflow_instance_id. Ignoring."
                )
                return

            logger.info(
                f"{self.name} processing agent response for workflow instance '{workflow_instance_id}'."
            )

            # Raise a workflow event with the Agent's Task Response
            self.raise_workflow_event(
                instance_id=workflow_instance_id,
                event_name="AgentTaskResponse",
                data=message,
            )

        except Exception as e:
            logger.error(f"Error processing agent response: {e}", exc_info=True)

================
File: workflow/orchestrators/roundrobin.py
================
from dapr_agents.workflow.messaging.decorator import message_router
from dapr_agents.workflow.orchestrators.base import OrchestratorWorkflowBase
from dapr.ext.workflow import DaprWorkflowContext
from dapr_agents.types import BaseMessage
from dapr_agents.workflow.decorators import workflow, task
from typing import Any, Optional, Dict
from pydantic import BaseModel, Field
from datetime import timedelta
import logging

logger = logging.getLogger(__name__)


class AgentTaskResponse(BaseMessage):
    """
    Represents a response message from an agent after completing a task.
    """

    workflow_instance_id: Optional[str] = Field(
        default=None, description="Dapr workflow instance id from source if available"
    )


class TriggerAction(BaseModel):
    """
    Represents a message used to trigger an agent's activity within the workflow.
    """

    task: Optional[str] = Field(
        None,
        description="The specific task to execute. If not provided, the agent will act based on its memory or predefined behavior.",
    )
    iteration: Optional[int] = Field(0, description="")
    workflow_instance_id: Optional[str] = Field(
        default=None, description="Dapr workflow instance id from source if available"
    )


class RoundRobinOrchestrator(OrchestratorWorkflowBase):
    """
    Implements a round-robin workflow where agents take turns performing tasks.
    The workflow iterates through conversations by selecting agents in a circular order.

    Uses `continue_as_new` to persist iteration state.
    """

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes and configures the round-robin workflow service.
        Registers tasks and workflows, then starts the workflow runtime.
        """
        self._workflow_name = "RoundRobinWorkflow"
        super().model_post_init(__context)

    @workflow(name="RoundRobinWorkflow")
    def main_workflow(self, ctx: DaprWorkflowContext, input: TriggerAction):
        """
        Executes a round-robin workflow where agents interact iteratively.

        Steps:
        1. Processes input and broadcasts the initial message.
        2. Iterates through agents, selecting a speaker each round.
        3. Waits for agent responses or handles timeouts.
        4. Updates the workflow state and continues the loop.
        5. Terminates when max iterations are reached.

        Uses `continue_as_new` to persist iteration state.

        Args:
            ctx (DaprWorkflowContext): The workflow execution context.
            input (TriggerAction): The current workflow state containing task and iteration.

        Returns:
            str: The last processed message when the workflow terminates.
        """
        task = input.get("task")
        iteration = input.get("iteration", 0)
        instance_id = ctx.instance_id

        if not ctx.is_replaying:
            logger.info(
                f"Round-robin iteration {iteration + 1} started (Instance ID: {instance_id})."
            )

        # Check Termination Condition
        if iteration >= self.max_iterations:
            logger.info(
                f"Max iterations reached. Ending round-robin workflow (Instance ID: {instance_id})."
            )
            return task

        # First iteration: Process input and broadcast
        if iteration == 0:
            message = yield ctx.call_activity(self.process_input, input={"task": task})
            logger.info(f"Initial message from {message['role']} -> {self.name}")

            # Broadcast initial message
            yield ctx.call_activity(
                self.broadcast_message_to_agents, input={"message": message}
            )

        # Select next speaker
        next_speaker = yield ctx.call_activity(
            self.select_next_speaker, input={"iteration": iteration}
        )

        # Trigger agent
        yield ctx.call_activity(
            self.trigger_agent, input={"name": next_speaker, "instance_id": instance_id}
        )

        # Wait for response or timeout
        logger.info("Waiting for agent response...")
        event_data = ctx.wait_for_external_event("AgentTaskResponse")
        timeout_task = ctx.create_timer(timedelta(seconds=self.timeout))
        any_results = yield self.when_any([event_data, timeout_task])

        if any_results == timeout_task:
            logger.warning(
                f"Agent response timed out (Iteration: {iteration + 1}, Instance ID: {instance_id})."
            )
            task_results = {
                "name": "timeout",
                "content": "Timeout occurred. Continuing...",
            }
        else:
            task_results = yield event_data
            logger.info(f"{task_results['name']} -> {self.name}")

        # Update for next iteration
        input["task"] = task_results["content"]
        input["iteration"] = iteration + 1

        # Restart workflow with updated state
        ctx.continue_as_new(input)

    @task
    async def process_input(self, task: str) -> Dict[str, Any]:
        """
        Processes the input message for the workflow.

        Args:
            task (str): The user-provided input task.
        Returns:
            dict: Serialized UserMessage with the content.
        """
        return {"role": "user", "name": self.name, "content": task}

    @task
    async def broadcast_message_to_agents(self, message: Dict[str, Any]):
        """
        Broadcasts a message to all agents.

        Args:
            message (Dict[str, Any]): The message content and additional metadata.
        """
        await self.broadcast_message(
            message=BaseMessage(**message), exclude_orchestrator=True
        )

    @task
    async def select_next_speaker(self, iteration: int) -> str:
        """
        Selects the next speaker in round-robin order.

        Args:
            iteration (int): The current iteration number.
        Returns:
            str: The name of the selected agent.
        """
        agents_metadata = self.get_agents_metadata(exclude_orchestrator=True)
        if not agents_metadata:
            logger.warning("No agents available for selection.")
            raise ValueError("Agents metadata is empty. Cannot select next speaker.")

        agent_names = list(agents_metadata.keys())

        # Determine the next agent in the round-robin order
        next_speaker = agent_names[iteration % len(agent_names)]
        logger.info(
            f"{self.name} selected agent {next_speaker} for iteration {iteration}."
        )
        return next_speaker

    @task
    async def trigger_agent(self, name: str, instance_id: str) -> None:
        """
        Triggers the specified agent to perform its activity.

        Args:
            name (str): Name of the agent to trigger.
            instance_id (str): Workflow instance ID for context.
        """
        await self.send_message_to_agent(
            name=name,
            message=TriggerAction(workflow_instance_id=instance_id),
        )

    @message_router
    async def process_agent_response(self, message: AgentTaskResponse):
        """
        Processes agent response messages sent directly to the agent's topic.

        Args:
            message (AgentTaskResponse): The agent's response containing task results.

        Returns:
            None: The function raises a workflow event with the agent's response.
        """
        try:
            workflow_instance_id = message.get("workflow_instance_id")

            if not workflow_instance_id:
                logger.error(
                    f"{self.name} received an agent response without a valid workflow_instance_id. Ignoring."
                )
                return

            logger.info(
                f"{self.name} processing agent response for workflow instance '{workflow_instance_id}'."
            )

            # Raise a workflow event with the Agent's Task Response
            self.raise_workflow_event(
                instance_id=workflow_instance_id,
                event_name="AgentTaskResponse",
                data=message,
            )

        except Exception as e:
            logger.error(f"Error processing agent response: {e}", exc_info=True)

================
File: workflow/__init__.py
================
from .base import WorkflowApp
from .task import WorkflowTask
from .agentic import AgenticWorkflow
from .orchestrators import LLMOrchestrator, RandomOrchestrator, RoundRobinOrchestrator
from .agents import AssistantAgent
from .decorators import workflow, task

================
File: workflow/agentic.py
================
import asyncio
import json
import logging
import os
import signal
import tempfile
import threading
import inspect
from fastapi import status, Request
from fastapi.responses import JSONResponse
from cloudevents.http.conversion import from_http
from cloudevents.http.event import CloudEvent
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    List,
    Optional,
    Tuple,
    Type,
    Union,
)
from pydantic import BaseModel, Field, ValidationError, PrivateAttr
from dapr.clients import DaprClient
from dapr_agents.agent.utils.text_printer import ColorTextFormatter
from dapr_agents.memory import (
    ConversationListMemory,
    ConversationVectorMemory,
    MemoryBase,
)
from dapr_agents.workflow.messaging import DaprPubSub
from dapr_agents.workflow.messaging.routing import MessageRoutingMixin
from dapr_agents.storage.daprstores.statestore import DaprStateStore
from dapr_agents.workflow import WorkflowApp

if TYPE_CHECKING:
    from fastapi import FastAPI

state_lock = threading.Lock()

logger = logging.getLogger(__name__)


class AgenticWorkflow(WorkflowApp, DaprPubSub, MessageRoutingMixin):
    """
    A class for managing agentic workflows, extending `WorkflowApp`.
    Handles agent interactions, workflow execution, messaging, and metadata management.
    """

    name: str = Field(..., description="The name of the agentic system.")
    message_bus_name: str = Field(
        ..., description="Dapr message bus component for pub/sub messaging."
    )
    broadcast_topic_name: Optional[str] = Field(
        "beacon_channel", description="Default topic for broadcasting messages."
    )
    state_store_name: str = Field(
        ..., description="Dapr state store for workflow state."
    )
    state_key: str = Field(
        default="workflow_state",
        description="Dapr state key for workflow state storage.",
    )
    state: Optional[Union[BaseModel, dict]] = Field(
        default=None, description="Current state of the workflow."
    )
    state_format: Optional[Type[BaseModel]] = Field(
        default=None, description="Schema to enforce state structure."
    )
    agents_registry_store_name: str = Field(
        ..., description="Dapr state store for agent metadata."
    )
    agents_registry_key: str = Field(
        default="agents_registry", description="Key for agents registry in state store."
    )
    max_iterations: int = Field(
        default=20, description="Maximum iterations for workflows.", ge=1
    )
    memory: MemoryBase = Field(
        default_factory=ConversationListMemory,
        description="Handles conversation history storage.",
    )
    save_state_locally: bool = Field(
        default=True, description="Whether to save workflow state locally."
    )
    local_state_path: Optional[str] = Field(
        default=None, description="Path for saving local state."
    )

    # Private internal attributes (not schema/validated)
    _state_store_client: Optional[DaprStateStore] = PrivateAttr(default=None)
    _text_formatter: Optional[ColorTextFormatter] = PrivateAttr(default=None)
    _agent_metadata: Optional[Dict[str, Any]] = PrivateAttr(default=None)
    _workflow_name: str = PrivateAttr(default=None)
    _dapr_client: Optional[DaprClient] = PrivateAttr(default=None)
    _is_running: bool = PrivateAttr(default=False)
    _shutdown_event: asyncio.Event = PrivateAttr(default_factory=asyncio.Event)
    _http_server: Optional[Any] = PrivateAttr(default=None)
    _subscriptions: Dict[str, Callable] = PrivateAttr(default_factory=dict)
    _topic_handlers: Dict[
        Tuple[str, str], Dict[Type[BaseModel], Callable]
    ] = PrivateAttr(default_factory=dict)

    def model_post_init(self, __context: Any) -> None:
        """Initializes the workflow service, messaging, and metadata storage."""

        # Set up color formatter for logging and CLI printing
        self._text_formatter = ColorTextFormatter()

        # Initialize state store client (used for persisting workflow state to Dapr)
        self._state_store_client = DaprStateStore(store_name=self.state_store_name)
        logger.info(f"State store '{self.state_store_name}' initialized.")

        # Load or initialize the current workflow state
        self.initialize_state()

        # Create a Dapr client for service-to-service calls or state interactions
        self._dapr_client = DaprClient()

        super().model_post_init(__context)

    @property
    def app(self) -> "FastAPI":
        """
        Returns the FastAPI application instance if the workflow was initialized as a service.

        Raises:
            RuntimeError: If the FastAPI server has not been initialized via `.as_service()` first.
        """
        if self._http_server:
            return self._http_server.app
        raise RuntimeError("FastAPI server not initialized. Call `as_service()` first.")

    def register_routes(self):
        """
        Hook method to register user-defined routes via `@route(...)` decorators,
        including FastAPI route options like `tags`, `summary`, `response_model`, etc.
        """
        for name, method in inspect.getmembers(self, predicate=inspect.ismethod):
            if getattr(method, "_is_fastapi_route", False):
                path = getattr(method, "_route_path")
                method_type = getattr(method, "_route_method", "GET")
                extra_kwargs = getattr(method, "_route_kwargs", {})

                logger.info(f"Registering route {method_type} {path} -> {name}")
                self.app.add_api_route(
                    path, method, methods=[method_type], **extra_kwargs
                )

    def as_service(self, port: int, host: str = "0.0.0.0"):
        """
        Enables FastAPI-based service mode for the agent by initializing a FastAPI server instance.
        Must be called before `start()` if you want to expose HTTP endpoints.
        """
        from dapr_agents.service.fastapi import FastAPIServerBase

        self._http_server = FastAPIServerBase(
            service_name=self.name,
            service_port=port,
            service_host=host,
        )

        # Register built-in routes
        self.app.add_api_route("/status", lambda: {"ok": True})
        self.app.add_api_route(
            "/start-workflow", self.run_workflow_from_request, methods=["POST"]
        )

        # Allow subclass to register additional routes
        self.register_routes()

        return self

    def handle_shutdown_signal(self, sig):
        logger.info(f"Shutdown signal {sig} received. Stopping service gracefully...")
        self._shutdown_event.set()
        asyncio.create_task(self.stop())

    async def start(self):
        """
        Starts the agent workflow service, optionally as a FastAPI server if .as_service() was called.
        Registers signal handlers and message routes if running in headless mode.
        """
        if self._is_running:
            logger.warning(
                "Service is already running. Ignoring duplicate start request."
            )
            return

        logger.info("Starting Agent Workflow Service...")
        self._shutdown_event.clear()

        try:
            # Headless mode (no HTTP server)
            if not hasattr(self, "_http_server") or self._http_server is None:
                logger.info("Running in headless mode.")
                loop = asyncio.get_running_loop()

                for sig in (signal.SIGINT, signal.SIGTERM):
                    try:
                        loop.add_signal_handler(
                            sig,
                            lambda s=sig: asyncio.create_task(
                                self.handle_shutdown_signal(s)
                            ),
                        )
                    except NotImplementedError:
                        logger.warning(
                            f"Signal {sig} not supported in this environment."
                        )

                self.register_message_routes()

                self._is_running = True
                while not self._shutdown_event.is_set():
                    await asyncio.sleep(1)

            # FastAPI mode
            else:
                logger.info("Running in FastAPI service mode.")
                self.register_message_routes()
                self._is_running = True
                await self._http_server.start()

        except asyncio.CancelledError:
            logger.info("Service received cancellation signal.")
        finally:
            await self.stop()

    async def stop(self):
        """
        Gracefully stops the agent service by unsubscribing and stopping the HTTP server if present.
        """
        if not self._is_running:
            logger.warning("Service is not running. Ignoring stop request.")
            return

        logger.info("Stopping Agent Workflow Service...")

        # Unsubscribe from all topics
        for (pubsub_name, topic_name), close_fn in self._subscriptions.items():
            try:
                logger.info(
                    f"Unsubscribing from pubsub '{pubsub_name}' topic '{topic_name}'"
                )
                close_fn()
            except Exception as e:
                logger.error(f"Failed to unsubscribe from topic '{topic_name}': {e}")

        self._subscriptions.clear()

        # If running as FastAPI, stop the HTTP server
        if hasattr(self, "_http_server") and self._http_server:
            logger.info("Stopping FastAPI server...")
            await self._http_server.stop()

        # Stop the workflow runtime
        if self.wf_runtime_is_running:
            logger.info("Shutting down workflow runtime.")
            self.stop_runtime()
            self.wf_runtime_is_running = False

        self._is_running = False
        logger.info("Agent Workflow Service stopped successfully.")

    def get_chat_history(self, task: Optional[str] = None) -> List[dict]:
        """
        Retrieves and validates the agent's chat history.

        This function fetches messages stored in the agent's memory, optionally filtering
        them based on the given task using vector similarity. The retrieved messages are
        validated using Pydantic (if applicable) and returned as a list of dictionaries.

        Args:
            task (str, optional): A specific task description to filter relevant messages
                using vector embeddings. If not provided, retrieves the full chat history.

        Returns:
            List[dict]: A list of chat history messages, each represented as a dictionary.
                If a message is a Pydantic model, it is serialized using `model_dump()`.
        """
        if isinstance(self.memory, ConversationVectorMemory) and task:
            query_embeddings = self.memory.vector_store.embedding_function.embed(task)
            chat_history = self.memory.get_messages(query_embeddings=query_embeddings)
        else:
            chat_history = self.memory.get_messages()
        chat_history_messages = [
            msg.model_dump() if isinstance(msg, BaseModel) else msg
            for msg in chat_history
        ]
        return chat_history_messages

    def initialize_state(self) -> None:
        """
        Initializes the workflow state by using a provided state, loading from storage, or setting an empty state.

        If the user provides a state, it is validated and used. Otherwise, the method attempts to load
        the existing state from storage. If no stored state is found, an empty dictionary is initialized.

        Ensures `self.state` is always a valid dictionary. If a state format (`self.state_format`)
        is provided, the structure is validated before saving.

        Raises:
            TypeError: If `self.state` is not a dictionary or a valid Pydantic model.
            RuntimeError: If state initialization or loading from storage fails.
        """
        try:
            # Load from storage if the user didn't provide a state
            if self.state is None:
                logger.info("No user-provided state. Attempting to load from storage.")
                self.state = self.load_state()

            # Ensure state is a valid dictionary or Pydantic model
            if isinstance(self.state, BaseModel):
                logger.debug(
                    "User provided a state as a Pydantic model. Converting to dict."
                )
                self.state = self.state.model_dump()

            if not isinstance(self.state, dict):
                raise TypeError(
                    f"Invalid state type: {type(self.state)}. Expected dict or Pydantic model."
                )

            logger.debug(f"Workflow state initialized with {len(self.state)} key(s).")
            self.save_state()

        except Exception as e:
            logger.error(f"Failed to initialize workflow state: {e}")
            raise RuntimeError(f"Error initializing workflow state: {e}") from e

    def validate_state(self, state_data: dict) -> dict:
        """
        Validates the workflow state against the defined schema (`state_format`).

        If a `state_format` (Pydantic model) is provided, this method ensures that
        the `state_data` conforms to the expected structure. If validation succeeds,
        it returns the structured state as a dictionary.

        Args:
            state_data (dict): The raw state data to validate.

        Returns:
            dict: The validated and structured state.

        Raises:
            ValidationError: If the state data does not conform to the expected schema.
        """
        try:
            if not self.state_format:
                logger.warning(
                    "No schema (state_format) provided; returning state as-is."
                )
                return state_data

            logger.info("Validating workflow state against schema.")
            validated_state: BaseModel = self.state_format(
                **state_data
            )  # Validate with Pydantic
            return validated_state.model_dump()  # Convert validated model to dict

        except ValidationError as e:
            logger.error(f"State validation failed: {e}")
            raise ValidationError(f"Invalid workflow state: {e.errors()}") from e

    def load_state(self) -> dict:
        """
        Loads the workflow state from the Dapr state store.

        This method attempts to retrieve the stored state from the configured Dapr state store.
        If no state exists in storage, it initializes an empty state.

        Returns:
            dict: The loaded and optionally validated state.

        Raises:
            RuntimeError: If the state store is not properly configured.
            TypeError: If the retrieved state is not a dictionary.
            ValidationError: If state schema validation fails.
        """
        try:
            if (
                not self._state_store_client
                or not self.state_store_name
                or not self.state_key
            ):
                logger.error("State store is not configured. Cannot load state.")
                raise RuntimeError(
                    "State store is not configured. Please provide 'state_store_name' and 'state_key'."
                )

            # Avoid overwriting state if self.state is already set
            if self.state:
                logger.info(
                    "Using existing in-memory state. Skipping load from storage."
                )
                return self.state

            has_state, state_data = self._state_store_client.try_get_state(
                self.state_key
            )

            if has_state and state_data:
                logger.info(
                    f"Existing state found for key '{self.state_key}'. Validating it."
                )

                if not isinstance(state_data, dict):
                    raise TypeError(
                        f"Invalid state type retrieved: {type(state_data)}. Expected dict."
                    )

                return (
                    self.validate_state(state_data) if self.state_format else state_data
                )

            logger.info(
                f"No existing state found for key '{self.state_key}'. Initializing empty state."
            )
            return {}

        except Exception as e:
            logger.error(f"Failed to load state for key '{self.state_key}': {e}")
            raise RuntimeError(f"Error loading workflow state: {e}") from e

    def get_local_state_file_path(self) -> str:
        """
        Returns the file path for saving the local state.

        If `local_state_path` is None, it defaults to the current working directory with a filename based on `state_key`.
        """
        directory = self.local_state_path or os.getcwd()
        os.makedirs(directory, exist_ok=True)  # Ensure directory exists
        return os.path.join(directory, f"{self.state_key}.json")

    def save_state_to_disk(
        self, state_data: str, filename: Optional[str] = None
    ) -> None:
        """
        Safely saves the workflow state to a local JSON file using a uniquely named temp file.
        - Writes to a temp file in parallel.
        - Locks only the final atomic replacement step to avoid overwriting.
        """
        try:
            # Determine save location
            save_directory = self.local_state_path or os.getcwd()
            os.makedirs(save_directory, exist_ok=True)  # Ensure directory exists
            filename = filename or f"{self.name}_state.json"
            file_path = os.path.join(save_directory, filename)

            # Write to a uniquely named temp file
            with tempfile.NamedTemporaryFile(
                "w", dir=save_directory, delete=False
            ) as tmp_file:
                tmp_file.write(state_data)
                temp_path = tmp_file.name  # Save temp file path

            # Lock only for the final atomic file replacement
            with state_lock:
                # Load the existing state (merge changes)
                existing_state = {}
                if os.path.exists(file_path):
                    with open(file_path, "r", encoding="utf-8") as file:
                        try:
                            existing_state = json.load(file)
                        except json.JSONDecodeError:
                            logger.warning(
                                "Existing state file is corrupt or empty. Overwriting."
                            )

                # Merge new state into existing state
                new_state = (
                    json.loads(state_data)
                    if isinstance(state_data, str)
                    else state_data
                )
                merged_state = {**existing_state, **new_state}  # Merge updates

                # Write merged state back to a new temp file
                with open(temp_path, "w", encoding="utf-8") as file:
                    json.dump(merged_state, file, indent=4)

                # Atomically replace the old state file
                os.replace(temp_path, file_path)

            logger.debug(f"Workflow state saved locally at '{file_path}'.")

        except Exception as e:
            logger.error(f"Failed to save workflow state to disk: {e}")
            raise RuntimeError(f"Error saving workflow state to disk: {e}")

    def save_state(
        self,
        state: Optional[Union[dict, BaseModel, str]] = None,
        force_reload: bool = False,
    ) -> None:
        """
        Saves the current workflow state to the Dapr state store and optionally as a local backup.

        This method updates the internal `self.state`, serializes it, and persists it to Dapr's state store.
        If `save_state_locally` is `True`, it calls `save_state_to_disk` to write the state to a local file.

        Args:
            state (Optional[Union[dict, BaseModel, str]], optional):
                The new state to save. If not provided, the method saves the existing `self.state`.
            force_reload (bool, optional):
                If `True`, reloads the state from the store after saving to ensure consistency.
                Defaults to `False`.

        Raises:
            RuntimeError: If the state store is not configured.
            TypeError: If the provided state is not a supported type (dict, BaseModel, or JSON string).
            ValueError: If the provided state is a string but not a valid JSON format.
            Exception: If any error occurs during the save operation.
        """
        try:
            if (
                not self._state_store_client
                or not self.state_store_name
                or not self.state_key
            ):
                logger.error("State store is not configured. Cannot save state.")
                raise RuntimeError(
                    "State store is not configured. Please provide 'state_store_name' and 'state_key'."
                )

            # Update self.state with the new state if provided
            self.state = state or self.state
            if not self.state:
                logger.warning("Skipping state save: Empty state.")
                return

            # Convert state to a JSON-compatible format
            if isinstance(self.state, BaseModel):
                state_to_save = self.state.model_dump_json()
            elif isinstance(self.state, dict):
                state_to_save = json.dumps(self.state)
            elif isinstance(self.state, str):
                try:
                    json.loads(self.state)  # Ensure the string is valid JSON
                except json.JSONDecodeError as e:
                    raise ValueError(f"Invalid JSON string provided as state: {e}")
                state_to_save = self.state
            else:
                raise TypeError(
                    f"Invalid state type: {type(self.state)}. Expected dict, BaseModel, or JSON string."
                )

            # Save state in Dapr
            self._state_store_client.save_state(self.state_key, state_to_save)
            logger.debug(f"Successfully saved state for key '{self.state_key}'.")

            # Save state locally if enabled
            if self.save_state_locally:
                self.save_state_to_disk(state_data=state_to_save)

            # Reload state after saving if requested
            if force_reload:
                self.state = self.load_state()
                logger.debug(f"State reloaded after saving for key '{self.state_key}'.")

        except Exception as e:
            logger.error(f"Failed to save state for key '{self.state_key}': {e}")
            raise

    def get_agents_metadata(
        self, exclude_self: bool = True, exclude_orchestrator: bool = False
    ) -> dict:
        """
        Retrieves metadata for all registered agents while ensuring orchestrators do not interact with other orchestrators.

        Args:
            exclude_self (bool, optional): If True, excludes the current agent (`self.name`). Defaults to True.
            exclude_orchestrator (bool, optional): If True, excludes all orchestrators from the results. Defaults to False.

        Returns:
            dict: A mapping of agent names to their metadata. Returns an empty dict if no agents are found.

        Raises:
            RuntimeError: If the state store is not properly configured or retrieval fails.
        """
        try:
            # Fetch agent metadata
            agents_metadata = (
                self.get_data_from_store(
                    self.agents_registry_store_name, self.agents_registry_key
                )
                or {}
            )

            if agents_metadata:
                logger.info(
                    f"Agents found in '{self.agents_registry_store_name}' for key '{self.agents_registry_key}'."
                )

                # Filter based on self-exclusion and orchestrator exclusion
                filtered_metadata = {
                    name: metadata
                    for name, metadata in agents_metadata.items()
                    if not (
                        exclude_self and name == self.name
                    )  # Exclude self if requested
                    and not (
                        exclude_orchestrator and metadata.get("orchestrator", False)
                    )  # Exclude orchestrators only if exclude_orchestrator=True
                }

                if not filtered_metadata:
                    logger.info("No other agents found after filtering.")

                return filtered_metadata

            logger.info(
                f"No agents found in '{self.agents_registry_store_name}' for key '{self.agents_registry_key}'."
            )
            return {}
        except Exception as e:
            logger.error(f"Failed to retrieve agents metadata: {e}", exc_info=True)
            raise RuntimeError(f"Error retrieving agents metadata: {str(e)}") from e

    async def broadcast_message(
        self,
        message: Union[BaseModel, dict],
        exclude_orchestrator: bool = False,
        **kwargs,
    ) -> None:
        """
        Sends a message to all agents (or only to non-orchestrator agents if exclude_orchestrator=True).

        Args:
            message (Union[BaseModel, dict]): The message content as a Pydantic model or dictionary.
            exclude_orchestrator (bool, optional): If True, excludes orchestrators from receiving the message. Defaults to False.
            **kwargs: Additional metadata fields to include in the message.
        """
        try:
            # Retrieve agents metadata while respecting the exclude_orchestrator flag
            agents_metadata = self.get_agents_metadata(
                exclude_orchestrator=exclude_orchestrator
            )

            if not agents_metadata:
                logger.warning("No agents available for broadcast.")
                return

            logger.info(
                f"{self.name} broadcasting message to {self.broadcast_topic_name}."
            )

            await self.publish_event_message(
                topic_name=self.broadcast_topic_name,
                pubsub_name=self.message_bus_name,
                source=self.name,
                message=message,
                **kwargs,
            )

            logger.debug(f"{self.name} broadcasted message.")
        except Exception as e:
            logger.error(f"Failed to broadcast message: {e}", exc_info=True)

    async def send_message_to_agent(
        self, name: str, message: Union[BaseModel, dict], **kwargs
    ) -> None:
        """
        Sends a message to a specific agent.

        Args:
            name (str): The name of the target agent.
            message (Union[BaseModel, dict]): The message content as a Pydantic model or dictionary.
            **kwargs: Additional metadata fields to include in the message.
        """
        try:
            agents_metadata = self.get_agents_metadata()

            if name not in agents_metadata:
                logger.warning(
                    f"Target '{name}' is not registered as an agent. Skipping message send."
                )
                return  # Do not raise an error—just warn and move on.

            agent_metadata = agents_metadata[name]
            logger.info(f"{self.name} sending message to agent '{name}'.")

            await self.publish_event_message(
                topic_name=agent_metadata["topic_name"],
                pubsub_name=agent_metadata["pubsub_name"],
                source=self.name,
                message=message,
                **kwargs,
            )

            logger.debug(f"{self.name} sent message to agent '{name}'.")
        except Exception as e:
            logger.error(
                f"Failed to send message to agent '{name}': {e}", exc_info=True
            )

    def print_interaction(
        self, sender_agent_name: str, recipient_agent_name: str, message: str
    ) -> None:
        """
        Prints the interaction between two agents in a formatted and colored text.

        Args:
            sender_agent_name (str): The name of the agent sending the message.
            recipient_agent_name (str): The name of the agent receiving the message.
            message (str): The message content to display.
        """
        separator = "-" * 80

        # Print sender -> recipient and the message
        interaction_text = [
            (sender_agent_name, "dapr_agents_mustard"),
            (" -> ", "dapr_agents_teal"),
            (f"{recipient_agent_name}\n\n", "dapr_agents_mustard"),
            (message + "\n\n", None),
            (separator + "\n", "dapr_agents_teal"),
        ]

        # Print the formatted text
        self._text_formatter.print_colored_text(interaction_text)

    def register_agentic_system(self) -> None:
        """
        Registers the agent's metadata in the Dapr state store under 'agents_metadata'.
        """
        try:
            # Update the agents registry store with the new agent metadata
            self.register_agent(
                store_name=self.agents_registry_store_name,
                store_key=self.agents_registry_key,
                agent_name=self.name,
                agent_metadata=self._agent_metadata,
            )
        except Exception as e:
            logger.error(f"Failed to register metadata for agent {self.name}: {e}")
            raise e

    async def run_workflow_from_request(self, request: Request) -> JSONResponse:
        """
        Run a workflow instance triggered by an incoming HTTP POST request.
        Supports dynamic workflow name via query param (?name=...).

        Args:
            request (Request): The incoming request containing input data for the workflow.

        Returns:
            JSONResponse: A 202 Accepted response with the workflow instance ID if successful,
                        or a 400/500 error response if the request fails validation or execution.
        """
        try:
            # Extract workflow name from query parameters or use default
            workflow_name = request.query_params.get("name") or self._workflow_name
            if not workflow_name:
                return JSONResponse(
                    content={"error": "No workflow name specified."},
                    status_code=status.HTTP_400_BAD_REQUEST,
                )

            # Validate workflow name against registered workflows
            if workflow_name not in self.workflows:
                return JSONResponse(
                    content={
                        "error": f"Unknown workflow '{workflow_name}'. Available: {list(self.workflows.keys())}"
                    },
                    status_code=status.HTTP_400_BAD_REQUEST,
                )

            # Parse body as CloudEvent or fallback to JSON
            try:
                event: CloudEvent = from_http(
                    dict(request.headers), await request.body()
                )
                input_data = event.data
            except Exception:
                input_data = await request.json()

            logger.info(f"Starting workflow '{workflow_name}' with input: {input_data}")
            instance_id = self.run_workflow(workflow=workflow_name, input=input_data)

            asyncio.create_task(self.monitor_workflow_completion(instance_id))

            return JSONResponse(
                content={
                    "message": "Workflow initiated successfully.",
                    "workflow_instance_id": instance_id,
                },
                status_code=status.HTTP_202_ACCEPTED,
            )

        except Exception as e:
            logger.error(f"Error starting workflow: {str(e)}", exc_info=True)
            return JSONResponse(
                content={"error": "Failed to start workflow", "details": str(e)},
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            )

================
File: workflow/base.py
================
import asyncio
import functools
import inspect
import json
import logging
import sys
import time
import uuid
from typing import Any, Callable, Dict, List, Optional, TypeVar, Union

from pydantic import BaseModel, ConfigDict, Field

from durabletask import task as dtask

from dapr.clients import DaprClient
from dapr.clients.grpc._request import (
    TransactionOperationType,
    TransactionalStateOperation,
)
from dapr.clients.grpc._response import StateResponse
from dapr.clients.grpc._state import Concurrency, Consistency, StateOptions
from dapr.ext.workflow import (
    DaprWorkflowClient,
    WorkflowActivityContext,
    WorkflowRuntime,
)
from dapr.ext.workflow.workflow_state import WorkflowState

from dapr_agents.llm.chat import ChatClientBase
from dapr_agents.types.workflow import DaprWorkflowStatus
from dapr_agents.workflow.task import WorkflowTask
from dapr_agents.workflow.utils import get_decorated_methods

logger = logging.getLogger(__name__)

T = TypeVar("T")


class WorkflowApp(BaseModel):
    """
    A Pydantic-based class to encapsulate a Dapr Workflow runtime and manage workflows and tasks.
    """

    llm: Optional[ChatClientBase] = Field(
        default=None, description="The default LLM client for all LLM-based tasks."
    )
    timeout: int = Field(
        default=300,
        description="Default timeout duration in seconds for workflow tasks.",
    )

    # Initialized in model_post_init
    wf_runtime: Optional[WorkflowRuntime] = Field(
        default=None, init=False, description="Workflow runtime instance."
    )
    wf_runtime_is_running: Optional[bool] = Field(
        default=None, init=False, description="Is the Workflow runtime running?"
    )
    wf_client: Optional[DaprWorkflowClient] = Field(
        default=None, init=False, description="Workflow client instance."
    )
    client: Optional[DaprClient] = Field(
        default=None, init=False, description="Dapr client instance."
    )
    tasks: Dict[str, Callable] = Field(
        default_factory=dict, init=False, description="Dictionary of registered tasks."
    )
    workflows: Dict[str, Callable] = Field(
        default_factory=dict,
        init=False,
        description="Dictionary of registered workflows.",
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def model_post_init(self, __context: Any) -> None:
        """
        Initialize the Dapr workflow runtime and register tasks & workflows.
        """
        # Initialize clients and runtime
        self.wf_runtime = WorkflowRuntime()
        self.wf_runtime_is_running = False
        self.wf_client = DaprWorkflowClient()
        self.client = DaprClient()
        logger.info("WorkflowApp initialized; discovering tasks and workflows.")

        # Discover and register tasks and workflows
        discovered_tasks = self._discover_tasks()
        self._register_tasks(discovered_tasks)
        discovered_wfs = self._discover_workflows()
        self._register_workflows(discovered_wfs)

        super().model_post_init(__context)

    def get_chat_history(self) -> List[Any]:
        """
        Stub for fetching past conversation history. Override in subclasses.
        """
        logger.debug("Fetching chat history (default stub)")
        return []

    def _choose_llm_for(self, method: Callable) -> Optional[ChatClientBase]:
        """
        Encapsulate LLM selection logic.
          1. Use per-task override if provided on decorator.
          2. Else if marked as explicitly requiring an LLM, fall back to default app LLM.
          3. Otherwise, returns None.
        """
        per_task = getattr(method, "_task_llm", None)
        if per_task:
            return per_task
        if getattr(method, "_explicit_llm", False):
            return self.llm
        return None

    def _discover_tasks(self) -> Dict[str, Callable]:
        """Gather all @task-decorated functions and methods."""
        module = sys.modules["__main__"]
        tasks: Dict[str, Callable] = {}
        # Free functions in __main__
        for name, fn in inspect.getmembers(module, inspect.isfunction):
            if getattr(fn, "_is_task", False) and fn.__module__ == module.__name__:
                tasks[getattr(fn, "_task_name", name)] = fn
        # Bound methods (if any) discovered via helper
        for name, method in get_decorated_methods(self, "_is_task").items():
            tasks[getattr(method, "_task_name", name)] = method
        logger.debug(f"Discovered tasks: {list(tasks)}")
        return tasks

    def _register_tasks(self, tasks: Dict[str, Callable]) -> None:
        """Register each discovered task with the Dapr runtime."""
        for task_name, method in tasks.items():
            llm = self._choose_llm_for(method)
            logger.debug(
                f"Registering task '{task_name}' with llm={getattr(llm, '__class__', None)}"
            )
            kwargs = getattr(method, "_task_kwargs", {})
            task_instance = WorkflowTask(
                func=method,
                description=getattr(method, "_task_description", None),
                agent=getattr(method, "_task_agent", None),
                llm=llm,
                include_chat_history=getattr(
                    method, "_task_include_chat_history", False
                ),
                workflow_app=self,
                **kwargs,
            )
            # Wrap for Dapr invocation
            wrapped = self._make_task_wrapper(task_name, method, task_instance)
            activity_decorator = self.wf_runtime.activity(name=task_name)
            self.tasks[task_name] = activity_decorator(wrapped)

    def _make_task_wrapper(
        self, task_name: str, method: Callable, task_instance: WorkflowTask
    ) -> Callable:
        """Produce the function that Dapr will invoke for each activity."""

        def run_sync(coro):
            # Try to get the running event loop and run until complete
            try:
                loop = asyncio.get_running_loop()
                return loop.run_until_complete(coro)
            except RuntimeError:
                # If no running loop, create one
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                return loop.run_until_complete(coro)

        @functools.wraps(method)
        def wrapper(ctx: WorkflowActivityContext, *args, **kwargs):
            wf_ctx = WorkflowActivityContext(ctx)
            try:
                call = task_instance(wf_ctx, *args, **kwargs)
                if asyncio.iscoroutine(call):
                    return run_sync(call)
                return call
            except Exception:
                logger.exception(f"Task '{task_name}' failed")
                raise

        return wrapper

    def _discover_workflows(self) -> Dict[str, Callable]:
        """Gather all @workflow-decorated functions and methods."""
        module = sys.modules["__main__"]
        wfs: Dict[str, Callable] = {}
        for name, fn in inspect.getmembers(module, inspect.isfunction):
            if getattr(fn, "_is_workflow", False) and fn.__module__ == module.__name__:
                wfs[getattr(fn, "_workflow_name", name)] = fn
        for name, method in get_decorated_methods(self, "_is_workflow").items():
            wfs[getattr(method, "_workflow_name", name)] = method
        logger.info(f"Discovered workflows: {list(wfs)}")
        return wfs

    def _register_workflows(self, wfs: Dict[str, Callable]) -> None:
        """Register each discovered workflow with the Dapr runtime."""
        for wf_name, method in wfs.items():
            # Use a closure helper to avoid late-binding capture issues.
            def make_wrapped(meth: Callable) -> Callable:
                @functools.wraps(meth)
                def wrapped(*args, **kwargs):
                    return meth(*args, **kwargs)

                return wrapped

            decorator = self.wf_runtime.workflow(name=wf_name)
            self.workflows[wf_name] = decorator(make_wrapped(method))

    def start_runtime(self):
        """Idempotently start the Dapr workflow runtime."""
        if not self.wf_runtime_is_running:
            logger.info("Starting workflow runtime.")
            self.wf_runtime.start()
            self.wf_runtime_is_running = True
        else:
            logger.debug("Workflow runtime already running; skipping.")

    def stop_runtime(self):
        """Idempotently stop the Dapr workflow runtime."""
        if self.wf_runtime_is_running:
            logger.info("Stopping workflow runtime.")
            self.wf_runtime.shutdown()
            self.wf_runtime_is_running = False
        else:
            logger.debug("Workflow runtime already stopped; skipping.")

    def register_agent(
        self, store_name: str, store_key: str, agent_name: str, agent_metadata: dict
    ) -> None:
        """
        Merges the existing data with the new data and updates the store.

        Args:
            store_name (str): The name of the Dapr state store component.
            key (str): The key to update.
            data (dict): The data to update the store with.
        """
        # retry the entire operation up to ten times sleeping 1 second between each attempt
        for attempt in range(1, 11):
            try:
                response: StateResponse = self.client.get_state(
                    store_name=store_name, key=store_key
                )
                if not response.etag:
                    # if there is no etag the following transaction won't work as expected
                    # so we need to save an empty object with a strong consistency to force the etag to be created
                    self.client.save_state(
                        store_name=store_name,
                        key=store_key,
                        value=json.dumps({}),
                        state_metadata={"contentType": "application/json"},
                        options=StateOptions(
                            concurrency=Concurrency.first_write,
                            consistency=Consistency.strong,
                        ),
                    )
                    # raise an exception to retry the entire operation
                    raise Exception(f"No etag found for key: {store_key}")
                existing_data = json.loads(response.data) if response.data else {}
                if (agent_name, agent_metadata) in existing_data.items():
                    logger.debug(f"agent {agent_name} already registered.")
                    return None
                agent_data = {agent_name: agent_metadata}
                merged_data = {**existing_data, **agent_data}
                logger.debug(f"merged data: {merged_data} etag: {response.etag}")
                try:
                    # using the transactional API to be able to later support the Dapr outbox pattern
                    self.client.execute_state_transaction(
                        store_name=store_name,
                        operations=[
                            TransactionalStateOperation(
                                key=store_key,
                                data=json.dumps(merged_data),
                                etag=response.etag,
                                operation_type=TransactionOperationType.upsert,
                            )
                        ],
                        transactional_metadata={"contentType": "application/json"},
                    )
                except Exception as e:
                    raise e
                return None
            except Exception as e:
                logger.error(f"Error on transaction attempt: {attempt}: {e}")
                logger.info("Sleeping for 1 second before retrying transaction...")
                time.sleep(1)
        raise Exception(
            f"Failed to update state store key: {store_key} after 10 attempts."
        )

    def get_data_from_store(self, store_name: str, key: str) -> Optional[dict]:
        """
        Retrieves data from the Dapr state store using the given key.

        Args:
            store_name (str): The name of the Dapr state store component.
            key (str): The key to fetch data from.

        Returns:
            Optional[dict]: the retrieved dictionary or None if not found.
        """
        try:
            response: StateResponse = self.client.get_state(
                store_name=store_name, key=key
            )
            data = response.data
            return json.loads(data) if data else None
        except Exception:
            logger.warning(
                f"Error retrieving data for key '{key}' from store '{store_name}'"
            )
            return None

    def resolve_task(self, task: Union[str, Callable]) -> Callable:
        """
        Resolves a registered task function by its name or decorated function.

        Args:
            task (Union[str, Callable]): The task name or callable function.

        Returns:
            Callable: The resolved task function.

        Raises:
            AttributeError: If the task is not found.
        """
        if isinstance(task, str):
            task_name = task
        elif callable(task):
            task_name = getattr(task, "_task_name", task.__name__)
        else:
            raise ValueError(f"Invalid task reference: {task}")

        task_func = self.tasks.get(task_name)
        if not task_func:
            raise AttributeError(f"Task '{task_name}' not found.")

        return task_func

    def resolve_workflow(self, workflow: Union[str, Callable]) -> Callable:
        """
        Resolves a registered workflow function by its name or decorated function.

        Args:
            workflow (Union[str, Callable]): The workflow name or callable function.

        Returns:
            Callable: The resolved workflow function.

        Raises:
            AttributeError: If the workflow is not found.
        """
        if isinstance(workflow, str):
            workflow_name = workflow  # Direct lookup by string name
        elif callable(workflow):
            workflow_name = getattr(workflow, "_workflow_name", workflow.__name__)
        else:
            raise ValueError(f"Invalid workflow reference: {workflow}")

        workflow_func = self.workflows.get(workflow_name)
        if not workflow_func:
            raise AttributeError(f"Workflow '{workflow_name}' not found.")

        return workflow_func

    def run_workflow(
        self, workflow: Union[str, Callable], input: Union[str, Dict[str, Any]] = None
    ) -> str:
        """
        Starts a workflow execution.

        Args:
            workflow (Union[str, Callable]): The workflow name or callable.
            input (Union[str, Dict[str, Any]], optional): Input data for the workflow.

        Returns:
            str: The instance ID of the started workflow.

        Raises:
            Exception: If workflow execution fails.
        """
        try:
            # Start Workflow Runtime
            if not self.wf_runtime_is_running:
                self.start_runtime()

            # Generate unique instance ID
            instance_id = uuid.uuid4().hex

            # Resolve the workflow function
            workflow_func = self.resolve_workflow(workflow)

            # Schedule workflow execution
            instance_id = self.wf_client.schedule_new_workflow(
                workflow=workflow_func, input=input, instance_id=instance_id
            )

            logger.info(f"Started workflow with instance ID {instance_id}.")
            return instance_id
        except Exception as e:
            logger.error(f"Failed to start workflow {workflow}: {e}")
            raise

    async def monitor_workflow_state(self, instance_id: str) -> Optional[WorkflowState]:
        """
        Monitors and retrieves the final state of a workflow instance.

        Args:
            instance_id (str): The workflow instance ID.

        Returns:
            Optional[WorkflowState]: The final state of the workflow or None if not found.
        """
        try:
            state: WorkflowState = await asyncio.to_thread(
                self.wait_for_workflow_completion,
                instance_id,
                fetch_payloads=True,
                timeout_in_seconds=self.timeout,
            )

            if not state:
                logger.error(f"Workflow '{instance_id}' not found.")
                return None

            return state
        except TimeoutError:
            logger.error(f"Workflow '{instance_id}' monitoring timed out.")
            return None
        except Exception as e:
            logger.error(f"Error retrieving workflow state for '{instance_id}': {e}")
            return None

    async def monitor_workflow_completion(self, instance_id: str) -> None:
        """
        Monitors the execution of a workflow and logs its final state.

        Args:
            instance_id (str): The workflow instance ID.
        """
        try:
            logger.info(f"Monitoring workflow '{instance_id}'...")

            # Retrieve workflow state
            state = await self.monitor_workflow_state(instance_id)
            if not state:
                return  # Error already logged in monitor_workflow_state

            # Extract relevant details
            workflow_status = state.runtime_status.name
            failure_details = (
                state.failure_details
            )  # This is an object, not a dictionary

            if workflow_status == "COMPLETED":
                logger.info(
                    f"Workflow '{instance_id}' completed successfully. Status: {workflow_status}."
                )

                if state.serialized_output:
                    logger.debug(
                        f"Output: {json.dumps(state.serialized_output, indent=2)}"
                    )

            elif workflow_status in ("FAILED", "ABORTED"):
                # Ensure `failure_details` exists before accessing attributes
                error_type = getattr(failure_details, "error_type", "Unknown")
                message = getattr(failure_details, "message", "No message provided")
                stack_trace = getattr(
                    failure_details, "stack_trace", "No stack trace available"
                )

                logger.error(
                    f"Workflow '{instance_id}' failed.\n"
                    f"Error Type: {error_type}\n"
                    f"Message: {message}\n"
                    f"Stack Trace:\n{stack_trace}\n"
                    f"Input: {json.dumps(state.serialized_input, indent=2)}"
                )

                self.terminate_workflow(instance_id)

            else:
                logger.warning(
                    f"Workflow '{instance_id}' ended with status '{workflow_status}'.\n"
                    f"Input: {json.dumps(state.serialized_input, indent=2)}"
                )

            logger.debug(
                f"Workflow Details: Instance ID={state.instance_id}, Name={state.name}, "
                f"Created At={state.created_at}, Last Updated At={state.last_updated_at}"
            )

        except Exception as e:
            logger.error(
                f"Error monitoring workflow '{instance_id}': {e}", exc_info=True
            )
        finally:
            logger.info(f"Finished monitoring workflow '{instance_id}'.")

    async def run_and_monitor_workflow_async(
        self,
        workflow: Union[str, Callable],
        input: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> Optional[str]:
        """
        Runs a workflow asynchronously and monitors its completion.

        Args:
            workflow (Union[str, Callable]): The workflow name or callable.
            input (Optional[Union[str, Dict[str, Any]]]): The workflow input payload.

        Returns:
            Optional[str]: The serialized output of the workflow.
        """
        instance_id = None
        try:
            # Off-load the potentially blocking run_workflow call to a thread.
            instance_id = await asyncio.to_thread(self.run_workflow, workflow, input)

            # Await the asynchronous monitoring of the workflow state.
            state = await self.monitor_workflow_state(instance_id)

            if not state:
                raise RuntimeError(f"Workflow '{instance_id}' not found.")

            workflow_status = (
                DaprWorkflowStatus[state.runtime_status.name]
                if state.runtime_status.name in DaprWorkflowStatus.__members__
                else DaprWorkflowStatus.UNKNOWN
            )

            if workflow_status == DaprWorkflowStatus.COMPLETED:
                logger.info(f"Workflow '{instance_id}' completed successfully!")
                logger.debug(f"Output: {state.serialized_output}")
            else:
                logger.error(
                    f"Workflow '{instance_id}' ended with status '{workflow_status.value}'."
                )

            # Return the final state output
            return state.serialized_output

        except Exception as e:
            logger.error(f"Error during workflow '{instance_id}': {e}")
            raise
        finally:
            logger.info(f"Finished workflow with Instance ID: {instance_id}.")
            # Off-load the stop_runtime call as it may block.
            await asyncio.to_thread(self.stop_runtime)

    def run_and_monitor_workflow_sync(
        self,
        workflow: Union[str, Callable],
        input: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> Optional[str]:
        """
        Synchronous wrapper for running and monitoring a workflow.
        This allows calling code that is not async to still run the workflow.

        Args:
            workflow (Union[str, Callable]): The workflow name or callable.
            input (Optional[Union[str, Dict[str, Any]]]): The workflow input payload.

        Returns:
            Optional[str]: The serialized output of the workflow.
        """
        return asyncio.run(self.run_and_monitor_workflow_async(workflow, input))

    def terminate_workflow(
        self, instance_id: str, *, output: Optional[Any] = None
    ) -> None:
        """
        Terminates a running workflow.

        Args:
            instance_id (str): The workflow instance ID.
            output (Optional[Any]): Optional output to set for the terminated workflow.

        Raises:
            Exception: If the termination fails.
        """
        try:
            self.wf_client.terminate_workflow(instance_id=instance_id, output=output)
            logger.info(
                f"Successfully terminated workflow '{instance_id}' with output: {output}"
            )
        except Exception as e:
            logger.error(f"Failed to terminate workflow '{instance_id}'. Error: {e}")
            raise Exception(f"Error terminating workflow '{instance_id}': {e}")

    def get_workflow_state(self, instance_id: str) -> Optional[Any]:
        """
        Retrieves the state of a workflow instance.

        Args:
            instance_id (str): The workflow instance ID.

        Returns:
            Optional[Any]: The workflow state if found.

        Raises:
            RuntimeError: If retrieving the state fails.
        """
        try:
            state = self.wf_client.get_workflow_state(instance_id)
            logger.info(
                f"Retrieved state for workflow {instance_id}: {state.runtime_status}."
            )
            return state
        except Exception as e:
            logger.error(f"Failed to retrieve workflow state for {instance_id}: {e}")
            return None

    def wait_for_workflow_completion(
        self,
        instance_id: str,
        fetch_payloads: bool = True,
        timeout_in_seconds: int = 120,
    ) -> Optional[WorkflowState]:
        """
        Waits for a workflow to complete and retrieves its state.

        Args:
            instance_id (str): The workflow instance ID.
            fetch_payloads (bool): Whether to fetch input/output payloads.
            timeout_in_seconds (int): Maximum wait time in seconds.

        Returns:
            Optional[WorkflowState]: The final state or None if it times out.

        Raises:
            RuntimeError: If waiting for completion fails.
        """
        try:
            state = self.wf_client.wait_for_workflow_completion(
                instance_id,
                fetch_payloads=fetch_payloads,
                timeout_in_seconds=timeout_in_seconds,
            )
            if state:
                logger.info(
                    f"Workflow {instance_id} completed with status: {state.runtime_status}."
                )
            else:
                logger.warning(
                    f"Workflow {instance_id} did not complete within the timeout period."
                )
            return state
        except Exception as e:
            logger.error(
                f"Error while waiting for workflow {instance_id} completion: {e}"
            )
            return None

    def raise_workflow_event(
        self, instance_id: str, event_name: str, *, data: Any | None = None
    ) -> None:
        """
        Raises an event for a running workflow instance.

        Args:
            instance_id (str): The workflow instance ID.
            event_name (str): The name of the event to raise.
            data (Any | None): Optional event data.

        Raises:
            Exception: If raising the event fails.
        """
        try:
            logger.info(
                f"Raising workflow event '{event_name}' for instance '{instance_id}'"
            )
            self.wf_client.raise_workflow_event(
                instance_id=instance_id, event_name=event_name, data=data
            )
            logger.info(
                f"Successfully raised workflow event '{event_name}' for instance '{instance_id}'!"
            )
        except Exception as e:
            logger.error(
                f"Error raising workflow event '{event_name}' for instance '{instance_id}'. "
                f"Data: {data}, Error: {e}"
            )
            raise Exception(
                f"Failed to raise workflow event '{event_name}' for instance '{instance_id}': {str(e)}"
            )

    def invoke_service(
        self,
        service: str,
        method: str,
        http_method: str = "POST",
        input: Optional[Dict[str, Any]] = None,
        timeout: Optional[int] = None,
    ) -> Any:
        """
        Invokes an external service via Dapr.

        Args:
            service (str): The service name.
            method (str): The method to call.
            http_method (str, optional): The HTTP method (default: "POST").
            input (Optional[Dict[str, Any]], optional): The request payload.
            timeout (Optional[int], optional): Timeout in seconds.

        Returns:
            Any: The response from the service.

        Raises:
            Exception: If the invocation fails.
        """
        try:
            resp = self.client.invoke_method(
                app_id=service,
                method_name=method,
                http_verb=http_method,
                data=json.dumps(input) if input else None,
                timeout=timeout,
            )
            if resp.status_code != 200:
                raise Exception(
                    f"Error calling {service}.{method}: {resp.status_code}: {resp.text}"
                )

            agent_response = json.loads(resp.data.decode("utf-8"))
            logger.info(f"Agent's Response: {agent_response}")
            return agent_response
        except Exception as e:
            logger.error(f"Failed to invoke {service}.{method}: {e}")
            raise e

    def when_all(self, tasks: List[dtask.Task[T]]) -> dtask.WhenAllTask[T]:
        """
        Waits for all given tasks to complete.

        Args:
            tasks (List[dtask.Task[T]]): The tasks to wait for.

        Returns:
            dtask.WhenAllTask[T]: A task that completes when all tasks finish.
        """
        return dtask.when_all(tasks)

    def when_any(self, tasks: List[dtask.Task[T]]) -> dtask.WhenAnyTask:
        """
        Waits for any one of the given tasks to complete.

        Args:
            tasks (List[dtask.Task[T]]): The tasks to monitor.

        Returns:
            dtask.WhenAnyTask: A task that completes when the first task finishes.
        """
        return dtask.when_any(tasks)

================
File: workflow/decorators.py
================
import functools
import inspect
from typing import Any, Callable, Optional
import logging

from pydantic import BaseModel, ValidationError

from dapr.ext.workflow import DaprWorkflowContext


def route(path: str, method: str = "GET", **kwargs):
    """
    Decorator to mark an instance method as a FastAPI route.

    Args:
        path (str): The URL path to bind this route to.
        method (str): The HTTP method to use (e.g., 'GET', 'POST'). Defaults to 'GET'.
        **kwargs: Additional arguments passed to FastAPI's `add_api_route`.

    Example:
        @route("/status", method="GET", summary="Show status", tags=["monitoring"])
        def health(self):
            return {"ok": True}
    """

    def decorator(func):
        func._is_fastapi_route = True
        func._route_path = path
        func._route_method = method.upper()
        func._route_kwargs = kwargs
        return func

    return decorator


def task(
    func: Optional[Callable] = None,
    *,
    name: Optional[str] = None,
    description: Optional[str] = None,
    agent: Optional[Any] = None,
    llm: Optional[Any] = None,
    include_chat_history: bool = False,
    **task_kwargs,
) -> Callable:
    """
    Decorator to register a function as a Dapr workflow task.

    This allows configuring a task with an LLM, agent, chat history, and other options.
    All additional keyword arguments are stored and forwarded to the WorkflowTask constructor.

    Args:
        func (Optional[Callable]): The function to wrap. Can also be used as `@task(...)`.
        name (Optional[str]): Optional custom task name. Defaults to the function name.
        description (Optional[str]): Optional prompt template for LLM-based execution.
        agent (Optional[Any]): Optional agent to handle the task instead of an LLM or function.
        llm (Optional[Any]): Optional LLM client used to execute the task.
        include_chat_history (bool): Whether to include prior messages in LLM calls.
        **task_kwargs: Additional keyword arguments to forward to `WorkflowTask`.

    Returns:
        Callable: The decorated function with attached task metadata.
    """

    if isinstance(func, str):
        # Allow syntax: @task("some description")
        description = func
        func = None

    def decorator(f: Callable) -> Callable:
        if not callable(f):
            raise ValueError(f"@task must be applied to a function, got {type(f)}.")

        # Attach task metadata
        f._is_task = True
        f._task_name = name or f.__name__
        f._task_description = description
        f._task_agent = agent
        f._task_llm = llm
        f._task_include_chat_history = include_chat_history
        f._explicit_llm = llm is not None or bool(description)
        f._task_kwargs = task_kwargs

        # wrap it so we can log, validate, etc., without losing signature/docs
        @functools.wraps(f)
        def wrapper(*args, **kwargs):
            logging.getLogger(__name__).debug(f"Calling task '{f._task_name}'")
            return f(*args, **kwargs)

        # copy our metadata onto the wrapper so discovery still sees it
        for attr in (
            "_is_task",
            "_task_name",
            "_task_description",
            "_task_agent",
            "_task_llm",
            "_task_include_chat_history",
            "_explicit_llm",
            "_task_kwargs",
        ):
            setattr(wrapper, attr, getattr(f, attr))

        return wrapper

    return (
        decorator(func) if func else decorator
    )  # Supports both @task and @task(name="custom")


def is_pydantic_model(obj: Any) -> bool:
    """Check if the given type is a subclass of Pydantic's BaseModel."""
    return isinstance(obj, type) and issubclass(obj, BaseModel)


def workflow(
    func: Optional[Callable] = None, *, name: Optional[str] = None
) -> Callable:
    """
    Decorator to register a function as a Dapr workflow with optional Pydantic validation.

    - Ensures the correct placement of `ctx: DaprWorkflowContext`
    - If an input parameter is a Pydantic model, validates and serializes it
    - Works seamlessly with standalone functions, instance methods, and class methods.

    Args:
        func (Callable, optional): Function to be decorated as a workflow.
        name (Optional[str]): The name to register the workflow with.

    Returns:
        Callable: The decorated function with input validation.
    """

    def decorator(f: Callable) -> Callable:
        if not callable(f):
            raise ValueError(f"@workflow must be applied to a function, got {type(f)}.")

        # Assign workflow metadata attributes
        f._is_workflow = True
        f._workflow_name = name or f.__name__

        sig = inspect.signature(f)
        params = list(sig.parameters.values())

        # Determine if function is an instance method, class method, or standalone function
        is_instance_method = False
        is_class_method = False

        if isinstance(f, classmethod):  # If already wrapped as a class method
            is_class_method = True
            f = f.__func__  # Extract the underlying function
            sig = inspect.signature(f)  # Recompute signature after unwrapping
            params = list(sig.parameters.values())  # Update parameter list

        elif params and params[0].name == "self":  # Instance method
            is_instance_method = True

        elif (
            params and params[0].name == "cls"
        ):  # Class method without @classmethod decorator
            is_class_method = True

        # Compute the expected index for `ctx`
        ctx_index = 1 if is_instance_method or is_class_method else 0

        # Ensure `ctx` is correctly positioned
        if (
            len(params) <= ctx_index
            or params[ctx_index].annotation is not DaprWorkflowContext
        ):
            raise TypeError(
                f"Workflow '{f.__name__}' must have 'ctx: DaprWorkflowContext' as the {'second' if ctx_index == 1 else 'first'} parameter."
            )

        # Identify the input parameter (third argument for methods, second otherwise)
        input_param_index = ctx_index + 1
        input_param = (
            params[input_param_index] if len(params) > input_param_index else None
        )
        input_type = input_param.annotation if input_param else None
        is_pydantic = is_pydantic_model(input_type)

        @functools.wraps(f)
        def wrapper(*args, **kwargs):
            """Wrapper for handling input validation and execution."""

            logging.getLogger(__name__).info(f"Starting workflow '{f._workflow_name}'")

            bound_args = sig.bind_partial(*args, **kwargs)
            bound_args.apply_defaults()

            # Extract `ctx` (first parameter for functions, second for methods)
            ctx = bound_args.arguments.get(params[ctx_index].name, None)
            if not isinstance(ctx, DaprWorkflowContext):
                raise TypeError(
                    f"Expected '{params[ctx_index].name}' to be a DaprWorkflowContext instance."
                )

            # Extract `input` (third parameter for methods, second otherwise)
            input_value = (
                bound_args.arguments.get(input_param.name, None)
                if input_param
                else None
            )

            # Ensure metadata is extracted without modifying input_value unnecessarily
            metadata = None
            if isinstance(input_value, dict) and "_message_metadata" in input_value:
                metadata = input_value.pop("_message_metadata")

            # Validate input if it's a Pydantic model
            if is_pydantic and input_value:
                try:
                    validated_input: BaseModel = input_type(
                        **input_value
                    )  # Validate with Pydantic
                except ValidationError as e:
                    raise ValueError(
                        f"Invalid input for workflow '{f._workflow_name}': {e.errors()}"
                    ) from e

                # Convert back to dict and reattach metadata
                validated_dict = validated_input.model_dump()
                if metadata is not None:
                    validated_dict[
                        "_message_metadata"
                    ] = metadata  # Ensure metadata is not lost

                # Overwrite the function argument with the modified dictionary
                bound_args.arguments[input_param.name] = validated_dict

            return f(*bound_args.args, **bound_args.kwargs)

        wrapper._is_workflow = True
        wrapper._workflow_name = f._workflow_name
        return wrapper

    return (
        decorator(func) if func else decorator
    )  # Supports both `@workflow` and `@workflow(name="custom")`

================
File: workflow/task.py
================
import asyncio
import inspect
import logging
from dataclasses import is_dataclass
from functools import update_wrapper
from types import SimpleNamespace
from typing import Any, Callable, Dict, List, Literal, Optional

from pydantic import BaseModel, ConfigDict, Field

from dapr.ext.workflow import WorkflowActivityContext

from dapr_agents.agent.base import AgentBase
from dapr_agents.llm.chat import ChatClientBase
from dapr_agents.llm.openai import OpenAIChatClient
from dapr_agents.llm.utils import StructureHandler
from dapr_agents.types import BaseMessage, ChatCompletion, UserMessage

logger = logging.getLogger(__name__)


class WorkflowTask(BaseModel):
    """
    Encapsulates task logic for execution by an LLM, agent, or Python function.

    Supports both synchronous and asynchronous tasks, with optional output validation
    using Pydantic models or specified return types.
    """

    func: Optional[Callable] = Field(
        None, description="The original function to be executed, if provided."
    )
    description: Optional[str] = Field(
        None, description="A description template for the task, used with LLM or agent."
    )
    agent: Optional[AgentBase] = Field(
        None, description="The agent used for task execution, if applicable."
    )
    llm: Optional[ChatClientBase] = Field(
        None, description="The LLM client for executing the task, if applicable."
    )
    include_chat_history: Optional[bool] = Field(
        False,
        description="Whether to include past conversation history in the LLM call.",
    )
    workflow_app: Optional[Any] = Field(
        None, description="Reference to the WorkflowApp instance."
    )
    structured_mode: Literal["json", "function_call"] = Field(
        default="json",
        description="Structured response mode for LLM output. Valid values: 'json', 'function_call'.",
    )
    task_kwargs: Dict[str, Any] = Field(
        default_factory=dict,
        exclude=True,
        description="Additional keyword arguments passed via the @task decorator.",
    )

    # Initialized during setup
    signature: Optional[inspect.Signature] = Field(
        None, init=False, description="The signature of the provided function."
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def model_post_init(self, __context: Any) -> None:
        """
        Post-initialization to set up function signatures and default LLM clients.
        """
        # Default to OpenAIChatClient if prompt‐based but no llm provided
        if self.description and not self.llm:
            self.llm = OpenAIChatClient()

        if self.func:
            # Preserve name / docs for stack traces
            update_wrapper(self, self.func)

        # Capture signature for input / output handling
        self.signature = inspect.signature(self.func) if self.func else None

        # Honor any structured_mode override
        if not self.structured_mode and "structured_mode" in self.task_kwargs:
            self.structured_mode = self.task_kwargs["structured_mode"]

        # Proceed with base model setup
        super().model_post_init(__context)

    async def __call__(self, ctx: WorkflowActivityContext, payload: Any = None) -> Any:
        """
        Executes the task, routing to agent, LLM, or pure-Python logic.

        Dispatches to Python, Agent, or LLM paths and validates output.

        Args:
            ctx (WorkflowActivityContext): The workflow execution context.
            payload (Any): The task input.

        Returns:
            Any: The result of the task.
        """
        # Prepare input dict
        data = self._normalize_input(payload) if payload is not None else {}
        logger.info(f"Executing task '{self.func.__name__}'")
        logger.debug(f"Executing task '{self.func.__name__}' with input {data!r}")

        try:
            executor = self._choose_executor()
            if executor in ("agent", "llm"):
                if not self.description:
                    raise ValueError("LLM/agent tasks require a description template")
                prompt = self.format_description(self.description, data)
                raw = await self._run_via_ai(prompt, executor)
            else:
                raw = await self._run_python(data)

            validated = await self._validate_output(raw)
            return validated

        except Exception:
            logger.exception(f"Error in task '{self.func.__name__}'")
            raise

    def _choose_executor(self) -> Literal["agent", "llm", "python"]:
        """
        Pick execution path.

        Returns:
            One of "agent", "llm", or "python".

        Raises:
            ValueError: If no valid executor is configured.
        """
        if self.agent:
            return "agent"
        if self.llm:
            return "llm"
        if self.func:
            return "python"
        raise ValueError("No execution path found for this task")

    async def _run_python(self, data: dict) -> Any:
        """
        Invoke the Python function directly.

        Args:
            data: Keyword arguments for the function.

        Returns:
            The function's return value.
        """
        logger.info("Invoking regular Python function")
        if asyncio.iscoroutinefunction(self.func):
            return await self.func(**data)
        else:
            return self.func(**data)

    async def _run_via_ai(self, prompt: str, executor: Literal["agent", "llm"]) -> Any:
        """
        Run the prompt through an Agent or LLM.

        Args:
            prompt: The fully formatted prompt string.
            kind: "agent" or "llm".

        Returns:
            Raw result from the AI path.
        """
        logger.info(f"Invoking task via {executor.upper()}")
        logger.debug(f"Invoking task with prompt: {prompt!r}")
        if executor == "agent":
            result = await self.agent.run(prompt)
        else:
            result = await self._invoke_llm(prompt)
        return self._convert_result(result)

    async def _invoke_llm(self, prompt: str) -> Any:
        """
        Build messages and call the LLM client.

        Args:
            prompt: The formatted prompt string.

        Returns:
            LLM-generated result.
        """
        # Gather history if needed
        history: List[BaseMessage] = []
        if self.include_chat_history and self.workflow_app:
            logger.debug("Retrieving chat history")
            history = self.workflow_app.get_chat_history()

        messages: List[BaseMessage] = history + [UserMessage(prompt)]
        params: Dict[str, Any] = {"messages": messages}

        # Add structured formatting if return type is a Pydantic model
        if (
            self.signature
            and self.signature.return_annotation is not inspect.Signature.empty
        ):
            model_cls = StructureHandler.resolve_response_model(
                self.signature.return_annotation
            )
            if model_cls:
                params["response_format"] = self.signature.return_annotation
                params["structured_mode"] = self.structured_mode

        logger.debug(f"LLM call params: {params}")
        return self.llm.generate(**params)

    def _normalize_input(self, raw_input: Any) -> dict:
        """
        Normalize various input types into a dict.

        Args:
            raw_input: Dataclass, SimpleNamespace, single value, or dict.

        Returns:
            A dict suitable for function invocation.

        Raises:
            ValueError: If signature is missing when wrapping a single value.
        """
        if is_dataclass(raw_input):
            return raw_input.__dict__
        if isinstance(raw_input, SimpleNamespace):
            return vars(raw_input)
        if not isinstance(raw_input, dict):
            # wrap single argument
            if not self.signature:
                raise ValueError("Cannot infer param name without signature")
            name = next(iter(self.signature.parameters))
            return {name: raw_input}
        return raw_input

    async def _validate_output(self, result: Any) -> Any:
        """
        Await and validate the result against return-type model.

        Args:
            result: Raw result from executor.

        Returns:
            Validated/transformed result.
        """
        if asyncio.iscoroutine(result):
            result = await result

        if (
            not self.signature
            or self.signature.return_annotation is inspect.Signature.empty
        ):
            return result

        return StructureHandler.validate_against_signature(
            result, self.signature.return_annotation
        )

    def _convert_result(self, result: Any) -> Any:
        """
        Unwrap AI return types into plain Python.

        Args:
            result: ChatCompletion, BaseModel, or list of BaseModel.

        Returns:
            A primitive, dict, or list of dicts.
        """
        # Unwrap ChatCompletion
        if isinstance(result, ChatCompletion):
            logger.debug("Extracted message content from ChatCompletion.")
            return result.get_content()
        # Pydantic → dict
        if isinstance(result, BaseModel):
            logger.debug("Converting Pydantic model to dictionary.")
            return result.model_dump()
        if isinstance(result, list) and all(isinstance(x, BaseModel) for x in result):
            logger.debug("Converting list of Pydantic models to list of dictionaries.")
            return [x.model_dump() for x in result]
        # If no specific conversion is necessary, return as-is
        logger.info("Returning final task result.")
        return result

    def format_description(self, template: str, data: dict) -> str:
        """
        Interpolate inputs into the prompt template.

        Args:
            template: The `{}`-style template string.
            data: Mapping of variable names to values.

        Returns:
            The fully formatted prompt.
        """
        if self.signature:
            bound = self.signature.bind(**data)
            bound.apply_defaults()
            return template.format(**bound.arguments)
        return template.format(**data)


class TaskWrapper:
    """
    A wrapper for WorkflowTask that preserves callable behavior and attributes like __name__.
    """

    def __init__(self, task_instance: WorkflowTask, name: str):
        """
        Initialize the TaskWrapper.

        Args:
            task_instance (WorkflowTask): The task instance to wrap.
            name (str): The task name.
        """
        self.task_instance = task_instance
        self.__name__ = name
        self.__doc__ = getattr(task_instance.func, "__doc__", None)
        self.__module__ = getattr(task_instance.func, "__module__", None)

    def __call__(self, *args, **kwargs):
        """
        Delegate the call to the wrapped WorkflowTask instance.
        """
        return self.task_instance(*args, **kwargs)

    def __getattr__(self, item):
        """
        Delegate attribute access to the wrapped task.
        """
        return getattr(self.task_instance, item)

    def __repr__(self):
        return f"<TaskWrapper name={self.__name__}>"

================
File: workflow/utils.py
================
import inspect
import logging
from typing import Any, Callable, Dict

logger = logging.getLogger(__name__)


def get_decorated_methods(instance: Any, attribute_name: str) -> Dict[str, Callable]:
    """
    Find all **public** bound methods on `instance` that carry a given decorator attribute.

    This will:
      1. Inspect the class for functions or methods.
      2. Bind them to the instance (so `self` is applied).
      3. Filter in only those where `hasattr(method, attribute_name) is True`.

    Args:
        instance:  Any object whose methods you want to inspect.
        attribute_name:
            The name of the attribute set by your decorator
            (e.g. "_is_task" or "_is_workflow").

    Returns:
        A dict mapping `method_name` → `bound method`.

    Example:
        >>> class A:
        ...     @task
        ...     def foo(self): ...
        ...
        >>> get_decorated_methods(A(), "_is_task")
        {"foo": <bound method A.foo of <A object ...>>}
    """
    discovered: Dict[str, Callable] = {}

    cls = type(instance)
    for name, member in inspect.getmembers(cls, predicate=inspect.isfunction):
        # skip private/protected
        if name.startswith("_"):
            continue

        # bind to instance so that signature(self, ...) works
        try:
            bound = getattr(instance, name)
        except Exception as e:
            logger.warning(f"Could not bind method '{name}': {e}")
            continue

        # pick up only those with our decorator flag
        if hasattr(bound, attribute_name):
            discovered[name] = bound
            logger.debug(f"Discovered decorated method: {name}")

    return discovered

================
File: __init__.py
================
from dapr_agents.agent import (
    Agent,
    AgentActor,
    ReActAgent,
    ToolCallAgent,
    OpenAPIReActAgent,
)
from dapr_agents.llm.openai import (
    OpenAIChatClient,
    OpenAIAudioClient,
    OpenAIEmbeddingClient,
)
from dapr_agents.llm.huggingface import HFHubChatClient
from dapr_agents.llm.nvidia import NVIDIAChatClient, NVIDIAEmbeddingClient
from dapr_agents.llm.elevenlabs import ElevenLabsSpeechClient
from dapr_agents.tool import AgentTool, tool
from dapr_agents.workflow import (
    WorkflowApp,
    AgenticWorkflow,
    LLMOrchestrator,
    RandomOrchestrator,
    RoundRobinOrchestrator,
    AssistantAgent,
)
from dapr_agents.executors import LocalCodeExecutor, DockerCodeExecutor



================================================================
End of Codebase
================================================================
