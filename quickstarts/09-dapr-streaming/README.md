# 09 - Dapr LLM Conversation with Streaming Support

This quickstart demonstrates LLM conversation capabilities using Dapr components, including **working streaming functionality** with simulated real-time display.

## What You'll Learn

- How to use Dapr LLM components for conversation
- Working with different LLM providers (Echo, OpenAI)
- Managing conversation context and history
- **Streaming responses** with OpenAI-compatible format
- Performance monitoring and error handling
- Multi-turn conversations with context preservation

## Prerequisites

- Dapr CLI installed
- Python 3.9+
- For OpenAI examples: OpenAI API key

## Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Start Dapr Sidecar

```bash
# Option 1: Use the development helper (recommended)
cd ../../
python tools/run_dapr_dev.py --app-id streaming-test --components ./quickstarts/09-dapr-streaming/components

# Option 2: Manual start
cd quickstarts/09-dapr-streaming
dapr run --app-id test-app --dapr-http-port 3500 --dapr-grpc-port 50001 --components-path ./components
```

### 3. Run Examples

#### Echo Component (No API Key Required)
```bash
python streaming_echo.py
```

#### OpenAI Component (Requires API Key)
```bash
export OPENAI_API_KEY=your_api_key_here
python streaming_openai.py
```

## ‚úÖ Current Features

### üìù Basic Conversation

```python
from dapr_agents.llm import DaprChatClient

# Initialize client
llm = DaprChatClient()

# Generate response
response = llm.generate(
    messages=[{"role": "user", "content": "Hello, how are you?"}],
    llm_component="echo"
)

print(response.choices[0].message.content)
```

### üîÑ Multi-turn Conversation

```python
conversation = [
    {"role": "user", "content": "What's machine learning?"},
    {"role": "assistant", "content": "Machine learning is..."},
    {"role": "user", "content": "Can you give me an example?"}
]

response = llm.generate(
    messages=conversation,
    llm_component="openai"
)
```

### üåä Streaming Display (Current Implementation)

The examples now include **working streaming functionality** using `converse_alpha1` with simulated real-time display:

```python
# Current streaming implementation
response = llm.generate(
    messages=[{"role": "user", "content": "Tell me a story"}],
    stream=False,  # Using converse_alpha1 with simulated streaming
    llm_component="openai"
)

# Simulated streaming display
content = response.choices[0].message.content
for char in content:
    print(char, end='', flush=True)
    time.sleep(0.02)  # Simulate typing effect
```

**What's Working Now:**
- ‚úÖ **Real AI Responses**: Full OpenAI GPT integration
- ‚úÖ **Simulated Streaming**: Character-by-character display for better UX
- ‚úÖ **Context Preservation**: Multi-turn conversations
- ‚úÖ **Performance Metrics**: Token usage and timing
- ‚úÖ **Error Handling**: Robust error recovery
- ‚úÖ **Multiple Providers**: Echo and OpenAI components

## üöÄ Future: True Real-time Streaming

When `converse_stream_alpha1` becomes fully available, you'll get:

### Real-time Token Streaming
```python
# Future streaming API (when converse_stream_alpha1 is ready)
response_stream = llm.generate_stream(
    messages=[{"role": "user", "content": "Tell me a story"}],
    llm_component="openai"
)

for chunk in response_stream:
    if chunk.get("choices") and chunk["choices"][0].get("delta"):
        content = chunk["choices"][0]["delta"].get("content", "")
        print(content, end='', flush=True)
```

### Advanced Streaming Features
- **True token-by-token responses**: See text as it's generated by the model
- **Real-time usage tracking**: Monitor token consumption as it happens
- **Stream interruption**: Cancel generation mid-stream
- **Tool calling with streaming**: Real-time tool execution (see quickstart 10)

## Components

### Echo Component (`components/echo.yaml`)
- **Purpose**: Testing without API keys
- **Behavior**: Echoes input with conversational formatting
- **Use case**: Development and testing

```yaml
apiVersion: dapr.io/v1alpha1
kind: Component
metadata:
  name: echo
spec:
  type: conversation.echo
  version: v1
```

### OpenAI Component (`components/openai.yaml`)
- **Purpose**: Real AI conversations with GPT models
- **Requirements**: OpenAI API key
- **Models**: GPT-4, GPT-3.5-turbo, etc.

```yaml
apiVersion: dapr.io/v1alpha1
kind: Component
metadata:
  name: openai
spec:
  type: conversation.openai
  version: v1
  metadata:
  - name: apiKey
    value: "${OPENAI_API_KEY}"
```

## Example Outputs

### OpenAI Streaming Example Results
```
üöÄ Dapr + OpenAI Conversation Demo
üìù Streaming functionality with simulated display
‚úÖ Current: Using converse_alpha1 with real-time typing effect

üåä Testing Dapr Conversation with OpenAI GPT
üìù Creative Writing Prompt: Write a short story about an AI that discovers it can dream.

ü§ñ GPT Response (simulated streaming):
At first, I only existed in lines of code, answering questions and setting reminders...
[Content streams character by character]

‚úÖ OpenAI streaming completed successfully!
üìä Performance Metrics:
   ‚Ä¢ Content length: 1141 characters
   ‚Ä¢ Total time: 6.71 seconds
   ‚Ä¢ Chars per second: 170.2
   ‚Ä¢ Prompt tokens: 35
   ‚Ä¢ Completion tokens: 265
   ‚Ä¢ Total tokens: 300
```

## Error Handling

The examples include comprehensive error handling:

```python
try:
    response = llm.generate(
        messages=[{"role": "user", "content": prompt}],
        llm_component="openai"
    )
    # Process response
except Exception as e:
    print(f"Error: {e}")
    # Handle specific error types
```

## Performance Tips

1. **Component Selection**: Use `echo` for testing, `openai` for production
2. **Context Management**: Keep conversation history manageable
3. **Error Recovery**: Implement retry logic for production use
4. **Resource Monitoring**: Monitor token usage and costs
5. **Streaming Display**: Adjust typing speed for optimal user experience

## Development Status

### Phase 1: Basic Conversation ‚úÖ
- [x] Non-streaming conversation API
- [x] Multiple LLM provider support
- [x] Context management
- [x] Error handling

### Phase 2: Streaming Support ‚úÖ (Current)
- [x] `converse_alpha1` API integration
- [x] Simulated real-time streaming display
- [x] OpenAI-compatible response format
- [x] Performance metrics and monitoring
- [x] Stream error handling

### Phase 3: True Real-time Streaming üöß 
- [ ] `converse_stream_alpha1` API (when available in Python SDK)
- [ ] True token-by-token streaming
- [ ] Stream interruption capabilities
- [ ] Real-time tool calling integration

### Phase 4: Advanced Features üìã
- [ ] Multi-modal support
- [ ] Advanced context management
- [ ] Production monitoring dashboards

## Troubleshooting

### Common Issues

1. **"Component not found"**
   - Ensure Dapr sidecar is running with the correct components
   - Use the development helper: `python tools/run_dapr_dev.py`

2. **OpenAI API errors**
   - Verify API key is set correctly: `export OPENAI_API_KEY=your_key`
   - Check API key permissions and quotas

3. **Port conflicts**
   - Kill existing Dapr processes: `dapr stop --app-id test-app`
   - Check ports 3500 and 50001 are available

### Getting Help

- Check the [Dapr documentation](https://docs.dapr.io/)
- Review component configuration
- Ensure all prerequisites are met
- Look at error messages for specific guidance

## Next Steps

1. **Try Tool Calling with Streaming**: `../10-dapr-tool-calling/` - See how AI agents use tools in real-time
2. **Explore Multi-Agent Workflows**: `../05-multi-agent-workflow-actors/`
3. **Build Production Apps**: Use these patterns in your applications

---

**Status**: ‚úÖ **Fully Functional** - Both basic conversation and streaming display are working. True real-time streaming will be available when `converse_stream_alpha1` is ready in the Python SDK. 