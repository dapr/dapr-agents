#!/usr/bin/env python3

"""
Dapr Conversation Example with Echo Component
This example demonstrates basic LLM conversation using the echo component.

Note: Streaming functionality (converse_stream_alpha1) is currently in development 
and will be available in future versions of the Python SDK.
"""

import time
from dotenv import load_dotenv
from colorama import init, Fore, Style
from dapr_agents.llm import DaprChatClient

# Initialize colorama for colored output
init()


def main():
    """Main conversation example with echo component."""
    load_dotenv()

    print(
        f"{Fore.CYAN}üöÄ Testing Dapr Conversation with Echo Component{Style.RESET_ALL}"
    )
    print(
        f"{Fore.YELLOW}This example demonstrates LLM conversation without requiring API keys{Style.RESET_ALL}"
    )
    print(
        f"{Fore.MAGENTA}üìù Note: Streaming support is in development and coming soon!{Style.RESET_ALL}\n"
    )

    # Initialize the Dapr chat client
    llm = DaprChatClient()

    # Test basic conversation
    prompt = "Tell me a story about a brave knight on a quest to save the kingdom."

    print(f"{Fore.GREEN}üìù Prompt:{Style.RESET_ALL} {prompt}\n")
    print(f"{Fore.BLUE}Response:{Style.RESET_ALL}")

    try:
        # Generate response (non-streaming for now)
        start_time = time.time()

        response = llm.generate(
            messages=[{"role": "user", "content": prompt}],
            stream=False,  # Streaming not yet available
            llm_component="echo",
        )

        end_time = time.time()

        # Extract and display the response
        if hasattr(response, "choices") and response.choices:
            content = response.choices[0].message.content
            print(content)
        else:
            print("Response received but format is unexpected")
            print(f"Response type: {type(response)}")
            print(f"Response: {response}")

        print(f"\n{Fore.MAGENTA}‚úÖ Conversation completed!{Style.RESET_ALL}")
        print(f"{Fore.CYAN}üìä Stats:{Style.RESET_ALL}")
        print(f"   ‚Ä¢ Response time: {end_time - start_time:.2f} seconds")
        print("   ‚Ä¢ Component used: echo")

    except Exception as e:
        print(f"\n{Fore.RED}‚ùå Error during conversation: {e}{Style.RESET_ALL}")
        print(
            f"{Fore.YELLOW}üí° Make sure Dapr sidecar is running with echo component{Style.RESET_ALL}"
        )
        return

    # Test conversation with context
    print(f"\n{Fore.CYAN}üîÑ Testing conversation with context{Style.RESET_ALL}")

    conversation = [
        {"role": "user", "content": "What's your favorite color?"},
        {
            "role": "assistant",
            "content": "I enjoy discussing the concept of blue - it represents depth and tranquility.",
        },
        {"role": "user", "content": "Why do you find that color appealing?"},
    ]

    print(f"{Fore.GREEN}üìù Multi-turn conversation:{Style.RESET_ALL}")
    for msg in conversation:
        role_color = Fore.CYAN if msg["role"] == "user" else Fore.MAGENTA
        print(
            f"   {role_color}{msg['role'].title()}:{Style.RESET_ALL} {msg['content']}"
        )

    print(f"\n{Fore.BLUE}Response:{Style.RESET_ALL}")

    try:
        start_time = time.time()

        response = llm.generate(
            messages=conversation, stream=False, llm_component="echo"
        )

        end_time = time.time()

        # Extract and display the response
        if hasattr(response, "choices") and response.choices:
            content = response.choices[0].message.content
            print(content)

        print(f"\n{Fore.MAGENTA}‚úÖ Context conversation completed!{Style.RESET_ALL}")
        print(f"{Fore.CYAN}üìä Stats:{Style.RESET_ALL}")
        print(f"   ‚Ä¢ Response time: {end_time - start_time:.2f} seconds")

    except Exception as e:
        print(f"\n{Fore.RED}‚ùå Error during context conversation: {e}{Style.RESET_ALL}")

    # Show what's coming
    print(f"\n{Fore.CYAN}üöÄ Coming Soon: Streaming Support{Style.RESET_ALL}")
    print(
        f"{Fore.YELLOW}When streaming is available, you'll be able to:{Style.RESET_ALL}"
    )
    print("   ‚Ä¢ See responses as they're generated in real-time")
    print("   ‚Ä¢ Get faster perceived response times")
    print("   ‚Ä¢ Build more interactive chat experiences")
    print("   ‚Ä¢ Monitor token usage in real-time")

    print(
        f"\n{Fore.GREEN}‚úÖ Example completed! Try the OpenAI example for more advanced features.{Style.RESET_ALL}"
    )


if __name__ == "__main__":
    main()
